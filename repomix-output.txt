This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.github/
  workflows/
    pages.yml
    release.yml
    tests.yml
cfg/
  config.go
core/
  events.go
db/
  change_log_event_test.go
  change_log_event.go
  change_log.go
  cleanup.go
  global_change_log_script.tmpl
  sqlite.go
  table_change_log_script.tmpl
  utils_test.go
  utils.go
docs/
  docs/
    architecture.md
    configuration-reference.md
    demo.md
    faq.md
    introduction.md
    nats-configuration.md
    production-deployment.md
    quick-start.md
    replication.md
    snapshots.md
    troubleshooting.md
  src/
    components/
      HomepageFeatures/
        index.tsx
        styles.module.css
    css/
      custom.css
    pages/
      index.module.css
      index.tsx
  static/
    img/
      logo.svg
  .gitignore
  docusaurus.config.ts
  package.json
  README.md
  sidebars.ts
  tsconfig.json
examples/
  nats-server-1.conf
  nats-server-2.conf
  nats-server-3.conf
  node-1-config.toml
  node-2-config.toml
  node-3-config.toml
  run-cluster.sh
logstream/
  replication_event_test.go
  replication_event.go
  replication_state_test.go
  replication_state.go
  replicator_meta_store.go
  replicator.go
pool/
  connection_pool_test.go
  connection_pool.go
  sqlite_driver_connector.go
snapshot/
  db_snapshot_test.go
  db_snapshot.go
  nats_snapshot.go
  nats_storage.go
  s3_storage.go
  sftp_storage.go
  webdav_storage.go
stream/
  embedded_nats.go
  nats_logger.go
  nats.go
  routes_discover.go
telemetry/
  telemetry.go
tests/
  e2e/
    config_test.go
    e2e_suite_test.go
    e2e_test.go
    helpers_test.go
utils/
  deep.go
  state_context.go
  stop_watch.go
  timeout.go
.gitignore
build-linux.sh
CODE_OF_CONDUCT.md
config.toml
go.mod
harmonylite.go
LICENSE
README.md

================================================================
Files
================================================================

================
File: .github/workflows/pages.yml
================
name: 📕 Deploy docs site to Pages

on:
  push:
    branches: ["master"]
    paths:
      - 'docs/**'                            # Only run when files in the docs directory change
      - '.github/workflows/pages.yml'        # Or when this workflow file itself changes
  workflow_dispatch:  # Keep manual trigger option

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  build:
    defaults:
      run:
        working-directory: docs
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Detect package manager
        id: detect-package-manager
        run: |
          if [ -f "${{ github.workspace }}/docs/yarn.lock" ]; then
            echo "manager=yarn" >> $GITHUB_OUTPUT
            echo "command=install" >> $GITHUB_OUTPUT
            echo "runner=yarn" >> $GITHUB_OUTPUT
            exit 0
          elif [ -f "${{ github.workspace }}/docs/package.json" ]; then
            echo "manager=npm" >> $GITHUB_OUTPUT
            echo "command=ci" >> $GITHUB_OUTPUT
            echo "runner=npm run" >> $GITHUB_OUTPUT
            exit 0
          else
            echo "Unable to determine package manager"
            exit 1
          fi
      - name: Setup Node
        uses: actions/setup-node@v3
        with:
          node-version: "18"
          cache: ${{ steps.detect-package-manager.outputs.manager }}
          cache-dependency-path: docs/package-lock.json
      - name: Setup Pages
        uses: actions/configure-pages@v2
      - name: Restore cache
        uses: actions/cache@v3
        with:
          path: |
            docs/node_modules
            docs/.docusaurus
          key: ${{ runner.os }}-docusaurus-${{ hashFiles('**/package-lock.json', '**/yarn.lock') }}-${{ hashFiles('**.[jt]s', '**.[jt]sx') }}
          restore-keys: |
            ${{ runner.os }}-docusaurus-${{ hashFiles('**/package-lock.json', '**/yarn.lock') }}-
      - name: Install dependencies
        run: ${{ steps.detect-package-manager.outputs.manager }} ${{ steps.detect-package-manager.outputs.command }}
      - name: Build Docusaurus site
        run: ${{ steps.detect-package-manager.outputs.runner }} build
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs/build

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

================
File: .github/workflows/release.yml
================
name: Release

on:
  release:
    types: [created]
  workflow_dispatch:        # Allow manual triggering

jobs:
  tests:
    uses: ./.github/workflows/tests.yml

  linux-build:
    needs: tests
    # Skip build job on pull requests if they come from forks
    # This saves CI minutes while still running tests
    if: ${{ github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - arch:   amd64
            cc:     gcc

          - arch:   amd64
            cc:     gcc
            static: true

          - arch: arm64
            cc:   aarch64-linux-gnu-gcc

          - arch: arm64
            cc:   aarch64-linux-gnu-gcc
            static: true
          - arch: arm
            arm:  6
            cc:   arm-linux-gnueabi-gcc

          - arch: arm
            arm:  6
            cc:   arm-linux-gnueabi-gcc
            static: true

          - arch: arm
            arm:  7
            cc:   arm-linux-gnueabihf-gcc

          - arch: arm
            arm:  7
            cc:   arm-linux-gnueabihf-gcc
            static: true
    env:
      GOOS:   linux
      GOARCH: ${{ matrix.arch }}
      GOARM:  ${{ matrix.arm }}
      CC:     ${{ matrix.cc }}
      LDFLAGS: ${{ matrix.static && '-extldflags "-static"' || '' }}
      SUFFIX: "${{ matrix.static && '-static' || ''}}"
      VERSION: "${{ github.event_name == 'release' && github.event.release.name || github.sha }}"
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-go@v4
        with:
          go-version: '^1.24.0'
          check-latest: true

      - name: Install cross-compilers
        run: |
          sudo apt-get update 
          sudo apt-get install -y gcc-aarch64-linux-gnu gcc-arm-linux-gnueabihf gcc-arm-linux-gnueabi

      - name: Build harmonylite
        run: |
          CGO_ENABLED=1 go build -ldflags "-s -w ${{ env.LDFLAGS }}" -o harmonylite .
          tar -czvf harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.GOARM }}${{ env.SUFFIX }}.tar.gz harmonylite config.toml LICENSE README.md examples/*

      - name: Upload binary artifact
        uses: actions/upload-artifact@v4
        with:
          name: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.GOARM }}${{ env.SUFFIX }}.tar.gz
          path: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.GOARM }}${{ env.SUFFIX }}.tar.gz
          if-no-files-found: error

      - name: Get release
        id: release
        uses: bruceadams/get-release@v1.2.3
        if: github.event_name == 'release'
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Upload release tarball
        uses: actions/upload-release-asset@v1.0.2
        if: github.event_name == 'release'
        env:
          GITHUB_TOKEN: ${{ github.token }}
        with:
          upload_url: ${{ steps.release.outputs.upload_url }}
          asset_path: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.GOARM }}${{ env.SUFFIX }}.tar.gz
          asset_name: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.GOARM }}${{ env.SUFFIX }}.tar.gz
          asset_content_type: application/gzip
          
  mac-build:
    needs: tests
    # Skip build job on pull requests if they come from forks
    if: ${{ github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository }}
    runs-on: macos-latest
    strategy:
      matrix:
        include:
          - arch: arm64
            cc: gcc

          - arch: amd64
            cc: gcc
    env:
      GOOS: darwin
      GOARCH: ${{ matrix.arch }}
      CC: ${{ matrix.cc }}
      LDFLAGS: ${{ matrix.static && '-extldflags "-static"' || '' }}
      SUFFIX: "${{ matrix.static && '-static' || ''}}"
      VERSION: "${{ github.event_name == 'release' && github.event.release.name || github.sha }}"
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-go@v4
        with:
          go-version: '^1.24.0'
          check-latest: true

      - name: Build harmonylite
        run: |
          CGO_ENABLED=1 go build -o harmonylite .
          tar -czvf harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.SUFFIX }}.tar.gz harmonylite config.toml LICENSE README.md examples/*

      - name: Upload binary artifact
        uses: actions/upload-artifact@v4
        with:
          name: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.SUFFIX }}.tar.gz
          path: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.SUFFIX }}.tar.gz
          if-no-files-found: error

      - name: Get release
        id: release
        uses: bruceadams/get-release@v1.3.2
        if: github.event_name == 'release'
        env:
          GITHUB_TOKEN: ${{ github.token }}

      - name: Upload release tarball
        uses: actions/upload-release-asset@v1.0.2
        if: github.event_name == 'release'
        env:
          GITHUB_TOKEN: ${{ github.token }}
        with:
          upload_url: ${{ steps.release.outputs.upload_url }}
          asset_path: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.SUFFIX }}.tar.gz
          asset_name: harmonylite-${{ env.VERSION }}-${{ env.GOOS }}-${{ env.GOARCH }}${{ env.SUFFIX }}.tar.gz
          asset_content_type: application/gzip

================
File: .github/workflows/tests.yml
================
name: Tests

on:
  workflow_call:
  workflow_dispatch:        # Allow manual triggering
  push:
    branches: [master]
    paths-ignore:
      - 'docs/**'           # Ignore documentation changes
      - '**.md'             # Ignore markdown files
      - '.github/workflows/pages.yml'  # Ignore docs workflow file
  pull_request:
      types:
        - opened
        - synchronize
        - reopened
      paths-ignore:
        - 'docs/**'           # Ignore documentation changes
        - '**.md'             # Ignore markdown files
        - '.github/workflows/pages.yml'  # Ignore docs workflow file

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '^1.24.0'
          check-latest: true

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y sqlite3 libsqlite3-dev

      - name: Run unit tests
        run: |
          # Run all tests except those in the e2e directory
          go test -v $(go list ./... | grep -v "/tests/e2e")

  e2e-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
      - uses: actions/checkout@v3

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '^1.24.0'
          check-latest: true

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y sqlite3 libsqlite3-dev
          go install github.com/onsi/ginkgo/v2/ginkgo@latest

      - name: Build HarmonyLite
        run: |
          CGO_ENABLED=1 CGO_CFLAGS="-Wno-typedef-redefinition -Wno-nullability-completeness" go build -o harmonylite .
          # Make sure the binary is executable
          chmod +x harmonylite
          # Print working directory and check if the binary exists
          pwd
          ls -la

      - name: Run E2E Tests
        run: |
           ginkgo -v tests/e2e

================
File: cfg/config.go
================
package cfg

import (
	"flag"
	"fmt"
	"hash/fnv"
	"os"
	"path"
	"path/filepath"

	"github.com/BurntSushi/toml"
	"github.com/denisbrodbeck/machineid"
	"github.com/google/uuid"
	"github.com/rs/zerolog/log"
)

type SnapshotStoreType string

const NodeNamePrefix = "harmonylite-node"
const EmbeddedClusterName = "e-harmonylite"
const (
	Nats   SnapshotStoreType = "nats"
	S3     SnapshotStoreType = "s3"
	WebDAV SnapshotStoreType = "webdav"
	SFTP   SnapshotStoreType = "sftp"
)

type ReplicationLogConfiguration struct {
	Shards         uint64 `toml:"shards"`
	MaxEntries     int64  `toml:"max_entries"`
	Replicas       int    `toml:"replicas"`
	Compress       bool   `toml:"compress"`
	UpdateExisting bool   `toml:"update_existing"`
}

type WebDAVConfiguration struct {
	Url string `toml:"url"`
}

type SFTPConfiguration struct {
	Url string `toml:"url"`
}

type S3Configuration struct {
	DirPath      string `toml:"path"`
	Endpoint     string `toml:"endpoint"`
	AccessKey    string `toml:"access_key"`
	SecretKey    string `toml:"secret"`
	SessionToken string `toml:"session_token"`
	Bucket       string `toml:"bucket"`
	UseSSL       bool   `toml:"use_ssl"`
}

type ObjectStoreConfiguration struct {
	Replicas   int    `toml:"replicas"`
	BucketName string `toml:"bucket"`
}

type SnapshotConfiguration struct {
	Enable    bool                     `toml:"enabled"`
	Interval  uint32                   `toml:"interval"`
	StoreType SnapshotStoreType        `toml:"store"`
	Nats      ObjectStoreConfiguration `toml:"nats"`
	S3        S3Configuration          `toml:"s3"`
	WebDAV    WebDAVConfiguration      `toml:"webdav"`
	SFTP      SFTPConfiguration        `toml:"sftp"`
}

type NATSConfiguration struct {
	URLs                 []string `toml:"urls"`
	SubjectPrefix        string   `toml:"subject_prefix"`
	StreamPrefix         string   `toml:"stream_prefix"`
	ServerConfigFile     string   `toml:"server_config"`
	SeedFile             string   `toml:"seed_file"`
	CredsUser            string   `toml:"user_name"`
	CredsPassword        string   `toml:"user_password"`
	CAFile               string   `toml:"ca_file"`
	CertFile             string   `toml:"cert_file"`
	KeyFile              string   `toml:"key_file"`
	BindAddress          string   `toml:"bind_address"`
	ConnectRetries       int      `toml:"connect_retries"`
	ReconnectWaitSeconds int      `toml:"reconnect_wait_seconds"`
}

type LoggingConfiguration struct {
	Verbose bool   `toml:"verbose"`
	Format  string `toml:"format"`
}

type PrometheusConfiguration struct {
	Bind      string `toml:"bind"`
	Enable    bool   `toml:"enable"`
	Namespace string `toml:"namespace"`
	Subsystem string `toml:"subsystem"`
}

type Configuration struct {
	SeqMapPath      string `toml:"seq_map_path"`
	DBPath          string `toml:"db_path"`
	NodeID          uint64 `toml:"node_id"`
	Publish         bool   `toml:"publish"`
	Replicate       bool   `toml:"replicate"`
	ScanMaxChanges  uint32 `toml:"scan_max_changes"`
	CleanupInterval uint32 `toml:"cleanup_interval"`
	SleepTimeout    uint32 `toml:"sleep_timeout"`
	PollingInterval uint32 `toml:"polling_interval"`

	Snapshot       SnapshotConfiguration       `toml:"snapshot"`
	ReplicationLog ReplicationLogConfiguration `toml:"replication_log"`
	NATS           NATSConfiguration           `toml:"nats"`
	Logging        LoggingConfiguration        `toml:"logging"`
	Prometheus     PrometheusConfiguration     `toml:"prometheus"`
}

var ConfigPathFlag = flag.String("config", "", "Path to configuration file")
var CleanupFlag = flag.Bool("cleanup", false, "Only cleanup harmonylite triggers and changelogs")
var SaveSnapshotFlag = flag.Bool("save-snapshot", false, "Only take snapshot and upload")
var ClusterAddrFlag = flag.String("cluster-addr", "", "Cluster listening address")
var ClusterPeersFlag = flag.String("cluster-peers", "", "Comma separated list of clusters")
var LeafServerFlag = flag.String("leaf-servers", "", "Comma separated list of leaf servers")
var ProfServer = flag.String("pprof", "", "PProf listening address")

var DataRootDir = os.TempDir()
var Config = &Configuration{
	SeqMapPath:      path.Join(DataRootDir, "seq-map.cbor"),
	DBPath:          path.Join(DataRootDir, "harmonylite.db"),
	NodeID:          0,
	Publish:         true,
	Replicate:       true,
	ScanMaxChanges:  512,
	CleanupInterval: 5000,
	SleepTimeout:    0,
	PollingInterval: 0,

	Snapshot: SnapshotConfiguration{
		Enable:    true,
		Interval:  0,
		StoreType: Nats,
		Nats: ObjectStoreConfiguration{
			Replicas: 1,
		},
		S3:     S3Configuration{},
		WebDAV: WebDAVConfiguration{},
		SFTP:   SFTPConfiguration{},
	},

	ReplicationLog: ReplicationLogConfiguration{
		Shards:         1,
		MaxEntries:     1024,
		Replicas:       1,
		Compress:       true,
		UpdateExisting: false,
	},

	NATS: NATSConfiguration{
		URLs:                 []string{},
		SubjectPrefix:        "harmonylite-change-log",
		StreamPrefix:         "harmonylite-changes",
		ServerConfigFile:     "",
		SeedFile:             "",
		CredsPassword:        "",
		CredsUser:            "",
		BindAddress:          ":-1",
		ConnectRetries:       5,
		ReconnectWaitSeconds: 2,
	},

	Logging: LoggingConfiguration{
		Verbose: false,
		Format:  "console",
	},

	Prometheus: PrometheusConfiguration{
		Bind:      ":3010",
		Enable:    false,
		Namespace: "harmonylite",
		Subsystem: "",
	},
}

func init() {
	id, err := machineid.ID()
	if err != nil {
		log.Warn().Err(err).Msg("⚠️⚠️⚠️ Unable to read machine ID from OS, generating random ID ⚠️⚠️⚠️")
		id = uuid.NewString()
	}

	hasher := fnv.New64()
	_, err = hasher.Write([]byte(id))
	if err != nil {
		panic(err)
	}

	Config.NodeID = hasher.Sum64()
}

func Load(filePath string) error {
	_, err := toml.DecodeFile(filePath, Config)
	if os.IsNotExist(err) {
		return nil
	}

	if err != nil {
		return err
	}

	DataRootDir, err = filepath.Abs(path.Dir(Config.DBPath))
	if err != nil {
		return err
	}

	if Config.SeqMapPath == "" {
		Config.SeqMapPath = path.Join(DataRootDir, "seq-map.cbor")
	}

	return nil
}

func (c *Configuration) SnapshotStorageType() SnapshotStoreType {
	return c.Snapshot.StoreType
}

func (c *Configuration) NodeName() string {
	return fmt.Sprintf("%s-%d", NodeNamePrefix, c.NodeID)
}

================
File: core/events.go
================
package core

import "github.com/fxamacker/cbor/v2"

var CBORTags = cbor.NewTagSet()

type ReplicableEvent[T any] interface {
	Wrap() (T, error)
	Unwrap() (T, error)
}

================
File: db/change_log_event_test.go
================
package db

import (
	"reflect"
	"testing"
	"time"

	"github.com/fxamacker/cbor/v2"
	"github.com/wongfei2009/harmonylite/core"
)

func TestChangeLogEvent_Wrap(t *testing.T) {
	// Create a test event with standard types
	event := ChangeLogEvent{
		Id:        123,
		Type:      "insert",
		TableName: "users",
		Row: map[string]any{
			"id":         1,
			"name":       "John Doe",
			"created_at": time.Date(2025, 3, 1, 12, 0, 0, 0, time.UTC),
			"active":     true,
			"score":      42.5,
		},
		tableInfo: []*ColumnInfo{
			{Name: "id", Type: "INTEGER", IsPrimaryKey: true},
			{Name: "name", Type: "TEXT", IsPrimaryKey: false},
			{Name: "created_at", Type: "TIMESTAMP", IsPrimaryKey: false},
			{Name: "active", Type: "BOOLEAN", IsPrimaryKey: false},
			{Name: "score", Type: "REAL", IsPrimaryKey: false},
		},
	}

	// Wrap the event
	wrapped, err := event.Wrap()
	if err != nil {
		t.Fatalf("Error wrapping event: %v", err)
	}

	// Check if the non-sensitive fields remain the same
	if wrapped.Id != event.Id {
		t.Errorf("Expected wrapped.Id = %d, got %d", event.Id, wrapped.Id)
	}
	if wrapped.Type != event.Type {
		t.Errorf("Expected wrapped.Type = %s, got %s", event.Type, wrapped.Type)
	}
	if wrapped.TableName != event.TableName {
		t.Errorf("Expected wrapped.TableName = %s, got %s", event.TableName, wrapped.TableName)
	}

	// Check if time fields are properly wrapped
	timeValue, ok := wrapped.Row["created_at"].(sensitiveTypeWrapper)
	if !ok {
		t.Errorf("Expected time value to be wrapped in sensitiveTypeWrapper")
	} else {
		originalTime := event.Row["created_at"].(time.Time)
		wrappedTime := timeValue.Time
		if !wrappedTime.Equal(originalTime) {
			t.Errorf("Expected wrapped time %v to equal original time %v", wrappedTime, originalTime)
		}
	}

	// Check if non-time fields are unchanged
	if wrapped.Row["id"] != event.Row["id"] {
		t.Errorf("Expected wrapped.Row[id] = %v, got %v", event.Row["id"], wrapped.Row["id"])
	}
	if wrapped.Row["name"] != event.Row["name"] {
		t.Errorf("Expected wrapped.Row[name] = %v, got %v", event.Row["name"], wrapped.Row["name"])
	}
	if wrapped.Row["active"] != event.Row["active"] {
		t.Errorf("Expected wrapped.Row[active] = %v, got %v", event.Row["active"], wrapped.Row["active"])
	}
	if wrapped.Row["score"] != event.Row["score"] {
		t.Errorf("Expected wrapped.Row[score] = %v, got %v", event.Row["score"], wrapped.Row["score"])
	}
}

func TestChangeLogEvent_Unwrap(t *testing.T) {
	// Create a wrapped test event
	originalTime := time.Date(2025, 3, 1, 12, 0, 0, 0, time.UTC)
	wrappedEvent := ChangeLogEvent{
		Id:        123,
		Type:      "insert",
		TableName: "users",
		Row: map[string]any{
			"id":         1,
			"name":       "John Doe",
			"created_at": sensitiveTypeWrapper{Time: &originalTime},
			"active":     true,
			"score":      42.5,
		},
		tableInfo: []*ColumnInfo{
			{Name: "id", Type: "INTEGER", IsPrimaryKey: true},
			{Name: "name", Type: "TEXT", IsPrimaryKey: false},
			{Name: "created_at", Type: "TIMESTAMP", IsPrimaryKey: false},
			{Name: "active", Type: "BOOLEAN", IsPrimaryKey: false},
			{Name: "score", Type: "REAL", IsPrimaryKey: false},
		},
	}

	// Unwrap the event
	unwrapped, err := wrappedEvent.Unwrap()
	if err != nil {
		t.Fatalf("Error unwrapping event: %v", err)
	}

	// Check if the non-sensitive fields remain the same
	if unwrapped.Id != wrappedEvent.Id {
		t.Errorf("Expected unwrapped.Id = %d, got %d", wrappedEvent.Id, unwrapped.Id)
	}
	if unwrapped.Type != wrappedEvent.Type {
		t.Errorf("Expected unwrapped.Type = %s, got %s", wrappedEvent.Type, unwrapped.Type)
	}
	if unwrapped.TableName != wrappedEvent.TableName {
		t.Errorf("Expected unwrapped.TableName = %s, got %s", wrappedEvent.TableName, unwrapped.TableName)
	}

	// Check if time fields are properly unwrapped
	switch unwrappedTime := unwrapped.Row["created_at"].(type) {
	case time.Time:
		if !unwrappedTime.Equal(originalTime) {
			t.Errorf("Expected unwrapped time %v to equal original time %v", unwrappedTime, originalTime)
		}
	case *time.Time:
		if !unwrappedTime.Equal(originalTime) {
			t.Errorf("Expected unwrapped time %v to equal original time %v", unwrappedTime, originalTime)
		}
	default:
		t.Errorf("Expected unwrapped time value to be time.Time or *time.Time, got %T", unwrapped.Row["created_at"])
	}

	// Check if non-time fields are unchanged
	if unwrapped.Row["id"] != wrappedEvent.Row["id"] {
		t.Errorf("Expected unwrapped.Row[id] = %v, got %v", wrappedEvent.Row["id"], unwrapped.Row["id"])
	}
	if unwrapped.Row["name"] != wrappedEvent.Row["name"] {
		t.Errorf("Expected unwrapped.Row[name] = %v, got %v", wrappedEvent.Row["name"], unwrapped.Row["name"])
	}
	if unwrapped.Row["active"] != wrappedEvent.Row["active"] {
		t.Errorf("Expected unwrapped.Row[active] = %v, got %v", wrappedEvent.Row["active"], unwrapped.Row["active"])
	}
	if unwrapped.Row["score"] != wrappedEvent.Row["score"] {
		t.Errorf("Expected unwrapped.Row[score] = %v, got %v", wrappedEvent.Row["score"], unwrapped.Row["score"])
	}
}

func TestChangeLogEvent_Hash(t *testing.T) {
	// Create events with the same data but different non-hash-relevant fields
	event1 := ChangeLogEvent{
		Id:        123,
		Type:      "insert",
		TableName: "users",
		Row: map[string]any{
			"id":    1,
			"name":  "John Doe",
			"email": "john@example.com",
		},
		tableInfo: []*ColumnInfo{
			{Name: "id", Type: "INTEGER", IsPrimaryKey: true},
			{Name: "name", Type: "TEXT", IsPrimaryKey: false},
			{Name: "email", Type: "TEXT", IsPrimaryKey: false},
		},
	}

	event2 := ChangeLogEvent{
		Id:        456,      // Different ID should not affect hash
		Type:      "update", // Different type should not affect hash
		TableName: "users",
		Row: map[string]any{
			"id":    1,                       // Same primary key
			"name":  "Different Name",        // Different non-PK value should not affect hash
			"email": "different@example.com", // Different non-PK value should not affect hash
		},
		tableInfo: []*ColumnInfo{
			{Name: "id", Type: "INTEGER", IsPrimaryKey: true},
			{Name: "name", Type: "TEXT", IsPrimaryKey: false},
			{Name: "email", Type: "TEXT", IsPrimaryKey: false},
		},
	}

	// Create an event with a different hash-relevant field
	event3 := ChangeLogEvent{
		Id:        789,
		Type:      "insert",
		TableName: "users",
		Row: map[string]any{
			"id":    2, // Different primary key should cause different hash
			"name":  "Jane Doe",
			"email": "jane@example.com",
		},
		tableInfo: []*ColumnInfo{
			{Name: "id", Type: "INTEGER", IsPrimaryKey: true},
			{Name: "name", Type: "TEXT", IsPrimaryKey: false},
			{Name: "email", Type: "TEXT", IsPrimaryKey: false},
		},
	}

	// Test with a different table name
	event4 := ChangeLogEvent{
		Id:        123,
		Type:      "insert",
		TableName: "profiles", // Different table name should cause different hash
		Row: map[string]any{
			"id":    1,
			"name":  "John Doe",
			"email": "john@example.com",
		},
		tableInfo: []*ColumnInfo{
			{Name: "id", Type: "INTEGER", IsPrimaryKey: true},
			{Name: "name", Type: "TEXT", IsPrimaryKey: false},
			{Name: "email", Type: "TEXT", IsPrimaryKey: false},
		},
	}

	// Calculate hashes
	hash1, err := event1.Hash()
	if err != nil {
		t.Fatalf("Error calculating hash for event1: %v", err)
	}

	hash2, err := event2.Hash()
	if err != nil {
		t.Fatalf("Error calculating hash for event2: %v", err)
	}

	hash3, err := event3.Hash()
	if err != nil {
		t.Fatalf("Error calculating hash for event3: %v", err)
	}

	hash4, err := event4.Hash()
	if err != nil {
		t.Fatalf("Error calculating hash for event4: %v", err)
	}

	// Verify hashes
	if hash1 != hash2 {
		t.Errorf("Expected hash1 == hash2, got %d != %d", hash1, hash2)
	}

	if hash1 == hash3 {
		t.Errorf("Expected hash1 != hash3, got %d == %d", hash1, hash3)
	}

	if hash1 == hash4 {
		t.Errorf("Expected hash1 != hash4, got %d == %d", hash1, hash4)
	}
}

func TestChangeLogEvent_MarshalUnmarshal(t *testing.T) {
	// Create an event with a time field
	originalTime := time.Date(2025, 3, 1, 12, 0, 0, 0, time.UTC)
	event := ChangeLogEvent{
		Id:        123,
		Type:      "insert",
		TableName: "users",
		Row: map[string]any{
			"id":         1,
			"name":       "John Doe",
			"created_at": originalTime,
		},
		tableInfo: []*ColumnInfo{
			{Name: "id", Type: "INTEGER", IsPrimaryKey: true},
			{Name: "name", Type: "TEXT", IsPrimaryKey: false},
			{Name: "created_at", Type: "TIMESTAMP", IsPrimaryKey: false},
		},
	}

	// Wrap and marshal the event
	wrapped, err := event.Wrap()
	if err != nil {
		t.Fatalf("Error wrapping event: %v", err)
	}

	em, err := cbor.EncOptions{}.EncModeWithTags(core.CBORTags)
	if err != nil {
		t.Fatalf("Error creating encoder: %v", err)
	}

	data, err := em.Marshal(wrapped)
	if err != nil {
		t.Fatalf("Error marshaling event: %v", err)
	}

	// Unmarshal back to an event
	var unmarshaled ChangeLogEvent
	dm, err := cbor.DecOptions{}.DecModeWithTags(core.CBORTags)
	if err != nil {
		t.Fatalf("Error creating decoder: %v", err)
	}

	err = dm.Unmarshal(data, &unmarshaled)
	if err != nil {
		t.Fatalf("Error unmarshaling event: %v", err)
	}

	// Verify the unmarshaled event
	if unmarshaled.Id != event.Id {
		t.Errorf("Expected unmarshaled.Id = %d, got %d", event.Id, unmarshaled.Id)
	}
	if unmarshaled.Type != event.Type {
		t.Errorf("Expected unmarshaled.Type = %s, got %s", event.Type, unmarshaled.Type)
	}
	if unmarshaled.TableName != event.TableName {
		t.Errorf("Expected unmarshaled.TableName = %s, got %s", event.TableName, unmarshaled.TableName)
	}

	// Check if time field was correctly marshaled and unmarshaled
	timeWrapper, ok := unmarshaled.Row["created_at"].(sensitiveTypeWrapper)
	if !ok {
		t.Errorf("Expected unmarshaled time to be a sensitiveTypeWrapper, got %T", unmarshaled.Row["created_at"])
	} else {
		if !timeWrapper.Time.Equal(originalTime) {
			t.Errorf("Expected unmarshaled time %v to equal original time %v", timeWrapper.Time, originalTime)
		}
	}

	// Unwrap the unmarshaled event
	unwrapped, err := unmarshaled.Unwrap()
	if err != nil {
		t.Fatalf("Error unwrapping unmarshaled event: %v", err)
	}

	// Check the unwrapped time
	switch unwrappedTime := unwrapped.Row["created_at"].(type) {
	case time.Time:
		if !unwrappedTime.Equal(originalTime) {
			t.Errorf("Expected unwrapped time %v to equal original time %v", unwrappedTime, originalTime)
		}
	case *time.Time:
		if !unwrappedTime.Equal(originalTime) {
			t.Errorf("Expected unwrapped time %v to equal original time %v", unwrappedTime, originalTime)
		}
	default:
		t.Errorf("Expected unwrapped time to be a time.Time or *time.Time, got %T", unwrapped.Row["created_at"])
	}
}

func TestSensitiveTypeWrapper_GetValue(t *testing.T) {
	// Test with time
	now := time.Now()
	wrapper := sensitiveTypeWrapper{
		Time: &now,
	}

	val := wrapper.GetValue()

	// The return type could be either time.Time or *time.Time depending on implementation
	switch timeVal := val.(type) {
	case time.Time:
		if !timeVal.Equal(now) {
			t.Errorf("Expected GetValue() to return %v, got %v", now, timeVal)
		}
	case *time.Time:
		if !timeVal.Equal(now) {
			t.Errorf("Expected GetValue() to return %v, got %v", now, timeVal)
		}
	default:
		t.Errorf("Expected GetValue() to return time.Time or *time.Time, got %T", val)
	}

	// Test with nil
	wrapper = sensitiveTypeWrapper{
		Time: nil,
	}

	val = wrapper.GetValue()
	// Don't directly compare with nil as the actual value might be
	// a typed nil which doesn't equal untyped nil in Go
	if val != nil && val != (*time.Time)(nil) {
		t.Errorf("Expected GetValue() to return nil or nil pointer, got %v (type: %T)", val, val)
	}
}

func TestChangeLogEvent_getSortedPKColumns(t *testing.T) {
	// Create an event with multiple primary keys in unsorted order
	event := ChangeLogEvent{
		TableName: "test_table",
		tableInfo: []*ColumnInfo{
			{Name: "c", Type: "INTEGER", IsPrimaryKey: true, PrimaryKeyIndex: 3},
			{Name: "a", Type: "TEXT", IsPrimaryKey: true, PrimaryKeyIndex: 1},
			{Name: "d", Type: "INTEGER", IsPrimaryKey: false},
			{Name: "b", Type: "TEXT", IsPrimaryKey: true, PrimaryKeyIndex: 2},
		},
	}

	// Get sorted primary key columns
	columns := event.getSortedPKColumns()

	// Expected result: [a, b, c] (alphabetically sorted)
	expected := []string{"a", "b", "c"}
	if !reflect.DeepEqual(columns, expected) {
		t.Errorf("Expected getSortedPKColumns() to return %v, got %v", expected, columns)
	}

	// Test cache by modifying the table info (should still use cached result)
	event.tableInfo = []*ColumnInfo{
		{Name: "z", Type: "INTEGER", IsPrimaryKey: true},
		{Name: "y", Type: "TEXT", IsPrimaryKey: true},
	}

	columns = event.getSortedPKColumns()
	if !reflect.DeepEqual(columns, expected) {
		t.Errorf("Expected getSortedPKColumns() to return cached result %v, got %v", expected, columns)
	}

	// Clear cache and test again
	tablePKColumnsLock.Lock()
	delete(tablePKColumnsCache, event.TableName)
	tablePKColumnsLock.Unlock()

	columns = event.getSortedPKColumns()
	expectedAfterCacheClear := []string{"y", "z"}
	if !reflect.DeepEqual(columns, expectedAfterCacheClear) {
		t.Errorf("Expected getSortedPKColumns() to return %v after cache clear, got %v", expectedAfterCacheClear, columns)
	}
}

================
File: db/change_log_event.go
================
package db

import (
	"hash/fnv"
	"reflect"
	"sort"
	"sync"
	"time"

	"github.com/fxamacker/cbor/v2"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/core"
)

var tablePKColumnsCache = make(map[string][]string)
var tablePKColumnsLock = sync.RWMutex{}

type sensitiveTypeWrapper struct {
	Time *time.Time `cbor:"1,keyasint,omitempty"`
}

type ChangeLogEvent struct {
	Id        int64
	Type      string
	TableName string
	Row       map[string]any
	tableInfo []*ColumnInfo `cbor:"-"`
}

func init() {
	err := core.CBORTags.Add(
		cbor.TagOptions{
			DecTag: cbor.DecTagRequired,
			EncTag: cbor.EncTagRequired,
		},
		reflect.TypeOf(sensitiveTypeWrapper{}),
		32,
	)

	log.Panic().Err(err)
}

func (s sensitiveTypeWrapper) GetValue() any {
	// Right now only sensitive value is Time
	return s.Time
}

func (e ChangeLogEvent) Wrap() (ChangeLogEvent, error) {
	return e.prepare(), nil
}

func (e ChangeLogEvent) Unwrap() (ChangeLogEvent, error) {
	ret := ChangeLogEvent{
		Id:        e.Id,
		TableName: e.TableName,
		Type:      e.Type,
		Row:       map[string]any{},
		tableInfo: e.tableInfo,
	}

	for k, v := range e.Row {
		if st, ok := v.(sensitiveTypeWrapper); ok {
			ret.Row[k] = st.GetValue()
			continue
		}

		ret.Row[k] = v
	}

	return ret, nil
}

func (e ChangeLogEvent) Hash() (uint64, error) {
	hasher := fnv.New64()
	enc := cbor.NewEncoder(hasher)
	err := enc.StartIndefiniteArray()
	if err != nil {
		return 0, err
	}

	err = enc.Encode(e.TableName)
	if err != nil {
		return 0, err
	}

	pkColumns := e.getSortedPKColumns()
	for _, pk := range pkColumns {
		err = enc.Encode([]any{pk, e.Row[pk]})
		if err != nil {
			return 0, err
		}
	}

	err = enc.EndIndefinite()
	if err != nil {
		return 0, err
	}

	return hasher.Sum64(), nil
}

func (e ChangeLogEvent) getSortedPKColumns() []string {
	tablePKColumnsLock.RLock()

	if values, found := tablePKColumnsCache[e.TableName]; found {
		tablePKColumnsLock.RUnlock()
		return values
	}
	tablePKColumnsLock.RUnlock()

	pkColumns := make([]string, 0, len(e.tableInfo))
	for _, itm := range e.tableInfo {
		if itm.IsPrimaryKey {
			pkColumns = append(pkColumns, itm.Name)
		}
	}
	sort.Strings(pkColumns)

	tablePKColumnsLock.Lock()
	defer tablePKColumnsLock.Unlock()

	tablePKColumnsCache[e.TableName] = pkColumns
	return pkColumns
}

func (e ChangeLogEvent) prepare() ChangeLogEvent {
	needsTransform := false
	preparedRow := map[string]any{}
	for k, v := range e.Row {
		if t, ok := v.(time.Time); ok {
			preparedRow[k] = sensitiveTypeWrapper{Time: &t}
			needsTransform = true
		} else {
			preparedRow[k] = v
		}
	}

	if !needsTransform {
		return e
	}

	return ChangeLogEvent{
		Id:        e.Id,
		Type:      e.Type,
		TableName: e.TableName,
		Row:       preparedRow,
		tableInfo: e.tableInfo,
	}
}

================
File: db/change_log.go
================
package db

import (
	"bytes"
	"context"
	"database/sql"
	"errors"
	"fmt"
	"regexp"
	"strings"
	"text/template"
	"time"

	"github.com/wongfei2009/harmonylite/cfg"
	"github.com/wongfei2009/harmonylite/utils"

	_ "embed"

	"github.com/doug-martin/goqu/v9"
	"github.com/fsnotify/fsnotify"
	"github.com/rs/zerolog/log"
	"github.com/samber/lo"
)

var ErrNoTableMapping = errors.New("no table mapping found")
var ErrLogNotReadyToPublish = errors.New("not ready to publish changes")
var ErrEndOfWatch = errors.New("watching event finished")

//go:embed table_change_log_script.tmpl
var tableChangeLogScriptTemplate string

//go:embed global_change_log_script.tmpl
var globalChangeLogScriptTemplate string
var tableChangeLogTpl *template.Template
var globalChangeLogTpl *template.Template

var spaceStripper = regexp.MustCompile(`\n\s+`)

type ChangeLogState = int16

const (
	Pending   ChangeLogState = 0
	Published ChangeLogState = 1
	Failed    ChangeLogState = -1
)
const changeLogName = "change_log"
const upsertQuery = `INSERT OR REPLACE INTO %s(%s) VALUES (%s)`

type globalChangeLogTemplateData struct {
	Prefix string
}

type triggerTemplateData struct {
	Prefix    string
	TableName string
	Columns   []*ColumnInfo
	Triggers  map[string]string
}

type globalChangeLogEntry struct {
	Id            int64  `db:"id"`
	ChangeTableId int64  `db:"change_table_id"`
	TableName     string `db:"table_name"`
}

type changeLogEntry struct {
	Id    int64  `db:"id"`
	Type  string `db:"type"`
	State string `db:"state"`
}

func init() {
	tableChangeLogTpl = template.Must(
		template.New("tableChangeLogScriptTemplate").Parse(tableChangeLogScriptTemplate),
	)

	globalChangeLogTpl = template.Must(
		template.New("globalChangeLogScriptTemplate").Parse(globalChangeLogScriptTemplate),
	)
}

func (conn *SqliteStreamDB) Replicate(event *ChangeLogEvent) error {
	if err := conn.consumeReplicationEvent(event); err != nil {
		return err
	}
	return nil
}

func (conn *SqliteStreamDB) CleanupChangeLogs(beforeTime time.Time) (int64, error) {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return 0, err
	}
	defer sqlConn.Return()

	total := int64(0)
	for name := range conn.watchTablesSchema {
		metaTableName := conn.metaTable(name, changeLogName)
		rs, err := sqlConn.DB().Delete(metaTableName).
			Where(
				goqu.C("state").Eq(Published),
				goqu.C("created_at").Lte(beforeTime.UnixMilli()),
			).
			Prepared(true).
			Executor().
			Exec()

		if err != nil {
			return 0, err
		}

		count, err := rs.RowsAffected()
		if err != nil {
			return 0, err
		}

		total += count
	}

	return total, nil
}

func (conn *SqliteStreamDB) metaTable(tableName string, name string) string {
	return conn.prefix + tableName + "_" + name
}

func (conn *SqliteStreamDB) globalMetaTable() string {
	return conn.prefix + "_change_log_global"
}

func (conn *SqliteStreamDB) globalCDCScript() (string, error) {
	buf := new(bytes.Buffer)
	err := globalChangeLogTpl.Execute(buf, &globalChangeLogTemplateData{
		Prefix: conn.prefix,
	})

	if err != nil {
		return "", err
	}

	return spaceStripper.ReplaceAllString(buf.String(), "\n    "), nil
}

func (conn *SqliteStreamDB) tableCDCScriptFor(tableName string) (string, error) {
	columns, ok := conn.watchTablesSchema[tableName]
	if !ok {
		return "", errors.New("table info not found")
	}

	buf := new(bytes.Buffer)
	err := tableChangeLogTpl.Execute(buf, &triggerTemplateData{
		Prefix:    conn.prefix,
		Triggers:  map[string]string{"insert": "NEW", "update": "NEW", "delete": "OLD"},
		Columns:   columns,
		TableName: tableName,
	})

	if err != nil {
		return "", err
	}

	return spaceStripper.ReplaceAllString(buf.String(), "\n    "), nil
}

func (conn *SqliteStreamDB) consumeReplicationEvent(event *ChangeLogEvent) error {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return err
	}
	defer sqlConn.Return()

	return sqlConn.DB().WithTx(func(tnx *goqu.TxDatabase) error {
		primaryKeyMap := conn.getPrimaryKeyMap(event)
		if primaryKeyMap == nil {
			return ErrNoTableMapping
		}

		logEv := log.Debug().
			Int64("event_id", event.Id).
			Str("type", event.Type)

		for k, v := range primaryKeyMap {
			logEv = logEv.Str(event.TableName+"."+k, fmt.Sprintf("%v", v))
		}

		logEv.Send()

		return replicateRow(tnx, event, primaryKeyMap)
	})
}

func (conn *SqliteStreamDB) getPrimaryKeyMap(event *ChangeLogEvent) map[string]any {
	ret := make(map[string]any)
	tableColsSchema, ok := conn.watchTablesSchema[event.TableName]
	if !ok {
		return nil
	}

	for _, col := range tableColsSchema {
		if col.IsPrimaryKey {
			ret[col.Name] = event.Row[col.Name]
		}
	}

	return ret
}

func (conn *SqliteStreamDB) initGlobalChangeLog() error {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return err
	}
	defer sqlConn.Return()

	script, err := conn.globalCDCScript()
	if err != nil {
		return err
	}

	log.Info().Msg("Creating global change log table")
	_, err = sqlConn.DB().Exec(script)
	if err != nil {
		return err
	}

	return nil
}

func (conn *SqliteStreamDB) initTriggers(tableName string) error {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return err
	}
	defer sqlConn.Return()

	name := strings.TrimSpace(tableName)
	if strings.HasPrefix(name, "sqlite_") || strings.HasPrefix(name, conn.prefix) {
		return fmt.Errorf("invalid table to watch %s", tableName)
	}

	script, err := conn.tableCDCScriptFor(name)
	if err != nil {
		log.Error().Err(err).Msg("Failed to prepare CDC statement")
		return err
	}

	log.Info().Msg(fmt.Sprintf("Creating trigger for %v", name))
	_, err = sqlConn.DB().Exec(script)
	if err != nil {
		return err
	}

	return nil
}

func (conn *SqliteStreamDB) filterChangesTo(changed chan fsnotify.Event, watcher *fsnotify.Watcher) {
	for {
		select {
		case ev, ok := <-watcher.Events:
			if !ok {
				close(changed)
				return
			}

			if ev.Op == fsnotify.Chmod {
				continue
			}

			changed <- ev
		}
	}
}

func (conn *SqliteStreamDB) watchChanges(watcher *fsnotify.Watcher, path string) {
	shmPath := path + "-shm"
	walPath := path + "-wal"

	errDB := watcher.Add(path)
	errShm := watcher.Add(shmPath)
	errWal := watcher.Add(walPath)
	dbChanged := make(chan fsnotify.Event)

	tickerDur := time.Duration(cfg.Config.PollingInterval) * time.Millisecond
	changeLogTicker := utils.NewTimeoutPublisher(tickerDur)

	// Publish change logs for any residual change logs before starting watcher
	conn.publishChangeLog()
	go conn.filterChangesTo(dbChanged, watcher)

	for {
		changeLogTicker.Reset()

		err := conn.WithReadTx(func(_tx *sql.Tx) error {
			select {
			case ev, ok := <-dbChanged:
				if !ok {
					return ErrEndOfWatch
				}

				log.Debug().Int("change", int(ev.Op)).Msg("Change detected")
				conn.publishChangeLog()
			case <-changeLogTicker.Channel():
				log.Debug().Dur("timeout", tickerDur).Msg("Change polling timeout")
				conn.publishChangeLog()
			}

			return nil
		})

		if err != nil {
			log.Warn().Err(err).Msg("Error watching changes; trying to resubscribe...")
			errDB = watcher.Add(path)
			errShm = watcher.Add(shmPath)
			errWal = watcher.Add(walPath)
		}

		if errDB != nil {
			errDB = watcher.Add(path)
		}

		if errShm != nil {
			errShm = watcher.Add(shmPath)
		}

		if errWal != nil {
			errWal = watcher.Add(walPath)
		}
	}
}

func (conn *SqliteStreamDB) getGlobalChanges(limit uint32) ([]globalChangeLogEntry, error) {
	sw := utils.NewStopWatch("scan_changes")
	defer sw.Log(log.Debug(), conn.stats.scanChanges)

	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return nil, err
	}
	defer sqlConn.Return()

	var entries []globalChangeLogEntry
	err = sqlConn.DB().
		From(conn.globalMetaTable()).
		Order(goqu.I("id").Asc()).
		Limit(uint(limit)).
		ScanStructs(&entries)

	if err != nil {
		return nil, err
	}
	return entries, nil
}

func (conn *SqliteStreamDB) countChanges() (int64, error) {
	sw := utils.NewStopWatch("count_changes")
	defer sw.Log(log.Debug(), conn.stats.countChanges)

	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return -1, err
	}
	defer sqlConn.Return()

	return sqlConn.DB().
		From(conn.globalMetaTable()).
		Count()
}

func (conn *SqliteStreamDB) publishChangeLog() {
	if !conn.publishLock.TryLock() {
		log.Warn().Msg("Publish in progress skipping...")
		return
	}
	defer conn.publishLock.Unlock()

	cnt, err := conn.countChanges()
	if err != nil {
		log.Error().Err(err).Msg("Unable to count global changes")
		return
	}

	conn.stats.pendingPublish.Set(float64(cnt))
	if cnt <= 0 {
		log.Debug().Msg("no new rows")
		return
	}

	changes, err := conn.getGlobalChanges(cfg.Config.ScanMaxChanges)
	if err != nil {
		log.Error().Err(err).Msg("Unable to scan global changes")
		return
	}

	if len(changes) < 0 {
		return
	}

	for _, change := range changes {
		logEntry := changeLogEntry{}
		found := false
		found, err = conn.getChangeEntry(&logEntry, change)

		if err != nil {
			log.Error().Err(err).Msg("Error scanning last row ID")
			return
		}

		if !found {
			log.Panic().
				Str("table", change.TableName).
				Int64("id", change.ChangeTableId).
				Msg("Global change log row not found in corresponding table")
			return
		}

		err = conn.consumeChangeLogs(change.TableName, []*changeLogEntry{&logEntry})
		if err != nil {
			if errors.Is(err, ErrLogNotReadyToPublish) || errors.Is(err, context.Canceled) {
				break
			}

			log.Error().Err(err).Msg("Unable to consume changes")
		}

		err = conn.markChangePublished(change)
		if err != nil {
			log.Error().Err(err).Msg("Unable to cleanup change log")
		}

		conn.stats.published.Inc()
	}
}

func (conn *SqliteStreamDB) markChangePublished(change globalChangeLogEntry) error {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return err
	}
	defer sqlConn.Return()

	return sqlConn.DB().WithTx(func(tx *goqu.TxDatabase) error {
		_, err = tx.Update(conn.metaTable(change.TableName, changeLogName)).
			Set(goqu.Record{"state": Published}).
			Where(goqu.Ex{"id": change.ChangeTableId}).
			Prepared(true).
			Executor().
			Exec()

		if err != nil {
			return err
		}

		_, err = tx.Delete(conn.globalMetaTable()).
			Where(goqu.C("id").Eq(change.Id)).
			Prepared(true).
			Executor().
			Exec()

		if err != nil {
			return err
		}

		return nil
	})
}

func (conn *SqliteStreamDB) getChangeEntry(entry *changeLogEntry, change globalChangeLogEntry) (bool, error) {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return false, err
	}
	defer sqlConn.Return()

	return sqlConn.DB().Select("id", "type", "state").
		From(conn.metaTable(change.TableName, changeLogName)).
		Where(
			goqu.C("state").Eq(Pending),
			goqu.C("id").Eq(change.ChangeTableId),
		).
		Prepared(true).
		ScanStruct(entry)
}

func (conn *SqliteStreamDB) consumeChangeLogs(tableName string, changes []*changeLogEntry) error {
	rowIds := lo.Map(changes, func(e *changeLogEntry, i int) int64 {
		return e.Id
	})

	changeMap := lo.Associate(
		changes,
		func(l *changeLogEntry) (int64, *changeLogEntry) {
			return l.Id, l
		},
	)

	idColumnName := conn.prefix + "change_log_id"
	rawRows, err := conn.fetchChangeRows(tableName, idColumnName, rowIds)
	if err != nil {
		return err
	}

	rows := &EnhancedRows{rawRows}
	defer rows.Finalize()

	for rows.Next() {
		row, err := rows.fetchRow()
		if err != nil {
			return err
		}

		changeRowID := row[idColumnName].(int64)
		changeRow := changeMap[changeRowID]
		delete(row, idColumnName)

		logger := log.With().
			Int64("rowid", changeRowID).
			Str("table", tableName).
			Str("type", changeRow.Type).
			Logger()

		if conn.OnChange != nil {
			err = conn.OnChange(&ChangeLogEvent{
				Id:        changeRowID,
				Type:      changeRow.Type,
				TableName: tableName,
				Row:       row,
				tableInfo: conn.watchTablesSchema[tableName],
			})

			if err != nil {
				if err == ErrLogNotReadyToPublish || err == context.Canceled {
					return err
				}

				logger.Error().Err(err).Msg("Failed to publish for table " + tableName)
				return err
			}
		}
	}

	return nil
}

func (conn *SqliteStreamDB) fetchChangeRows(
	tableName string,
	idColumnName string,
	rowIds []int64,
) (*sql.Rows, error) {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return nil, err
	}
	defer sqlConn.Return()

	columnNames := make([]any, 0)
	tableCols := conn.watchTablesSchema[tableName]
	columnNames = append(columnNames, goqu.C("id").As(idColumnName))
	for _, col := range tableCols {
		columnNames = append(columnNames, goqu.C("val_"+col.Name).As(col.Name))
	}

	query, params, err := sqlConn.DB().From(conn.metaTable(tableName, changeLogName)).
		Select(columnNames...).
		Where(goqu.C("id").In(rowIds)).
		Prepared(true).
		ToSQL()
	if err != nil {
		return nil, err
	}

	rawRows, err := sqlConn.DB().Query(query, params...)
	if err != nil {
		return nil, err
	}

	return rawRows, nil
}

func replicateRow(tx *goqu.TxDatabase, event *ChangeLogEvent, pkMap map[string]any) error {
	if event.Type == "insert" || event.Type == "update" {
		return replicateUpsert(tx, event, pkMap)
	}

	if event.Type == "delete" {
		return replicateDelete(tx, event, pkMap)
	}

	return fmt.Errorf("invalid operation type %s", event.Type)
}

func replicateUpsert(tx *goqu.TxDatabase, event *ChangeLogEvent, _ map[string]any) error {
	columnNames := make([]string, 0, len(event.Row))
	columnValues := make([]any, 0, len(event.Row))
	for k, v := range event.Row {
		columnNames = append(columnNames, k)
		columnValues = append(columnValues, v)
	}

	query := fmt.Sprintf(
		upsertQuery,
		event.TableName,
		strings.Join(columnNames, ", "),
		strings.Join(strings.Split(strings.Repeat("?", len(columnNames)), ""), ", "),
	)

	stmt, err := tx.Prepare(query)
	if err != nil {
		return err
	}

	_, err = stmt.Exec(columnValues...)
	return err
}

func replicateDelete(tx *goqu.TxDatabase, event *ChangeLogEvent, pkMap map[string]any) error {
	_, err := tx.Delete(event.TableName).
		Where(goqu.Ex(pkMap)).
		Prepared(true).
		Executor().
		Exec()

	return err
}

================
File: db/cleanup.go
================
package db

import (
	"fmt"

	"github.com/doug-martin/goqu/v9"
	"github.com/rs/zerolog/log"
)

const deleteTriggerQuery = `DROP TRIGGER IF EXISTS %s`
const deleteHarmonyLiteTables = `DROP TABLE IF EXISTS %s;`

func removeHarmonyLiteTriggers(conn *goqu.Database, prefix string) error {
	triggers := make([]string, 0)
	err := conn.
		Select("name").
		From("sqlite_master").
		Where(goqu.C("type").Eq("trigger"), goqu.C("name").Like(prefix+"%")).
		Prepared(true).
		ScanVals(&triggers)
	if err != nil {
		return err
	}

	for _, name := range triggers {
		query := fmt.Sprintf(deleteTriggerQuery, name)
		_, err = conn.Exec(query)
		if err != nil {
			log.Error().Err(err).Str("name", name).Msg("Unable to delete trigger")
			return err
		}
	}

	return nil
}

func removeHarmonyLiteTables(conn *goqu.Database, prefix string) error {
	tables := make([]string, 0)
	err := conn.
		Select("name").
		From("sqlite_master").
		Where(goqu.C("type").Eq("table"), goqu.C("name").Like(prefix+"%")).
		Prepared(true).
		ScanVals(&tables)
	if err != nil {
		return err
	}

	for _, name := range tables {
		query := fmt.Sprintf(deleteHarmonyLiteTables, name)
		_, err = conn.Exec(query)
		if err != nil {
			log.Error().Err(err).Msg("Unable to delete harmonylite tables")
			return err
		}
	}

	return nil
}

================
File: db/global_change_log_script.tmpl
================
{{$GlobalChangeLogTableName := (printf "%s_change_log_global" .Prefix)}}

CREATE TABLE IF NOT EXISTS {{$GlobalChangeLogTableName}} (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    change_table_id INTEGER,
    table_name      TEXT
);

================
File: db/sqlite.go
================
package db

import (
	"context"
	"database/sql"
	"database/sql/driver"
	"fmt"
	"io"
	"os"
	"sync"
	"time"

	"github.com/doug-martin/goqu/v9"
	"github.com/fsnotify/fsnotify"
	"github.com/mattn/go-sqlite3"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/pool"
	"github.com/wongfei2009/harmonylite/telemetry"
)

const snapshotTransactionMode = "exclusive"

var PoolSize = 4
var HarmonyLitePrefix = "__harmonylite__"

type statsSqliteStreamDB struct {
	published      telemetry.Counter
	pendingPublish telemetry.Gauge
	countChanges   telemetry.Histogram
	scanChanges    telemetry.Histogram
}

type SqliteStreamDB struct {
	OnChange      func(event *ChangeLogEvent) error
	pool          *pool.SQLitePool
	rawConnection *sqlite3.SQLiteConn
	publishLock   *sync.Mutex

	dbPath            string
	prefix            string
	watchTablesSchema map[string][]*ColumnInfo
	stats             *statsSqliteStreamDB
}

type ColumnInfo struct {
	Name            string `db:"name"`
	Type            string `db:"type"`
	NotNull         bool   `db:"notnull"`
	DefaultValue    any    `db:"dflt_value"`
	PrimaryKeyIndex int    `db:"pk"`
	IsPrimaryKey    bool
}

func RestoreFrom(destPath, bkFilePath string) error {
	dnsTpl := "%s?_journal_mode=WAL&_foreign_keys=false&_busy_timeout=30000&_sync=FULL&_txlock=%s"
	dns := fmt.Sprintf(dnsTpl, destPath, snapshotTransactionMode)
	destDB, dest, err := pool.OpenRaw(dns)
	if err != nil {
		return err
	}
	defer dest.Close()

	dns = fmt.Sprintf(dnsTpl, bkFilePath, snapshotTransactionMode)
	srcDB, src, err := pool.OpenRaw(dns)
	if err != nil {
		return err
	}
	defer src.Close()

	dgSQL := goqu.New("sqlite", destDB)
	sgSQL := goqu.New("sqlite", srcDB)

	// Source locking is required so that any lock related metadata is mirrored in destination
	// Transacting on both src and dest in immediate mode makes sure nobody
	// else is modifying or interacting with DB
	err = sgSQL.WithTx(func(dtx *goqu.TxDatabase) error {
		return dgSQL.WithTx(func(_ *goqu.TxDatabase) error {
			err = copyFile(destPath, bkFilePath)
			if err != nil {
				return err
			}

			err = copyFile(destPath+"-shm", bkFilePath+"-shm")
			if err != nil {
				return err
			}

			err = copyFile(destPath+"-wal", bkFilePath+"-wal")
			if err != nil {
				return err
			}

			return nil
		})
	})

	if err != nil {
		return err
	}

	err = performCheckpoint(dgSQL)
	if err != nil {
		return err
	}

	return nil
}

func GetAllDBTables(path string) ([]string, error) {
	connectionStr := fmt.Sprintf("%s?_journal_mode=WAL", path)
	conn, rawConn, err := pool.OpenRaw(connectionStr)
	if err != nil {
		return nil, err
	}
	defer rawConn.Close()
	defer conn.Close()

	gSQL := goqu.New("sqlite", conn)
	names := make([]string, 0)
	err = gSQL.WithTx(func(tx *goqu.TxDatabase) error {
		return listDBTables(&names, tx)
	})

	if err != nil {
		return nil, err
	}

	return names, nil
}

func OpenStreamDB(path string) (*SqliteStreamDB, error) {
	dbPool, err := pool.NewSQLitePool(fmt.Sprintf("%s?_journal_mode=WAL", path), PoolSize, true)
	if err != nil {
		return nil, err
	}

	conn, err := dbPool.Borrow()
	if err != nil {
		return nil, err
	}
	defer conn.Return()

	err = performCheckpoint(conn.DB())
	if err != nil {
		return nil, err
	}

	ret := &SqliteStreamDB{
		pool:              dbPool,
		dbPath:            path,
		prefix:            HarmonyLitePrefix,
		publishLock:       &sync.Mutex{},
		watchTablesSchema: map[string][]*ColumnInfo{},
		stats: &statsSqliteStreamDB{
			published:      telemetry.NewCounter("published", "number of rows published"),
			pendingPublish: telemetry.NewGauge("pending_publish", "rows pending publishing"),
			countChanges:   telemetry.NewHistogram("count_changes", "latency counting changes in microseconds"),
			scanChanges:    telemetry.NewHistogram("scan_changes", "latency scanning change rows in DB"),
		},
	}

	return ret, nil
}

func (conn *SqliteStreamDB) InstallCDC(tables []string) error {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return err
	}
	defer sqlConn.Return()

	err = sqlConn.DB().WithTx(func(tx *goqu.TxDatabase) error {
		for _, n := range tables {
			colInfo, err := getTableInfo(tx, n)
			if err != nil {
				return err
			}

			conn.watchTablesSchema[n] = colInfo
		}

		return nil
	})
	if err != nil {
		return err
	}

	err = conn.installChangeLogTriggers()
	if err != nil {
		return err
	}

	watcher, err := fsnotify.NewWatcher()
	if err != nil {
		return err
	}

	go conn.watchChanges(watcher, conn.dbPath)
	return nil
}

func (conn *SqliteStreamDB) RemoveCDC(tables bool) error {
	sqlConn, err := conn.pool.Borrow()
	if err != nil {
		return err
	}
	defer sqlConn.Return()

	log.Info().Msg("Uninstalling all CDC triggers...")
	err = removeHarmonyLiteTriggers(sqlConn.DB(), conn.prefix)
	if err != nil {
		return err
	}

	if tables {
		return removeHarmonyLiteTables(sqlConn.DB(), conn.prefix)
	}

	return nil
}

func (conn *SqliteStreamDB) installChangeLogTriggers() error {
	if err := conn.initGlobalChangeLog(); err != nil {
		return err
	}

	for tableName := range conn.watchTablesSchema {
		err := conn.initTriggers(tableName)
		if err != nil {
			return err
		}
	}
	return nil
}

func getTableInfo(tx *goqu.TxDatabase, table string) ([]*ColumnInfo, error) {
	query := "SELECT name, type, `notnull`, dflt_value, pk FROM pragma_table_info(?)"
	stmt, err := tx.Prepare(query)
	if err != nil {
		return nil, err
	}

	rows, err := stmt.Query(table)
	if err != nil {
		return nil, err
	}

	tableInfo := make([]*ColumnInfo, 0)
	hasPrimaryKey := false
	for rows.Next() {
		if rows.Err() != nil {
			return nil, rows.Err()
		}

		c := ColumnInfo{}
		err = rows.Scan(&c.Name, &c.Type, &c.NotNull, &c.DefaultValue, &c.PrimaryKeyIndex)
		if err != nil {
			return nil, err
		}

		c.IsPrimaryKey = c.PrimaryKeyIndex > 0

		if c.IsPrimaryKey {
			hasPrimaryKey = true
		}

		tableInfo = append(tableInfo, &c)
	}

	if !hasPrimaryKey {
		tableInfo = append(tableInfo, &ColumnInfo{
			Name:         "rowid",
			IsPrimaryKey: true,
			Type:         "INT",
			NotNull:      true,
			DefaultValue: nil,
		})
	}

	return tableInfo, nil
}

func (conn *SqliteStreamDB) BackupTo(bkFilePath string) error {
	sqlDB, rawDB, err := pool.OpenRaw(fmt.Sprintf("%s?mode=ro&_foreign_keys=false&_journal_mode=WAL", conn.dbPath))
	if err != nil {
		return err
	}
	defer sqlDB.Close()
	defer rawDB.Close()

	_, err = rawDB.Exec("VACUUM main INTO ?;", []driver.Value{bkFilePath})
	if err != nil {
		return err
	}

	err = rawDB.Close()
	if err != nil {
		return err
	}

	err = sqlDB.Close()
	if err != nil {
		return err
	}

	// Now since we have separate copy of DB we don't need to deal with WAL journals or foreign keys
	// We need to remove all the harmonylite specific tables, triggers, and vacuum out the junk.
	sqlDB, rawDB, err = pool.OpenRaw(fmt.Sprintf("%s?_foreign_keys=false&_journal_mode=TRUNCATE", bkFilePath))
	if err != nil {
		return err
	}

	gSQL := goqu.New("sqlite", sqlDB)
	err = removeHarmonyLiteTriggers(gSQL, conn.prefix)
	if err != nil {
		return err
	}

	err = removeHarmonyLiteTables(gSQL, conn.prefix)
	if err != nil {
		return err
	}

	_, err = gSQL.Exec("VACUUM;")
	if err != nil {
		return err
	}

	return nil
}

func (conn *SqliteStreamDB) GetRawConnection() *sqlite3.SQLiteConn {
	return conn.rawConnection
}

func (conn *SqliteStreamDB) GetPath() string {
	return conn.dbPath
}

func (conn *SqliteStreamDB) WithReadTx(cb func(tx *sql.Tx) error) error {
	var tx *sql.Tx = nil
	db, _, err := pool.OpenRaw(fmt.Sprintf("%s?_journal_mode=WAL", conn.dbPath))
	if err != nil {
		return err
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer func() {
		if r := recover(); r != nil {
			log.Error().Any("recover", r).Msg("Recovered read transaction")
		}

		if tx != nil {
			err = tx.Rollback()
			if err != nil {
				log.Error().Err(err).Msg("Error performing read transaction")
			}
		}

		db.Close()
		cancel()
	}()

	tx, err = db.BeginTx(ctx, &sql.TxOptions{ReadOnly: true})
	return cb(tx)
}

func copyFile(toPath, fromPath string) error {
	fi, err := os.OpenFile(fromPath, os.O_RDWR, 0)
	if err != nil {
		return err
	}
	defer fi.Close()

	fo, err := os.OpenFile(toPath, os.O_CREATE|os.O_WRONLY|os.O_TRUNC|os.O_SYNC, 0)
	if err != nil {
		return err
	}
	defer fo.Close()

	bytesWritten, err := io.Copy(fo, fi)
	log.Debug().
		Int64("bytes", bytesWritten).
		Str("from", fromPath).
		Str("to", toPath).
		Msg("File copied...")
	return err
}

func listDBTables(names *[]string, gSQL *goqu.TxDatabase) error {
	err := gSQL.Select("name").From("sqlite_schema").Where(
		goqu.C("type").Eq("table"),
		goqu.C("name").NotLike("sqlite_%"),
		goqu.C("name").NotLike(HarmonyLitePrefix+"%"),
	).ScanVals(names)

	if err != nil {
		return err
	}

	return nil
}

func performCheckpoint(gSQL *goqu.Database) error {
	rBusy, rLog, rCheckpoint := int64(1), int64(0), int64(0)
	log.Debug().Msg("Forcing WAL checkpoint")

	for rBusy != 0 {
		row := gSQL.QueryRow("PRAGMA wal_checkpoint(truncate);")
		err := row.Scan(&rBusy, &rLog, &rCheckpoint)
		if err != nil {
			return err
		}

		if rBusy != 0 {
			log.Debug().
				Int64("busy", rBusy).
				Int64("log", rLog).
				Int64("checkpoint", rCheckpoint).
				Msg("Waiting checkpoint...")

			time.Sleep(100 * time.Millisecond)
		}
	}

	return nil
}

================
File: db/table_change_log_script.tmpl
================
{{$ChangeLogTableName := (printf "%s%s_change_log" .Prefix .TableName)}}
{{$GlobalChangeLogTableName := (printf "%s_change_log_global" .Prefix)}}

CREATE TABLE IF NOT EXISTS {{$ChangeLogTableName}} (
    id   INTEGER PRIMARY KEY AUTOINCREMENT,
{{range $index, $col := .Columns}}
    val_{{$col.Name}} {{$col.Type}},
{{end}}
    type TEXT,
    created_at INTEGER,
    state INTEGER
);

CREATE INDEX IF NOT EXISTS {{$ChangeLogTableName}}_state_index ON {{$ChangeLogTableName}} (state);

{{range $trigger, $read_target := .Triggers}}
DROP TRIGGER IF EXISTS {{$ChangeLogTableName}}_on_{{$trigger}};
CREATE TRIGGER IF NOT EXISTS {{$ChangeLogTableName}}_on_{{$trigger}}
AFTER {{$trigger}} ON {{$.TableName}}
WHEN (SELECT COUNT(*) FROM pragma_function_list WHERE name='harmonylite_version') < 1
BEGIN

    INSERT INTO {{$ChangeLogTableName}}(
        {{range $col := $.Columns}}
            val_{{$col.Name}},
        {{end}}
        type,
        created_at,
        state
    ) VALUES(
        {{range $col := $.Columns}}
            {{$read_target}}.{{$col.Name}},
        {{end}}
        '{{$trigger}}',
        CAST((strftime('%s','now') || substr(strftime('%f','now'),4)) as INT),
        0 -- Pending
    );

    INSERT INTO {{$GlobalChangeLogTableName}} (change_table_id, table_name)
    VALUES (
        last_insert_rowid(),
        '{{$.TableName}}'
    );

END;
{{end}}

================
File: db/utils_test.go
================
package db

import (
	"database/sql"
	"os"
	"path/filepath"
	"testing"

	_ "github.com/mattn/go-sqlite3"
)

func TestEnhancedRows_FetchRow(t *testing.T) {
	// Create temporary file for the test database
	tmpFile, err := os.CreateTemp("", "harmonylite-utils-test-*.db")
	if err != nil {
		t.Fatalf("Failed to create temp file: %v", err)
	}
	defer os.Remove(tmpFile.Name())
	tmpFile.Close()

	// Open the database
	db, err := sql.Open("sqlite3", tmpFile.Name())
	if err != nil {
		t.Fatalf("Failed to open database: %v", err)
	}
	defer db.Close()

	// Create test table with multiple column types
	_, err = db.Exec(`
		CREATE TABLE test_table (
			id INTEGER PRIMARY KEY,
			text_col TEXT,
			int_col INTEGER,
			real_col REAL,
			null_col TEXT
		);
		
		INSERT INTO test_table (id, text_col, int_col, real_col, null_col)
		VALUES (1, 'test text', 42, 3.14, NULL);
	`)
	if err != nil {
		t.Fatalf("Failed to create test table: %v", err)
	}

	// Execute a query to get rows
	rows, err := db.Query("SELECT * FROM test_table")
	if err != nil {
		t.Fatalf("Failed to query test table: %v", err)
	}

	// Wrap in EnhancedRows
	enhancedRows := &EnhancedRows{rows}
	defer enhancedRows.Finalize()

	// Test fetching the row
	if !enhancedRows.Next() {
		t.Fatalf("Expected at least one row")
	}

	row, err := enhancedRows.fetchRow()
	if err != nil {
		t.Fatalf("Failed to fetch row: %v", err)
	}

	// Verify row contents
	if row["id"] != int64(1) {
		t.Errorf("Expected id=1, got %v (type: %T)", row["id"], row["id"])
	}

	if row["text_col"] != "test text" {
		t.Errorf("Expected text_col='test text', got %v", row["text_col"])
	}

	if row["int_col"] != int64(42) {
		t.Errorf("Expected int_col=42, got %v (type: %T)", row["int_col"], row["int_col"])
	}

	if row["real_col"] != 3.14 {
		t.Errorf("Expected real_col=3.14, got %v", row["real_col"])
	}

	if row["null_col"] != nil {
		t.Errorf("Expected null_col=nil, got %v", row["null_col"])
	}

	// Test that there are no more rows
	if enhancedRows.Next() {
		t.Errorf("Expected only one row")
	}
}

func TestEnhancedStatement_Finalize(t *testing.T) {
	// Create temporary file for the test database
	tmpFile, err := os.CreateTemp("", "harmonylite-utils-stmt-test-*.db")
	if err != nil {
		t.Fatalf("Failed to create temp file: %v", err)
	}
	defer os.Remove(tmpFile.Name())
	tmpFile.Close()

	// Open the database
	db, err := sql.Open("sqlite3", tmpFile.Name())
	if err != nil {
		t.Fatalf("Failed to open database: %v", err)
	}
	defer db.Close()

	// Create test table
	_, err = db.Exec(`
		CREATE TABLE test_statement (
			id INTEGER PRIMARY KEY,
			value TEXT
		);
	`)
	if err != nil {
		t.Fatalf("Failed to create test table: %v", err)
	}

	// Prepare a statement
	stmt, err := db.Prepare("INSERT INTO test_statement (value) VALUES (?)")
	if err != nil {
		t.Fatalf("Failed to prepare statement: %v", err)
	}

	// Wrap in EnhancedStatement
	enhancedStmt := &EnhancedStatement{stmt}

	// Execute the statement
	_, err = enhancedStmt.Exec("test value")
	if err != nil {
		t.Fatalf("Failed to execute statement: %v", err)
	}

	// Finalize the statement
	enhancedStmt.Finalize()

	// Verify that the statement is closed by trying to use it again
	_, err = enhancedStmt.Exec("should fail")
	if err == nil {
		t.Errorf("Expected error after finalizing statement, but got none")
	}
}

func TestEnhancedRows_Finalize(t *testing.T) {
	// Create temporary directory
	tempDir, err := os.MkdirTemp("", "harmonylite-rows-test-*")
	if err != nil {
		t.Fatalf("Failed to create temp directory: %v", err)
	}
	defer os.RemoveAll(tempDir)

	// Create test database
	dbPath := filepath.Join(tempDir, "test.db")
	db, err := sql.Open("sqlite3", dbPath)
	if err != nil {
		t.Fatalf("Failed to open database: %v", err)
	}
	defer db.Close()

	// Create test table
	_, err = db.Exec(`
		CREATE TABLE test_rows (
			id INTEGER PRIMARY KEY,
			value TEXT
		);
		
		INSERT INTO test_rows (value) VALUES ('test1'), ('test2'), ('test3');
	`)
	if err != nil {
		t.Fatalf("Failed to create test table: %v", err)
	}

	// Execute a query
	rows, err := db.Query("SELECT * FROM test_rows")
	if err != nil {
		t.Fatalf("Failed to query test table: %v", err)
	}

	// Wrap in EnhancedRows
	enhancedRows := &EnhancedRows{rows}

	// Consume some but not all rows
	if !enhancedRows.Next() {
		t.Fatalf("Expected at least one row")
	}

	// Finalize before consuming all rows
	enhancedRows.Finalize()

	// Verify that rows are closed by trying to use them again
	if enhancedRows.Next() {
		t.Errorf("Expected closed rows, but Next() returned true")
	}

	// This should not panic if rows were properly closed
	enhancedRows.Finalize()
}

================
File: db/utils.go
================
package db

import (
	"database/sql"

	"github.com/rs/zerolog/log"
)

type EnhancedStatement struct {
	*sql.Stmt
}

func (stmt *EnhancedStatement) Finalize() {
	err := stmt.Close()
	if err != nil {
		log.Error().Err(err).Msg("Unable to close statement")
	}
}

type EnhancedRows struct {
	*sql.Rows
}

func (rs *EnhancedRows) fetchRow() (map[string]any, error) {
	columns, err := rs.Columns()
	if err != nil {
		return nil, err
	}

	scanRow := make([]any, len(columns))
	rowPointers := make([]any, len(columns))
	for i := range scanRow {
		rowPointers[i] = &scanRow[i]
	}

	if err := rs.Scan(rowPointers...); err != nil {
		return nil, err
	}

	row := make(map[string]any)
	for i, column := range columns {
		row[column] = scanRow[i]
	}

	return row, nil
}

func (rs *EnhancedRows) Finalize() {
	err := rs.Close()
	if err != nil {
		log.Error().Err(err).Msg("Unable to close result set")
	}
}

================
File: docs/docs/architecture.md
================
# HarmonyLite Architecture

This document explains the core architecture, components, and design principles behind HarmonyLite. Understanding these concepts will help you better implement, configure, and troubleshoot your HarmonyLite deployment.

## Architectural Overview

HarmonyLite implements a leaderless, eventually consistent replication system for SQLite databases. The architecture consists of four main components working together:

1. **Change Data Capture (CDC)**: Monitors and records database changes
2. **Message Distribution**: Publishes and subscribes to change events
3. **Change Application**: Applies changes to local databases
4. **State Management**: Handles snapshots and recovery

The following diagram illustrates the high-level architecture:

```mermaid
flowchart TB
    subgraph "Node 1"
        DB1[SQLite DB] --> Triggers1[SQLite Triggers]
        Triggers1 --> ChangeLog1[Change Log]
        ChangeLog1 --> HarmonyLite1[HarmonyLite]
    end

    subgraph "NATS JetStream"
        Streams[(JetStream Streams)]
        Objects[(Object Store)]
    end

    subgraph "Node 2"
        HarmonyLite2[HarmonyLite] --> ChangeLog2[Change Log]
        ChangeLog2 --> Triggers2[SQLite Triggers]
        Triggers2 --> DB2[SQLite DB]
    end

    HarmonyLite1 <--> Streams
    HarmonyLite2 <--> Streams
    HarmonyLite1 <--> Objects
    HarmonyLite2 <--> Objects

    classDef sqlite fill:#f9f,stroke:#333,stroke-width:2px
    classDef harmony fill:#bbf,stroke:#333,stroke-width:2px
    classDef nats fill:#bfb,stroke:#333,stroke-width:2px
    
    class DB1,DB2 sqlite
    class HarmonyLite1,HarmonyLite2 harmony
    class Streams,Objects nats
```

## Core Components

### 1. Change Data Capture (CDC)

HarmonyLite uses SQLite triggers to capture all database changes:

- **Triggers**: Automatically installed on all tables to detect INSERT, UPDATE, and DELETE operations
- **Change Log Tables**: Each monitored table has a corresponding `__harmonylite__<table_name>_change_log` table
- **Global Change Log**: A master table (`__harmonylite___global_change_log`) tracks the sequence of operations

When a change occurs:
1. The trigger fires and captures the change details
2. Information is stored in the change log table
3. A reference is added to the global change log

#### Table Structure

Database changes are tracked in specialized tables with this structure:

```mermaid
erDiagram
    USERS {
        int id PK
        string name
        string email
    }
    
    GLOBAL_CHANGE_LOG {
        int id PK
        int change_table_id FK
        string table_name
    }
    
    USERS_CHANGE_LOG {
        int id PK
        int val_id
        string val_name
        string val_email
        string type
        int created_at
        int state
    }
    
    USERS ||--o{ USERS_CHANGE_LOG : "triggers create"
    USERS_CHANGE_LOG ||--o{ GLOBAL_CHANGE_LOG : "references"
```

### 2. Message Distribution

HarmonyLite uses NATS JetStream for reliable message distribution:

- **Change Detection**: Monitors the database for modifications
- **Change Collection**: Retrieves pending records from change log tables
- **Hash Calculation**: Computes a hash from table name and primary keys
- **Stream Selection**: Routes changes to specific streams based on the hash
- **Publishing**: Sends changes to NATS JetStream
- **Confirmation**: Marks changes as published after acknowledgment

This approach ensures changes to the same row are always handled in order, while allowing parallel processing of changes to different rows.

### 3. Change Application

When a node receives a change message:

1. It checks if the change was originated locally (to avoid cycles)
2. It verifies the change hasn't been applied before
3. It parses the change details (table, operation type, values)
4. It applies the change to the local database
5. It records the message sequence for recovery tracking

### 4. State Management

HarmonyLite maintains system state through:

- **Sequence Map**: Tracks the last processed message for each stream
- **Snapshots**: Periodic database snapshots for efficient recovery
- **CBOR Serialization**: Efficient binary encoding for change records

## Key Mechanisms

### Leaderless Replication

Unlike leader-follower systems, HarmonyLite operates without a designated leader:

- Any node can accept writes
- Changes propagate to all nodes
- No single point of failure
- Higher write availability

### Eventual Consistency

HarmonyLite prioritizes availability over immediate consistency:

- Changes eventually reach all nodes
- Last-writer-wins conflict resolution
- No global locking mechanism
- Non-blocking operations

### Sharding

Change streams can be sharded to improve performance:

- Each shard handles a subset of rows
- Determined by hashing table name and primary keys
- Enables parallel processing
- Configurable via `replication_log.shards`

### Message Flow

The complete message flow looks like this:

```mermaid
sequenceDiagram
    participant App1 as Application (Node 1)
    participant DB1 as SQLite DB (Node 1)
    participant HL1 as HarmonyLite (Node 1)
    participant NATS as NATS JetStream
    participant HL2 as HarmonyLite (Node 2)
    participant DB2 as SQLite DB (Node 2)
    participant App2 as Application (Node 2)
    
    App1->>DB1: INSERT/UPDATE/DELETE
    DB1->>DB1: Trigger executes
    DB1->>DB1: Record in change log
    
    HL1->>DB1: Poll for changes
    DB1->>HL1: Return pending changes
    HL1->>HL1: Calculate hash
    HL1->>NATS: Publish to stream
    NATS->>HL1: Acknowledge receipt
    HL1->>DB1: Mark as published
    
    NATS->>HL2: Deliver change
    HL2->>HL2: Process change
    HL2->>DB2: Apply change
    HL2->>NATS: Acknowledge processing
    
    App2->>DB2: Read updated data
    DB2->>App2: Return data
```

## Snapshot and Recovery

### Snapshot Creation

Snapshots provide efficient node recovery:

```mermaid
graph TB
    A[Monitor Sequence Numbers] --> B{Threshold Reached?}
    B -->|Yes| C[Create Temp Directory]
    B -->|No| A
    C --> D["VACUUM INTO (Temp Copy)"]
    D --> E[Remove Triggers & Logs]
    E --> F[Optimize with VACUUM]
    F --> G[Upload Snapshot]
    G --> H[Update Sequence Map]
    H --> A
```

### Node Recovery

When a node starts or needs to catch up:

```mermaid
graph TB
    I[Node Startup] --> J[Check DB Integrity]
    J --> K[Load Sequence Map]
    K --> L{Too Far Behind?}
    L -->|Yes| M[Download Snapshot]
    L -->|No| R[Normal Operation]
    M --> N[Exclusive Lock]
    N --> O[Replace DB Files]
    O --> P[Install Triggers]
    P --> Q[Process Recent Changes]
    Q --> R
```

## Understanding Trade-offs

### CAP Theorem Positioning

HarmonyLite makes specific trade-offs according to the CAP theorem:

- **Consistency**: Eventual (not strong)
- **Availability**: High (prioritized)
- **Partition Tolerance**: Maintained

This positions HarmonyLite as an AP system (Availability and Partition Tolerance) rather than a CP system.

### Suitable Use Cases

HarmonyLite is ideal for:
- Read-heavy workloads
- Systems that can tolerate eventual consistency
- Applications needing high write availability
- Edge computing and distributed systems

### Less Suitable Use Cases

HarmonyLite may not be the best choice for:
- Strong consistency requirements
- Complex transactional workloads
- Financial systems requiring immediate consistency
- Systems with strict ordering requirements

## Performance Characteristics

### Scalability

- **Read Scalability**: Excellent (horizontal)
- **Write Scalability**: Good (limited by conflict resolution)
- **Node Count**: Practical up to dozens of nodes

### Latency

- **Local Operations**: Minimal impact (~1-5ms overhead)
- **Replication Delay**: Typically 50-500ms depending on network
- **Recovery Time**: Proportional to changes since last snapshot

### Resource Usage

- **Memory**: Moderate (configurable)
- **CPU**: Low to moderate
- **Disk**: Additional space for change logs and snapshots
- **Network**: Proportional to change volume and compression settings

## Next Steps

- [Replication Details](replication.md) - Deep dive into the replication process
- [Snapshots](snapshots.md) - How snapshots and recovery work
- [Configuration Reference](configuration-reference.md) - Complete configuration options

================
File: docs/docs/configuration-reference.md
================
# Configuration Reference

This document provides a comprehensive reference for all HarmonyLite configuration options. Use this guide to understand available settings and fine-tune your deployment.

## Configuration File Format

HarmonyLite uses a TOML configuration file format. By default, it looks for `config.toml` in the current directory, but you can specify a different path with the `-config` command-line parameter.

## Basic Configuration

```toml
# Database path (required)
db_path = "/path/to/your.db"

# Unique node identifier (required, integer)
node_id = 1

# Path to persist sequence map (required)
seq_map_path = "/path/to/seq-map.cbor"

# Enable/disable publishing changes (optional, default: true)
publish = true

# Enable/disable replicating changes (optional, default: true)
replicate = true

# Cleanup interval in milliseconds (optional, default: 60000)
cleanup_interval = 60000
```

## Replication Log Settings

```toml
[replication_log]
# Number of shards for parallel processing (optional, default: 1)
shards = 1

# Maximum entries per stream (optional, default: 1024)
max_entries = 1024

# Number of stream replicas for fault tolerance (optional, default: 1)
replicas = 3

# Enable zstd compression for change logs (optional, default: false)
compress = true


## Snapshot Settings

```toml
[snapshot]
# Enable snapshot support (optional, default: false)
enabled = true

# Snapshot storage backend (required if enabled)
# Options: "nats", "s3", "webdav", "sftp"
store = "nats"

# Snapshot interval in milliseconds (optional, default: 3600000)
interval = 3600000

# Minimum changes before forcing snapshot (optional, default: 1000)
min_changes = 1000

# Maximum snapshot age in seconds (optional, default: 86400)
max_age = 86400
```

## NATS Configuration

```toml
[nats]
# List of NATS server URLs (optional, if empty uses embedded server)
urls = ["nats://localhost:4222"]

# Prefix for change log subjects (optional, default: "harmonylite-change-log")
subject_prefix = "harmonylite-change-log"

# Prefix for JetStream streams (optional, default: "harmonylite-changes")
stream_prefix = "harmonylite-changes"

# Bind address for embedded NATS server (optional, default: "0.0.0.0:4222")
bind_address = "0.0.0.0:4222"

# Path to custom NATS server config file (optional)
server_config = "/path/to/nats-server.conf"

# Connection retries for external servers (optional, default: 5)
connect_retries = 5

# Delay between reconnect attempts in seconds (optional, default: 2)
reconnect_wait_seconds = 2

# Authentication username (optional)
user_name = "harmonylite"

# Authentication password (optional)
user_password = "secure-password-here"

# Path to NKEY seed file (optional)
seed_file = "/path/to/user.seed"
```

## NATS Snapshot Storage

```toml
[snapshot.nats]
# Number of snapshot replicas (optional, default: 1)
replicas = 2

# Bucket name for object storage (optional)
bucket = "harmonylite-snapshots"
```

## S3 Snapshot Storage

```toml
[snapshot.s3]
# S3 endpoint (required for S3 storage)
endpoint = "s3.amazonaws.com"

# Path prefix inside bucket (optional)
path = "harmonylite/snapshots"

# Bucket name (required for S3 storage)
bucket = "your-backup-bucket"

# Use SSL for connections (optional, default: true)
use_ssl = true

# Access key for authentication (required for S3 storage)
access_key = "your-access-key"

# Secret key for authentication (required for S3 storage)
secret = "your-secret-key"

# Region name (optional)
region = "us-west-2"

# Custom part size for multipart uploads in bytes (optional, default: 5242880)
part_size = 5242880
```

## WebDAV Snapshot Storage

```toml
[snapshot.webdav]
# WebDAV server URL (required for WebDAV storage)
endpoint = "https://webdav.example.com"

# Path prefix on server (optional)
path = "harmonylite/snapshots"

# Username for authentication (optional)
username = "webdav-user"

# Password for authentication (optional)
password = "webdav-password"
```

## SFTP Snapshot Storage

```toml
[snapshot.sftp]
# SFTP server address (required for SFTP storage)
endpoint = "sftp.example.com:22"

# Path on remote server (optional, default: "/")
path = "/harmonylite/snapshots"

# Username for authentication (required for SFTP storage)
username = "sftp-user"

# Password for authentication (optional)
password = "sftp-password"

# Path to private key file for authentication (optional)
key_file = "/path/to/id_rsa"

# Passphrase for encrypted private key (optional)
passphrase = "key-passphrase"
```

## Logging Configuration

```toml
[logging]
# Enable verbose logging (optional, default: false)
verbose = true

# Log format (optional, default: "console")
# Options: "console", "json"
format = "json"

# Log level (optional, default: "info")
# Options: "debug", "info", "warn", "error"
level = "info"

# Path to log file (optional, if empty logs to stdout)
file = "/var/log/harmonylite.log"
```

## Prometheus Metrics

```toml
[prometheus]
# Enable Prometheus metrics (optional, default: false)
enable = true

# Metrics HTTP listener address (optional, default: "0.0.0.0:3010")
bind = "0.0.0.0:3010"

# Metrics namespace (optional, default: "harmonylite")
namespace = "harmonylite"

# Metrics subsystem (optional, default: "replication")
subsystem = "replication"
```

## Example Configurations

### Basic Single Node

```toml
db_path = "/path/to/data.db"
node_id = 1
seq_map_path = "/path/to/seq-map.cbor"

[replication_log]
shards = 1
max_entries = 1024
replicas = 1
```

### Production Cluster

```toml
db_path = "/var/lib/harmonylite/data.db"
node_id = 1
seq_map_path = "/var/lib/harmonylite/seq-map.cbor"
cleanup_interval = 30000

[replication_log]
shards = 4
max_entries = 2048
replicas = 3
compress = true

[snapshot]
enabled = true
store = "s3"
interval = 1800000

[snapshot.s3]
endpoint = "s3.amazonaws.com"
path = "harmonylite/snapshots"
bucket = "your-backup-bucket"
use_ssl = true
access_key = "your-access-key"
secret = "your-secret-key"
region = "us-west-2"

[nats]
urls = ["nats://nats-server-1:4222", "nats://nats-server-2:4222"]
connect_retries = 10
reconnect_wait_seconds = 5

[prometheus]
enable = true
bind = "0.0.0.0:3010"

[logging]
verbose = true
format = "json"
file = "/var/log/harmonylite.log"
```

### Edge Node (Read-Only)

```toml
db_path = "/var/lib/harmonylite/data.db"
node_id = 5
seq_map_path = "/var/lib/harmonylite/seq-map.cbor"
publish = false  # Don't publish changes
replicate = true  # Only receive changes

[replication_log]
shards = 2
max_entries = 1024
replicas = 2

[nats]
urls = ["nats://central-nats:4222"]
reconnect_wait_seconds = 10
```

## Command-Line Options

In addition to the configuration file, HarmonyLite accepts several command-line parameters:

| Parameter | Description |
|-----------|-------------|
| `-config` | Path to configuration file |
| `-cluster-addr` | Embedded NATS server cluster address |
| `-cluster-peers` | Comma-separated list of peer URLs |
| `-cleanup` | Clean up triggers and log tables |
| `-save-snapshot` | Force snapshot creation |
| `-version` | Show version information |
| `-help` | Display help information |

Example usage:
```bash
harmonylite -config /etc/harmonylite/config.toml -cluster-addr 127.0.0.1:4222 -cluster-peers nats://127.0.0.1:4223/
```

================
File: docs/docs/demo.md
================
# HarmonyLite Demo with PocketBase

This guide demonstrates setting up a distributed note-taking application using HarmonyLite for SQLite replication and **PocketBase** as the backend, with preconfigured admin and schema.

## Overview

In this demo, you'll:
- Set up a two-node HarmonyLite cluster for database replication
- Launch PocketBase instances preconfigured with an admin user and "notes" collection
- Test replication and fault tolerance in a distributed note-taking app

By the end, you'll see how HarmonyLite and PocketBase work together seamlessly.

## Prerequisites

- **HarmonyLite**: Downloaded and available in your PATH
- **PocketBase**: Downloaded from [PocketBase.io](https://pocketbase.io/docs/)
- A terminal and basic command-line skills
- About 10-15 minutes

## Step 1: Set Up Directory Structure

Create the demo directory and subdirectories:

```bash
mkdir -p harmonylite-demo/{pb-1,pb-2,user_pb_migrations}
cd harmonylite-demo
```

## Step 2: Preconfigure PocketBase

We'll initialize PocketBase with a default admin user and "notes" collection using a migration script.

1. **Create Admin User for Node 1**:
   ```bash
   ./pocketbase superuser create admin@example.com 1234567890 --dir=./pb-1
   ```

2. **Define "notes" Collection**:
   Create a migration file in `user_pb_migrations` (name it `init_notes.js`):
   ```javascript
   migrate((app) => {
      // Create a new "notes" base collection
      const collection = new Collection({
         name: "notes",
         type: "base",
         listRule: "",
         viewRule: "",
         createRule: "",
         updateRule: "",
         deleteRule: "",
         fields: [
            { name: "title", type: "text", required: true },
            { name: "content", type: "text" },
            { name: "is_important", type: "bool" }
         ]
      });
      
      return app.save(collection);
   }, (app) => {
      // Down migration code (for reverting the changes)
      const collection = app.findCollectionByNameOrId("notes");
      return app.delete(collection);
   });
   ```

3. **Apply Migrations**:
   ```bash
   ./pocketbase migrate --dir=./pb-1 --migrationsDir ./user_pb_migrations
   ```

4. **Clone Configuration to Node 2**:
   ```bash
   cp -r pb-1 pb-2
   ```

## Step 3: Start PocketBase Instances

Launch both PocketBase instances with the preconfigured setup:

**Terminal 1 - PocketBase 1**:
```bash
./pocketbase serve --dir=./pb-1 --http=localhost:8090
```

**Terminal 2 - PocketBase 2**:
```bash
./pocketbase serve --dir=./pb-2 --http=localhost:8091
```

Access the admin dashboards:
- Node 1: http://localhost:8090/_/ 
- Node 2: http://localhost:8091/_/ 
- Login credentials for both: `admin@example.com` / `1234567890`

## Step 4: Configure HarmonyLite

Create configuration files for two HarmonyLite nodes:

**node-1-config.toml**:
```toml
db_path = "./pb-1/data.db"
node_id = 1
seq_map_path = "./pb-1/seq-map.cbor"
[replication_log]
shards = 1
max_entries = 1024
replicas = 2
compress = true
[snapshot]
enabled = true
interval = 3600000
store = "nats"
```

**node-2-config.toml**:
```toml
db_path = "./pb-2/data.db"
node_id = 2
seq_map_path = "./pb-2/seq-map.cbor"
[replication_log]
shards = 1
max_entries = 1024
replicas = 2
compress = true
[snapshot]
enabled = true
interval = 3600000
store = "nats"
```

## Step 5: Start HarmonyLite Nodes

Run each node in a separate terminal:

**Terminal 3 - Node 1**:
```bash
harmonylite -config node-1-config.toml -cluster-addr localhost:4221 -cluster-peers 'nats://localhost:4222/'
```

**Terminal 4 - Node 2**:
```bash
harmonylite -config node-2-config.toml -cluster-addr localhost:4222 -cluster-peers 'nats://localhost:4221/'
```

You should see connectivity messages in the logs indicating successful cluster formation.

## Step 6: Test Replication

Verify data syncs between nodes:

1. **Add a Note in Node 1**:
   - Open http://localhost:8090/_/ and navigate to the "notes" collection
   - Click **New record** and enter:
     - Title: "Test Note"
     - Content: "This is a test"
     - Is Important: Toggle as desired
   - Save the record

2. **Check Node 2**:
   - After a few seconds, visit http://localhost:8091/_/ and open the "notes" collection
   - Confirm "Test Note" appears, demonstrating successful replication

3. **Test Bidirectional Sync**:
   - In Node 2, create a new note (e.g., "Node 2 Note")
   - Verify it appears in Node 1 after a brief sync period

## Step 7: Test Fault Tolerance

Simulate a node failure:

1. Stop Node 2 (Press Ctrl+C in Terminal 2)
2. Add a new note in Node 1 (e.g., "Offline Test")
3. Verify Node 1 continues to work normally
4. Restart Node 2:
   ```bash
   ./pocketbase serve --dir=./pb-2 --http=localhost:8091
   ```
   And in another terminal:
   ```bash
   harmonylite -config node-2-config.toml -cluster-addr localhost:4222 -cluster-peers 'nats://localhost:4221/'
   ```
5. Check http://localhost:8091/_/ > "notes" to confirm it catches up and syncs the new note

## Real-World Applications

This architecture can be extended to build:

- Distributed Task Management Systems
- Content Management Systems with High Availability
- Inventory Systems with Offline Support
- Field Service Applications
- Edge Computing Dashboards

## Conclusion

You've successfully built a preconfigured, distributed note-taking application with HarmonyLite and PocketBase, showcasing high availability and eventual consistency in a simple yet powerful setup.

## Next Steps

- Explore [advanced configuration options](configuration-reference.md)
- Learn about [production deployment best practices](production-deployment.md)
- Dive into the [system architecture](architecture.md)

================
File: docs/docs/faq.md
================
# Frequently Asked Questions

## General Questions

### What is HarmonyLite?

HarmonyLite is a distributed SQLite replication system designed to provide leaderless, eventually consistent replication across multiple nodes. It enables SQLite databases to synchronize changes between multiple instances without requiring a central coordinator.

### How does HarmonyLite differ from other SQLite replication solutions?

Unlike other solutions like rqlite, dqlite, or LiteFS which use a leader-follower architecture, HarmonyLite allows writes on any node (leaderless) and provides eventual consistency. This approach increases write availability at the cost of strong consistency guarantees.

| Feature | HarmonyLite | rqlite | dqlite | LiteFS |
|---------|------------|--------|--------|--------|
| Architecture | Leaderless | Leader-follower | Leader-follower | Primary-replica |
| Consistency | Eventual | Strong | Strong | Strong |
| Write Nodes | All nodes | Leader only | Leader only | Primary only |
| Application Changes | None | API changes | API changes | VFS layer |
| Replication Level | Logical (row) | Logical (SQL) | Physical | Physical |

### When should I use HarmonyLite?

HarmonyLite is ideal for:
- Applications that need high write availability
- Read-heavy workloads that benefit from distributing reads
- Edge computing scenarios where nodes may operate independently
- Systems that can tolerate eventual consistency
- Applications using SQLite that need replication without code changes

### What are the limitations of HarmonyLite?

HarmonyLite has some trade-offs to consider:
- Eventual (not strong) consistency guarantees
- Potential for conflicts in write-heavy scenarios
- No built-in conflict resolution beyond last-writer-wins
- Not ideal for financial systems or applications requiring strict transaction ordering
- Some overhead from triggers and change tracking

## Technical Questions

### How does replication work in HarmonyLite?

HarmonyLite uses a multi-step process for replication:
1. SQLite triggers capture database changes
2. Changes are logged in special tracking tables
3. HarmonyLite publishes changes to NATS JetStream
4. Other nodes subscribe to these changes
5. Changes are applied on receiving nodes
6. A last-writer-wins strategy resolves conflicts

### What is the maximum number of nodes supported?

HarmonyLite has been tested with dozens of nodes. The practical limit depends on:
- Network bandwidth and latency
- Write volume and patterns
- Hardware resources
- NATS JetStream configuration

For most use cases, 3-20 nodes provides a good balance of availability and performance.

### Does HarmonyLite require code changes to my application?

No, HarmonyLite works as a sidecar process without requiring code changes to your application. The only requirement is that your application must enable the `trusted_schema` pragma in SQLite:

```sql
PRAGMA trusted_schema = ON;
```

### What happens if a node goes offline?

If a node goes offline:
1. Other nodes continue operating normally
2. Changes are queued in NATS JetStream
3. When the node reconnects, it processes missed changes
4. If offline for an extended period, it may restore from a snapshot first

### How are conflicts handled?

HarmonyLite uses a "last-writer-wins" conflict resolution strategy:
1. Each change has a timestamp and node ID
2. If two nodes modify the same row, the latest change (by timestamp) is kept
3. If timestamps are identical, the node with the higher ID wins

Custom conflict resolution is not currently supported.

### What version of SQLite is required?

HarmonyLite requires SQLite version 3.35.0 or newer. This version introduced the `RETURNING` clause which is used by HarmonyLite's triggers.

### How much overhead does HarmonyLite add?

The overhead varies depending on your workload:
- **Storage**: Change log tables add 20-100% overhead depending on change frequency
- **CPU**: Minimal impact for read operations, 5-15% for write operations
- **Latency**: Local operations typically add 1-5ms overhead
- **Replication Delay**: Usually 50-500ms depending on network conditions

## Deployment Questions

### Can I use HarmonyLite with containerized applications?

Yes, HarmonyLite works well in containerized environments. You can:
1. Run HarmonyLite in its own container with a shared volume for the SQLite database
2. Use a sidecar container pattern alongside your application container
3. Deploy with Kubernetes using StatefulSets for stable node identities

### Can I deploy HarmonyLite across multiple regions?

Yes, HarmonyLite can work across regions with higher latency connections. For multi-region deployments:
1. Configure NATS with gateways between regions
2. Increase `max_entries` and `replicas` for better fault tolerance
3. Enable compression to reduce bandwidth usage
4. Expect higher replication latency due to network distance

### Do I need an external NATS server?

No, HarmonyLite can run with its embedded NATS server for simplicity:
- For small deployments (3-5 nodes), the embedded server works well
- For larger deployments, a dedicated NATS cluster is recommended
- For production, external NATS provides better monitoring and management

### How do I back up a HarmonyLite database?

HarmonyLite provides several backup options:
1. **Automatic snapshots**: Configure the `[snapshot]` section in your config
2. **Regular SQLite backups**: Standard SQLite backup procedures work normally
3. **Filesystem backups**: Back up the entire data directory
4. **Node replication**: Additional nodes can serve as live backups

### Can I use HarmonyLite with a read-only replica design?

Yes, HarmonyLite supports read-only replica configurations:
- Set `publish=false` on nodes designated as read-only replicas
- These nodes will receive changes but not publish their own
- This can be useful for creating dedicated reporting or analytics nodes

## Performance Questions

### How many transactions per second can HarmonyLite handle?

Performance depends on many factors, but general guidelines:
- **Single node write throughput**: 1,000-5,000 TPS
- **Cluster write throughput**: Similar to single node (eventual consistency)
- **Read throughput**: Scales linearly with number of nodes

These numbers can vary significantly based on hardware, network, and workload patterns.

### How does HarmonyLite perform with large databases?

HarmonyLite works with databases ranging from megabytes to many gigabytes:
- For large databases (10GB+), consider:
  - More frequent snapshots
  - Higher `cleanup_interval` values
  - SSD storage for better performance
  - Tuning SQLite's cache size in your application

### How do I optimize HarmonyLite for my workload?

For read-heavy workloads:
- Add more nodes to distribute reads
- Use appropriate SQLite indexes
- Consider memory tuning for SQLite in your application

For write-heavy workloads:
- Increase `replication_log.shards` for parallel processing
- Enable compression if network is a bottleneck
- Consider SSD storage for better performance
- Tune the `cleanup_interval` for optimal change log maintenance

### Does HarmonyLite support sharding?

HarmonyLite internally shards change streams for better performance, but doesn't provide application-level sharding. For very large datasets:
1. Consider application-level sharding with multiple HarmonyLite clusters
2. Each cluster manages a subset of your data
3. Your application directs operations to the appropriate cluster

## Troubleshooting Questions

### Why aren't my changes replicating?

Common reasons for replication issues:
1. **NATS connectivity problems**: Check network and firewall settings
2. **Triggers not installed**: Run `harmonylite -cleanup` and restart
3. **PRAGMA trusted_schema not enabled**: Ensure your app sets this
4. **Node configuration issues**: Verify `publish` and `replicate` settings
5. **Database permissions**: Check file permissions and ownership

See the [Troubleshooting Guide](troubleshooting.md) for detailed diagnostics.

### I'm getting "database is locked" errors. What should I do?

SQLite locking issues can occur for several reasons:
1. **Application transaction handling**: Ensure transactions are properly closed
2. **Journal mode**: Use WAL mode in your application
3. **Busy timeout**: Set an appropriate timeout in your application
4. **Multiple processes**: Check if multiple processes are accessing the database
5. **File system issues**: Verify proper file system permissions and mount options

### How do I recover from database corruption?

If you suspect database corruption:
1. Stop your application and HarmonyLite
2. Run `PRAGMA integrity_check;` on the database
3. If corruption is confirmed, restore from the latest snapshot:
   - Remove the database file and sequence map
   - Restart HarmonyLite to trigger automatic recovery
4. If snapshots aren't available, restore from a backup
5. As a last resort, dump and recreate the schema and data

### What if I need to make schema changes?

For schema changes:
1. Stop applications writing to the database
2. Apply schema changes on one node
3. Run cleanup to reset triggers:
   ```bash
   harmonylite -config /path/to/config.toml -cleanup
   ```
4. Restart HarmonyLite on that node
5. Wait for changes to replicate
6. Repeat for other nodes
7. Resume application connections

## Advanced Questions

### Can I use HarmonyLite with encrypted SQLite databases?

Yes, HarmonyLite works with encrypted SQLite databases. If using SQLCipher or similar encryption:
1. Your application needs to handle key management and decryption
2. HarmonyLite needs the same key to access the database
3. Snapshots will be encrypted with the same encryption

### Is HarmonyLite suitable for IoT or edge computing?

Yes, HarmonyLite is well-suited for IoT and edge computing:
- Lightweight enough to run on constrained devices
- Works with intermittent connectivity
- Eventual consistency model tolerates network disruptions
- Enables local writes with later synchronization
- Configurable storage and memory usage

### Can I integrate HarmonyLite with my existing monitoring system?

Yes, HarmonyLite provides several monitoring integration points:
- **Prometheus metrics**: Enable with the `[prometheus]` config section
- **Structured logging**: Use JSON logging for log aggregation systems
- **NATS monitoring**: Monitor the underlying NATS infrastructure
- **Health checks**: Use the metrics endpoint for service health checks

### Is there a cloud-hosted version of HarmonyLite?

Currently, HarmonyLite is self-hosted software that you deploy and manage. There is no official cloud-hosted service offering HarmonyLite as a managed service.

### How do I contribute to HarmonyLite?

Contributions are welcome! To contribute:
1. Fork the repository on GitHub
2. Create a feature branch for your changes
3. Make your changes, including tests
4. Submit a pull request
5. Engage with the community for feedback

## Support Questions

### Where can I get help with HarmonyLite?

Support resources include:
- **Documentation**: Comprehensive guides and references
- **GitHub Issues**: For bug reports and feature requests
- **Community Forums**: Discuss with other users and developers
- **Stack Overflow**: Tag questions with 'harmonylite'

### Is commercial support available?

For commercial support inquiries, please contact the maintainers directly through GitHub.

### How do I report a bug?

To report a bug:
1. Check existing GitHub issues to see if it's already reported
2. Create a new issue with:
   - Clear description of the problem
   - Steps to reproduce
   - Expected vs. actual behavior
   - Version information
   - Logs and configuration (redacted of sensitive data)

### How often is HarmonyLite updated?

HarmonyLite follows a regular release cycle with:
- **Patch releases**: Bug fixes and minor improvements
- **Minor releases**: New features and enhancements
- **Major releases**: Significant changes that may require configuration updates

Check the GitHub repository for the latest release information.

================
File: docs/docs/introduction.md
================
# Introduction to HarmonyLite

HarmonyLite is a distributed SQLite replication system designed to provide a leaderless, eventually consistent architecture for synchronizing SQLite databases across multiple nodes. It enhances SQLite's capabilities by enabling seamless data replication without requiring a central coordinating server.

## What is HarmonyLite?

HarmonyLite extends SQLite — the world's most widely deployed database engine — with distributed replication capabilities. While SQLite traditionally operates as a standalone database, HarmonyLite transforms it into a distributed system where multiple database instances can share and synchronize data changes across a network.

The system operates with a leaderless architecture, meaning any node can accept write operations without requiring central coordination. This approach prioritizes high availability and horizontal scalability over strong consistency guarantees.

## Key Features

- **Leaderless Architecture**: Any node can accept read and write operations without a central coordinator.
- **Eventually Consistent**: Changes propagate to all nodes, ensuring they eventually reach a consistent state.
- **Minimal Application Changes**: Works with existing SQLite applications with no code changes required.
- **Lightweight Sidecar Design**: Runs alongside your existing processes with minimal overhead.
- **Fault Tolerance**: Continues operating during temporary node failures or network partitions.
- **Horizontal Scaling**: Add more nodes to handle increased read workloads.
- **NATS Integration**: Leverages NATS JetStream for reliable message delivery and node coordination.
- **Change Data Capture**: Uses SQLite triggers to capture and propagate changes.
- **Snapshot Management**: Efficiently synchronizes new or recovering nodes.

## When to Use HarmonyLite

HarmonyLite is ideal for:

- **Edge Computing**: Distributed applications running across multiple devices or locations.
- **Micro-Services**: Services requiring local data access with eventual synchronization.
- **Read-Heavy Workloads**: Applications that benefit from distributing read operations.
- **Offline-First Applications**: Systems that need to operate during intermittent connectivity.
- **High Availability**: Use cases where continuous operation is critical, even during node failures.

## Architecture Overview

HarmonyLite orchestrates distributed SQLite replication through four key components:

1. **Change Data Capture (CDC)**: SQLite triggers monitor and record all database changes.
2. **NATS JetStream**: Provides reliable messaging and consensus for replication.
3. **Node Coordination**: Manages replication across distributed nodes.
4. **Snapshot Management**: Synchronizes database states for new or offline nodes.

The following diagram illustrates this architecture:

```mermaid
flowchart TB
    subgraph "Node 1"
        DB1[SQLite DB] --> Triggers1[SQLite Triggers]
        Triggers1 --> ChangeLog1[Change Log]
        ChangeLog1 --> HarmonyLite1[HarmonyLite]
    end

    subgraph "NATS JetStream"
        Streams[(JetStream Streams)]
        Objects[(Object Store)]
    end

    subgraph "Node 2"
        HarmonyLite2[HarmonyLite] --> ChangeLog2[Change Log]
        ChangeLog2 --> Triggers2[SQLite Triggers]
        Triggers2 --> DB2[SQLite DB]
    end

    HarmonyLite1 <--> Streams
    HarmonyLite2 <--> Streams
    HarmonyLite1 <--> Objects
    HarmonyLite2 <--> Objects

    classDef sqlite fill:#f9f,stroke:#333,stroke-width:2px
    classDef harmony fill:#bbf,stroke:#333,stroke-width:2px
    classDef nats fill:#bfb,stroke:#333,stroke-width:2px
    
    class DB1,DB2 sqlite
    class HarmonyLite1,HarmonyLite2 harmony
    class Streams,Objects nats
```

## Comparison with Other Solutions

| Feature               | HarmonyLite       | rqlite           | dqlite           | LiteFS           |
|-----------------------|-------------------|------------------|------------------|------------------|
| **Architecture**      | Leaderless        | Leader-follower  | Leader-follower  | Primary-replica  |
| **Consistency**       | Eventual          | Strong           | Strong           | Strong           |
| **Write Nodes**       | All nodes         | Leader only      | Leader only      | Primary only     |
| **Application Changes** | None            | API changes      | API changes      | VFS layer        |
| **Replication Level** | Logical (row)     | Logical (SQL)    | Physical         | Physical         |

Unlike [rqlite](https://github.com/rqlite/rqlite), [dqlite](https://dqlite.io/), and [LiteFS](https://github.com/superfly/litefs), which rely on a single write node, HarmonyLite offers:

- **Write Anywhere**: Any node can accept writes, eliminating single points of failure.
- **No Locking**: Avoids performance bottlenecks from global coordination.
- **Seamless Integration**: No need to modify your application code.
- **Sidecar Design**: Runs independently alongside your application.

## Next Steps

- [Quick Start Guide](quick-start.md) - Get HarmonyLite running in minutes
- [Demo](demo.md) - See a practical demonstration with Pocketbase
- [Architecture](architecture.md) - Understand the technical details

================
File: docs/docs/nats-configuration.md
================
# NATS Configuration Guide

NATS serves as the foundation for HarmonyLite's replication and coordination. This guide provides detailed information about configuring NATS for optimal performance, reliability, and security.

## What is NATS?

[NATS](https://nats.io/) is a high-performance, lightweight messaging system designed for distributed systems, cloud-native applications, and IoT. It provides the messaging backbone for HarmonyLite, enabling:

- **Reliable message delivery** via JetStream persistence
- **Cluster coordination** without a central leader
- **Snapshot storage** for efficient node recovery
- **Horizontal scaling** across many nodes

## Deployment Options

HarmonyLite offers two deployment modes for NATS:

### 1. Embedded NATS Server

The simplest approach is to use HarmonyLite's embedded NATS server, which is enabled by default when no external NATS URLs are specified.

#### Configuration:

```toml
# config.toml
[nats]
# Empty URLs array = use embedded server
urls = []
bind_address = "0.0.0.0:4222"
```

#### Starting a Cluster with Embedded NATS:

```bash
# Node 1
harmonylite -config node1-config.toml -cluster-addr 127.0.0.1:4221

# Node 2 (referencing Node 1)
harmonylite -config node2-config.toml -cluster-addr 127.0.0.1:4222 -cluster-peers nats://127.0.0.1:4221/

# Node 3 (referencing Nodes 1 and 2)
harmonylite -config node3-config.toml -cluster-addr 127.0.0.1:4223 -cluster-peers nats://127.0.0.1:4221/,nats://127.0.0.1:4222/
```

#### Benefits:
- Simple setup with no additional software
- Zero-configuration for development and testing
- Ideal for small deployments (3-5 nodes)

#### Limitations:
- Limited tuning and monitoring options
- Bound to HarmonyLite process lifecycle
- Less suitable for large production deployments

### 2. External NATS Server

For production deployments, a dedicated external NATS cluster provides better management, monitoring, and scaling options.

#### Configuration:

```toml
# config.toml
[nats]
urls = ["nats://nats-server-1:4222", "nats://nats-server-2:4222", "nats://nats-server-3:4222"]
connect_retries = 5
reconnect_wait_seconds = 2
```

#### Starting HarmonyLite with External NATS:

```bash
# All nodes simply connect to the existing NATS cluster
harmonylite -config config.toml
```

#### Benefits:
- Independent lifecycle management
- Dedicated resources for messaging
- Advanced configuration options
- Better monitoring and operational tools
- Suitable for large-scale deployments

#### Limitations:
- Additional infrastructure to manage
- More complex initial setup

## Optimizing NATS Storage

By default, NATS JetStream writes data to temporary directories, which isn't ideal for production. Here's how to configure a dedicated storage location:

### For Embedded NATS Server

1. **Create a NATS Config File**:

   ```
   # nats-server.conf
   jetstream {
     store_dir: "/var/lib/harmonylite/jetstream"
     max_memory_store: 52428800  # 50MB of memory
     max_file_store: 5368709120  # 5GB of disk
   }
   ```

2. **Point HarmonyLite to This Config**:

   ```toml
   [nats]
   server_config = "/etc/harmonylite/nats-server.conf"
   ```

3. **Ensure Directory Permissions**:

   ```bash
   sudo mkdir -p /var/lib/harmonylite/jetstream
   sudo chown harmonylite:harmonylite /var/lib/harmonylite/jetstream
   ```

### For External NATS Server

Configure storage settings in your NATS server configuration:

```
# External NATS server config
jetstream {
  store_dir: "/var/lib/nats/jetstream"
  max_memory_store: 1073741824  # 1GB memory
  max_file_store: 10737418240  # 10GB disk
}
```

## Security Configuration

### Authentication Options

#### 1. Username/Password

```toml
[nats]
user_name = "harmonylite"
user_password = "secure-password-here"
```

#### 2. NKeys Authentication

Generate an NKey pair:
```bash
# Install NATS CLI tools
curl -sf https://install.nats.io/install.sh | sh

# Generate a user key pair
nk -gen user > user.nkey

# Extract the public key
nk -inkey user.nkey -pubout
```

Configure HarmonyLite to use the seed file:
```toml
[nats]
seed_file = "/etc/harmonylite/user.nkey"
```

#### 3. TLS Configuration

For TLS-secured connections to NATS:

```toml
[nats]
urls = ["tls://nats-server:4222"]
tls_ca_file = "/etc/harmonylite/ca.pem"
tls_cert_file = "/etc/harmonylite/client-cert.pem"
tls_key_file = "/etc/harmonylite/client-key.pem"
```

## NATS JetStream Configuration

JetStream is NATS' persistence layer and is critical for HarmonyLite's replication. Here are key configuration parameters:

### Stream Settings

```toml
[replication_log]
# Number of streams to distribute changes
shards = 4

# Maximum stored messages per stream before purging old messages
max_entries = 1024

# Number of replicas for fault tolerance
replicas = 3

# Enable message compression to save bandwidth/storage
compress = true
```

### Consumer Settings

HarmonyLite automatically creates durable consumers for each node. The naming convention is:
- `harmonylite-node-<node_id>-<stream_name>-consumer`

For example: `harmonylite-node-1-harmonylite-changes-1-consumer`

## Multi-Region Deployment

For globally distributed applications, NATS can be configured to optimize cross-region traffic:

```mermaid
graph TB
    subgraph "Region A"
    A[NATS Server A] --- B[NATS Server B]
    B --- C[NATS Server C]
    end
    
    subgraph "Region B"
    D[NATS Server D] --- E[NATS Server E]
    E --- F[NATS Server F]
    end
    
    A -.-> D
    C -.-> F
    
    style A fill:#bbdefb
    style B fill:#bbdefb
    style C fill:#bbdefb
    style D fill:#c8e6c9
    style E fill:#c8e6c9
    style F fill:#c8e6c9
```

### Configuration Approach

1. **Deploy a Full NATS Cluster in Each Region**

2. **Configure Gateway Connections Between Regions**:
   ```
   # NATS server config in Region A
   gateway {
     name: "region-a"
     port: 7222
     gateways: [
       {name: "region-b", url: "nats://region-b-gateway:7222"}
     ]
   }
   ```

3. **Configure HarmonyLite to Connect to Local NATS Servers**:
   ```toml
   # For nodes in Region A
   [nats]
   urls = ["nats://region-a-server-1:4222", "nats://region-a-server-2:4222"]
   
   # For nodes in Region B
   [nats]
   urls = ["nats://region-b-server-1:4222", "nats://region-b-server-2:4222"]
   ```

## Monitoring NATS

### Built-in Monitoring Endpoint

NATS includes a built-in monitoring endpoint accessible via HTTP:

```
http://nats-server:8222/
```

This provides information about:
- Server status and connections
- JetStream status
- Cluster status
- Stream and consumer metrics

### Prometheus Integration

Enable Prometheus metrics in your NATS server configuration:

```
http_port: 8222


prometheus {
  port: 7777
}
```

### Key Metrics to Monitor

| Metric | Description | Alert Threshold |
|--------|-------------|----------------|
| `nats_jetstream_messages_count` | Total stored messages | N/A (trend) |
| `nats_jetstream_bytes_used` | Storage usage | >80% capacity |
| `nats_jetstream_consumer_ack_pending` | Pending message acknowledgments | >1000 for >5 min |
| `nats_jetstream_consumer_delivered_msgs` | Messages delivered to consumers | N/A (trend) |
| `nats_server_connections` | Connected clients | >90% of configured maximum |

## Common Issues and Solutions

### Connection Problems

**Symptoms**: HarmonyLite logs show "Unable to connect to NATS" errors.

**Solutions**:
- Verify NATS server addresses in `urls` are correct
- Check network connectivity and firewalls
- Ensure authentication credentials are correct
- Increase `connect_retries` and `reconnect_wait_seconds`

### Stream Creation Failures

**Symptoms**: "Failed to create stream" errors in logs.

**Solutions**:
- Verify NATS server has JetStream enabled
- Check storage capacity on NATS server
- Ensure proper permissions for JetStream operations
- Reduce `max_entries` or increase storage limits

### Message Publication Timeouts

**Symptoms**: High `pending_publish` metrics, slow replication.

**Solutions**:
- Increase number of shards (`replication_log.shards`)
- Enable compression (`replication_log.compress = true`)
- Check network latency between nodes
- Verify NATS server has sufficient resources

### Snapshot Storage Errors

**Symptoms**: "Unable to store snapshot" errors in logs.

**Solutions**:
- Verify NATS Object Store is properly configured
- Check disk space on NATS server
- Increase file size limits in NATS configuration
- Configure a different snapshot storage backend

## Best Practices

1. **Always Use Multiple NATS Servers** for high availability, at least three for a full cluster.

2. **Configure Proper Storage Paths** for JetStream data, avoiding `/tmp` for production.

3. **Enable Authentication** in production environments, preferably using NKeys or TLS.

4. **Monitor NATS Health** using the HTTP monitoring port and Prometheus metrics.

5. **Adjust Memory and Storage Limits** based on your workload characteristics.

6. **Implement Regular Backups** of NATS JetStream data for disaster recovery.

7. **Scale Shards with Write Volume** to maintain performance as your application grows.

8. **Use Compression** for high-volume or bandwidth-constrained environments.

================
File: docs/docs/production-deployment.md
================
# Production Deployment Guide

This guide provides comprehensive recommendations for deploying HarmonyLite in production environments. It covers hardware requirements, deployment architecture, security, monitoring, and maintenance procedures to ensure a reliable, high-performance system.

## System Requirements

### Hardware Recommendations

For production deployments, we recommend the following specifications per node:

| Component | Minimum | Recommended | High-Performance |
|-----------|---------|-------------|------------------|
| CPU | 2 cores | 4 cores | 8+ cores |
| RAM | 2 GB | 4-8 GB | 16+ GB |
| Storage | 20 GB SSD | 100+ GB SSD | 500+ GB NVMe SSD |
| Network | 100 Mbps | 1 Gbps | 10 Gbps |

Notes:
- CPU: Each additional core improves parallel processing capacity
- RAM: Larger for high-volume environments with many simultaneous connections
- Storage: SSD strongly recommended for performance; size based on your data volume and retention needs
- Network: Low latency is crucial for replication performance

### Operating System Recommendations

HarmonyLite works on most Linux distributions. We specifically recommend:

- Ubuntu Server 20.04/22.04 LTS
- Debian 11/12
- Amazon Linux 2/2023
- RHEL/CentOS 8 or newer

## Deployment Architecture

### Production Cluster Layout

For production deployments, we recommend a minimum of three nodes to ensure high availability:

```mermaid
graph TD
    subgraph "Node 1 (Primary Site)"
        HL1[HarmonyLite] --- DB1[(SQLite DB)]
        NATS1[NATS Server]
        HL1 --- NATS1
    end
    
    subgraph "Node 2 (Primary Site)"
        HL2[HarmonyLite] --- DB2[(SQLite DB)]
        NATS2[NATS Server]
        HL2 --- NATS2
    end
    
    subgraph "Node 3 (Secondary Site)"
        HL3[HarmonyLite] --- DB3[(SQLite DB)]
        NATS3[NATS Server]
        HL3 --- NATS3
    end
    
    NATS1 --- NATS2
    NATS1 --- NATS3
    NATS2 --- NATS3
    
    APP[Applications] --> HL1
    APP --> HL2
    APP --> HL3
    
    style HL1 fill:#bbdefb
    style HL2 fill:#bbdefb
    style HL3 fill:#bbdefb
    style NATS1 fill:#c8e6c9
    style NATS2 fill:#c8e6c9
    style NATS3 fill:#c8e6c9
    style DB1 fill:#ffccbc
    style DB2 fill:#ffccbc
    style DB3 fill:#ffccbc
```

For larger deployments, you can add additional nodes as needed, potentially with specialized roles:

- **Write-Heavy Nodes**: Optimized for high write throughput
- **Read-Only Nodes**: Configured to only replicate data
- **Edge Nodes**: Deployed closer to users for lower latency reads

### Network Configuration

Ensure these ports are open between HarmonyLite nodes:

| Port | Protocol | Purpose | Direction |
|------|----------|---------|-----------|
| 4222 | TCP | NATS client connections | Bidirectional |
| 6222 | TCP | NATS route connections | Bidirectional |
| 7222 | TCP | NATS gateway connections (multi-region) | Bidirectional |
| 8222 | TCP | NATS monitoring (optional) | Inbound |
| 3010 | TCP | Prometheus metrics (optional) | Inbound |

Use firewall rules to restrict access to these ports from unauthorized hosts.

## Storage Configuration

### File System Recommendations

For production deployments:

- **File System**: XFS or ext4
- **Mount Options**: `noatime,nodiratime` to reduce I/O overhead
- **I/O Scheduler**: `none` for NVMe SSDs, `mq-deadline` for SATA SSDs
- **RAID**: RAID-10 for high-performance setups, RAID-1 for basic redundancy

### Disk Layout

Organize your storage with separate volumes:

1. **Operating System**: 20-50 GB
2. **HarmonyLite Binaries**: 1 GB
3. **Database Files**: Size according to your data volume plus 100% growth buffer
4. **NATS JetStream Storage**: Size to accommodate at least 2x your expected change volume

Example layout for a production server:

```
/dev/sda1    /                    ext4    defaults                      50G
/dev/sda2    /var/lib/harmonylite xfs     noatime,nodiratime           200G
/dev/sda3    /var/lib/nats        xfs     noatime,nodiratime           100G
/dev/sda4    /var/log             ext4    defaults                     50G
```

## Setting Up Production Nodes

### Creating a Dedicated User

Create a system user for running HarmonyLite:

```bash
# Create user and required directories
sudo useradd -r -s /bin/false harmonylite
sudo mkdir -p /etc/harmonylite /var/lib/harmonylite /var/log/harmonylite

# Set permissions
sudo chown -R harmonylite:harmonylite /etc/harmonylite /var/lib/harmonylite /var/log/harmonylite
sudo chmod 750 /etc/harmonylite /var/lib/harmonylite /var/log/harmonylite
```

### Configuration File

Create a production configuration file at `/etc/harmonylite/config.toml`:

```toml
# Database settings
db_path = "/var/lib/harmonylite/data.db"
node_id = 1  # Unique per node
seq_map_path = "/var/lib/harmonylite/seq-map.cbor"
cleanup_interval = 30000  # 30 seconds

[replication_log]
shards = 4
max_entries = 2048
replicas = 3
compress = true

[snapshot]
enabled = true
store = "s3"  # Or another supported backend
interval = 3600000  # 1 hour

[snapshot.s3]
endpoint = "s3.amazonaws.com"
bucket = "your-backup-bucket"
path = "harmonylite/snapshots"
use_ssl = true
access_key = "your-access-key"
secret = "your-secret-key"
region = "us-west-2"

[nats]
urls = ["nats://nats-server-1:4222", "nats://nats-server-2:4222", "nats://nats-server-3:4222"]
connect_retries = 10
reconnect_wait_seconds = 5

[prometheus]
enable = true
bind = "127.0.0.1:3010"  # Only bind to localhost if using a reverse proxy

[logging]
verbose = false  # Less verbose in production
format = "json"  # Structured logging
level = "info"
file = "/var/log/harmonylite/harmonylite.log"
```

Customize configuration options for your specific environment.

### Systemd Service

Create a systemd service file at `/etc/systemd/system/harmonylite.service`:

```ini
[Unit]
Description=HarmonyLite SQLite Replication Service
After=network.target
Documentation=https://github.com/wongfei2009/harmonylite

[Service]
User=harmonylite
Group=harmonylite
Type=simple
ExecStart=/usr/local/bin/harmonylite -config /etc/harmonylite/config.toml
Restart=on-failure
RestartSec=5s
LimitNOFILE=65536
WorkingDirectory=/var/lib/harmonylite
StandardOutput=journal
StandardError=journal

# Hardening options
ProtectSystem=full
PrivateTmp=true
NoNewPrivileges=true
ProtectHome=true
ProtectControlGroups=true
ProtectKernelModules=true
ReadWritePaths=/var/lib/harmonylite /var/log/harmonylite

[Install]
WantedBy=multi-user.target
```

Enable and start the service:

```bash
sudo systemctl daemon-reload
sudo systemctl enable harmonylite
sudo systemctl start harmonylite
```

### Log Rotation

Create a logrotate configuration at `/etc/logrotate.d/harmonylite`:

```
/var/log/harmonylite/*.log {
    daily
    missingok
    rotate 14
    compress
    delaycompress
    notifempty
    create 0640 harmonylite harmonylite
    sharedscripts
    postrotate
        systemctl reload harmonylite
    endscript
}
```

## Monitoring and Alerting

### Prometheus Integration

HarmonyLite exposes metrics via the Prometheus endpoint configured in `[prometheus]`. Set up a Prometheus server to scrape these metrics:

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'harmonylite'
    scrape_interval: 15s
    static_configs:
      - targets: ['harmonylite-node-1:3010', 'harmonylite-node-2:3010', 'harmonylite-node-3:3010']
```

### Critical Metrics to Monitor

| Metric | Description | Warning Threshold | Critical Threshold |
|--------|-------------|-------------------|-------------------|
| `harmonylite_published` | Published changes counter | N/A (trend) | N/A (trend) |
| `harmonylite_pending_publish` | Pending changes | > 500 for > 5 min | > 1000 for > 10 min |
| `harmonylite_count_changes` | Change counting latency | P95 > 200ms | P95 > 500ms |
| `harmonylite_scan_changes` | Change scanning latency | P95 > 500ms | P95 > 1s |
| `harmonylite_snapshot_age` | Time since last snapshot | > 2h | > 6h |
| `harmonylite_node_status` | Node status (1=healthy) | < 1 | < 1 for > 5 min |

### Alert Rules Example

```yaml
# harmonylite_alerts.yml
groups:
- name: harmonylite
  rules:
  - alert: HarmonyLitePendingPublishHigh
    expr: harmonylite_pending_publish > 1000
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "HarmonyLite pending publish queue high"
      description: "Node {{ $labels.instance }} has {{ $value }} pending changes for over 10 minutes"

  - alert: HarmonyLiteNodeDown
    expr: harmonylite_node_status < 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "HarmonyLite node down"
      description: "Node {{ $labels.instance }} is not responding for over 5 minutes"
```

### Log Monitoring

Use a log aggregation system such as Elasticsearch, Loki, or Graylog to collect and analyze logs from all nodes. Configure alerts for critical error patterns.

## Backup and Recovery

### Regular Snapshots

Configure automatic snapshots with the `[snapshot]` section. For production, we recommend:

```toml
[snapshot]
enabled = true
interval = 3600000  # 1 hour
store = "s3"  # S3, WebDAV, or SFTP recommended for production
min_changes = 5000  # Minimum changes before forcing a snapshot

### Backup Strategies

Implement multiple layers of backup:

1. **HarmonyLite Snapshots**: Automatic snapshots via the configuration
2. **Database Backups**: Regular SQLite backups independent of HarmonyLite
3. **Filesystem Backups**: Server-level backups of the entire `/var/lib/harmonylite` directory

Sample backup script for SQLite:

```bash
#!/bin/bash
# /usr/local/bin/backup-harmonylite.sh

DATE=$(date +%Y%m%d-%H%M)
BACKUP_DIR="/var/backups/harmonylite"
DB_PATH="/var/lib/harmonylite/data.db"
BACKUP_PATH="${BACKUP_DIR}/harmonylite-${DATE}.db"

# Create backup directory if it doesn't exist
mkdir -p $BACKUP_DIR

# Create a backup copy using SQLite's CLI
sqlite3 $DB_PATH ".backup $BACKUP_PATH"

# Compress the backup
gzip $BACKUP_PATH

# Retain only the last 7 days of backups
find $BACKUP_DIR -name "harmonylite-*.db.gz" -type f -mtime +7 -delete

# Optional: Upload to remote storage
# aws s3 cp $BACKUP_PATH.gz s3://your-backup-bucket/harmonylite/
```

Add to crontab:
```
0 */4 * * * /usr/local/bin/backup-harmonylite.sh
```

### Recovery Procedures

To recover a node using the latest snapshot:

1. Stop the HarmonyLite service:
   ```bash
   sudo systemctl stop harmonylite
   ```

2. Remove the database and sequence map files:
   ```bash
   sudo rm /var/lib/harmonylite/data.db /var/lib/harmonylite/seq-map.cbor
   ```

3. Restart the service to trigger automatic restore:
   ```bash
   sudo systemctl start harmonylite
   ```

To recover from a backup file instead:

1. Stop the HarmonyLite service:
   ```bash
   sudo systemctl stop harmonylite
   ```

2. Remove existing database:
   ```bash
   sudo rm /var/lib/harmonylite/data.db
   ```

3. Restore from backup:
   ```bash
   sudo gunzip -c /var/backups/harmonylite/harmonylite-20230428-1200.db.gz > /var/lib/harmonylite/data.db
   sudo chown harmonylite:harmonylite /var/lib/harmonylite/data.db
   ```

4. Remove sequence map to force reinitialization:
   ```bash
   sudo rm /var/lib/harmonylite/seq-map.cbor
   ```

5. Restart the service:
   ```bash
   sudo systemctl start harmonylite
   ```

## Security Considerations

### Network Security

- Deploy nodes in a private network/VPC
- Use security groups/firewall rules to restrict access
- Consider a VPN for cross-region communication
- Use TLS for all NATS connections in production

### Authentication

For NATS authentication in production:

```toml
[nats]
# Option 1: Username/Password
user_name = "harmonylite"
user_password = "secure-random-password"

# Option 2: NKey authentication (preferred)
seed_file = "/etc/harmonylite/nkeys/user.nkey"
```

### Data Security

- Apply proper filesystem permissions
- Encrypt sensitive configuration values
- Use encrypted S3 buckets for snapshots
- Consider disk encryption for database volumes

### Secure Configuration Example

```toml
[nats]
urls = ["tls://nats-server-1:4222"]
seed_file = "/etc/harmonylite/nkeys/user.nkey"
tls_ca_file = "/etc/harmonylite/tls/ca.pem"
tls_cert_file = "/etc/harmonylite/tls/client-cert.pem"
tls_key_file = "/etc/harmonylite/tls/client-key.pem"

[snapshot.s3]
endpoint = "s3.amazonaws.com"
bucket = "your-encrypted-bucket"
use_ssl = true
access_key = "your-access-key"
secret = "your-secret-key"
region = "us-west-2"
server_side_encryption = "AES256"
```

## Scaling Strategies

### Horizontal Scaling

To scale the system horizontally:

1. Add more nodes with unique `node_id` values
2. Update NATS cluster configuration to include new nodes
3. Configure applications to distribute reads across all nodes

Writes can go to any node, but you may want to designate preferred write nodes for heavy write workloads.

### Shard Scaling

Increase the number of shards as write volume grows:

```toml
[replication_log]
shards = 8  # Start with 1-2, increase as needed
```

Each shard adds parallel processing capability but requires more NATS resources.

### Multi-Region Deployment

For global deployments, use NATS gateways to connect regions:

```mermaid
graph TB
    subgraph "US Region"
    US1[Node 1] --- US2[Node 2]
    US2 --- US3[Node 3]
    end
    
    subgraph "EU Region"
    EU1[Node 4] --- EU2[Node 5]
    EU2 --- EU3[Node 6]
    end
    
    subgraph "APAC Region"
    AP1[Node 7] --- AP2[Node 8]
    end
    
    US2 -.- EU2
    EU2 -.- AP2
    
    style US1 fill:#bbdefb
    style US2 fill:#bbdefb
    style US3 fill:#bbdefb
    style EU1 fill:#c8e6c9
    style EU2 fill:#c8e6c9
    style EU3 fill:#c8e6c9
    style AP1 fill:#ffccbc
    style AP2 fill:#ffccbc
```

Use regional configurations for optimal performance:

```toml
# US nodes
[nats]
urls = ["nats://us-nats-1:4222", "nats://us-nats-2:4222"]

# EU nodes
[nats]
urls = ["nats://eu-nats-1:4222", "nats://eu-nats-2:4222"]
```

## Maintenance Procedures

### Version Upgrades

To upgrade HarmonyLite:

1. Download the new version

2. Back up configuration files:
   ```bash
   sudo cp /etc/harmonylite/config.toml /etc/harmonylite/config.toml.bak
   ```

3. Stop the service:
   ```bash
   sudo systemctl stop harmonylite
   ```

4. Replace the binary:
   ```bash
   sudo cp harmonylite /usr/local/bin/
   sudo chmod +x /usr/local/bin/harmonylite
   ```

5. Start the service:
   ```bash
   sudo systemctl start harmonylite
   ```

6. Verify logs for successful startup:
   ```bash
   sudo journalctl -u harmonylite -f
   ```

Perform upgrades one node at a time to maintain availability.

### Schema Changes

When making schema changes to your SQLite database:

1. Stop applications writing to the database
2. Apply schema changes on one node
3. Run cleanup to reset triggers:
   ```bash
   harmonylite -config /etc/harmonylite/config.toml -cleanup
   ```
4. Restart HarmonyLite on that node:
   ```bash
   sudo systemctl restart harmonylite
   ```
5. Wait for changes to replicate
6. Repeat for other nodes
7. Resume application connections

### Routine Maintenance

1. **Log Rotation**: Ensured by logrotate configuration
2. **Cleanup**: Periodically run `harmonylite -cleanup` to remove old change logs
3. **Database Optimization**: Run `VACUUM` periodically on low-traffic periods
4. **Snapshot Testing**: Periodically test recovery from snapshots

## Performance Tuning

### Database Tuning

1. Set appropriate SQLite PRAGMA values in your application:
   ```sql
   PRAGMA journal_mode = WAL;
   PRAGMA synchronous = NORMAL;
   PRAGMA cache_size = -16000;  -- 16MB cache
   PRAGMA foreign_keys = ON;
   PRAGMA temp_store = MEMORY;
   ```

2. Keep indexes efficient:
   - Index columns used in WHERE clauses
   - Avoid over-indexing (indexes have write overhead)
   - Consider partial indexes for large tables

### Replication Tuning

Adjust these parameters based on your workload:

```toml
# High write throughput
[replication_log]
shards = 8
max_entries = 4096

# Low latency priority
cleanup_interval = 15000  # 15 seconds
```

### System Tuning

1. **Filesystem**: Adjust mount options for performance
   ```
   noatime,nodiratime,nobarrier
   ```

2. **I/O Scheduler**: Set appropriate scheduler
   ```bash
   echo none > /sys/block/nvme0n1/queue/scheduler  # For NVMe
   ```

3. **Memory Management**: Adjust swappiness
   ```bash
   echo 10 > /proc/sys/vm/swappiness
   ```

## Troubleshooting Common Issues

### Replication Delays

**Symptoms**: Changes take longer than expected to propagate.

**Solutions**:
- Check NATS connectivity between nodes
- Increase number of shards
- Check for network congestion or latency
- Verify adequate disk I/O performance

### Database Locks

**Symptoms**: "database is locked" errors in logs.

**Solutions**:
- Check for long-running transactions in your application
- Ensure applications use proper transaction handling
- Increase SQLite busy timeout in your application
- Verify proper WAL journal mode

### Memory Issues

**Symptoms**: High memory usage or out-of-memory errors.

**Solutions**:
- Decrease `max_entries` if very large
- Check for memory leaks in applications
- Consider increasing server memory

### Snapshot Failures

**Symptoms**: Snapshot creation or restoration fails.

**Solutions**:
- Check storage provider connectivity
- Verify credentials and permissions
- Ensure adequate disk space
- Check for database corruption

## Conclusion

A well-planned production deployment of HarmonyLite provides a robust foundation for distributed SQLite applications. By following these guidelines for hardware, configuration, monitoring, and maintenance, you can achieve a reliable, high-performance system that scales with your needs.

Remember these key principles:
- Start with at least three nodes for high availability
- Use proper authentication and network security
- Implement comprehensive monitoring and alerting
- Perform regular backups and test recovery procedures
- Scale horizontally as your workload grows

For additional assistance, consult the [troubleshooting guide](troubleshooting.md) or reach out to the community for support.

================
File: docs/docs/quick-start.md
================
# Quick Start Guide

This guide walks you through the process of setting up HarmonyLite on your system and creating a basic replication cluster. Follow these steps to get started with distributed SQLite replication in minutes.

## Prerequisites

Before proceeding, ensure your system meets the following requirements:

- **Operating System**: Linux or macOS (Windows users can use WSL)
- **SQLite**: Version 3.35.0 or newer
- **tar**: Required to extract the release archive
- **Internet Access**: To download the latest release

## Installing Dependencies

### On Ubuntu/Debian

```bash
sudo apt update
sudo apt install -y sqlite3 tar
```

### On macOS (using Homebrew)

```bash
brew install sqlite tar
```

### Verifying Installation

Check that SQLite is properly installed:

```bash
sqlite3 --version
```

You should see output like `SQLite 3.x.x`.

## Download and Install HarmonyLite

1. **Download the Latest Release**

   ```bash
   # Replace 'vX.Y.Z' with the latest version from GitHub releases
   curl -L https://github.com/wongfei2009/harmonylite/releases/download/vX.Y.Z/harmonylite-vX.Y.Z-linux-amd64.tar.gz -o harmonylite.tar.gz
   ```

2. **Extract the Archive**

   ```bash
   tar -xzf harmonylite.tar.gz
   cd harmonylite-vX.Y.Z
   ```

3. **Move the Binary to a System Path** (optional)

   ```bash
   sudo mv harmonylite /usr/local/bin/
   ```

## Create a Simple Two-Node Cluster

We'll set up a basic cluster with two nodes replicating a simple database.

### Step 1: Create Configuration Files

First, create configuration files for each node:

**node1-config.toml**:
```toml
# Node 1 Configuration
db_path = "/tmp/harmonylite-1.db"
node_id = 1
seq_map_path = "/tmp/harmonylite-1-seq-map.cbor"

[replication_log]
shards = 1
max_entries = 1024
replicas = 2

[snapshot]
enabled = true
interval = 3600000
store = "nats"
```

**node2-config.toml**:
```toml
# Node 2 Configuration
db_path = "/tmp/harmonylite-2.db"
node_id = 2
seq_map_path = "/tmp/harmonylite-2-seq-map.cbor"

[replication_log]
shards = 1
max_entries = 1024
replicas = 2

[snapshot]
enabled = true
interval = 3600000
store = "nats"
```

### Step 2: Initialize a Test Database

Create a simple test database with a table:

```bash
sqlite3 /tmp/harmonylite-1.db <<EOF
CREATE TABLE notes (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
EOF
```

```bash
sqlite3 /tmp/harmonylite-2.db <<EOF
CREATE TABLE notes (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
EOF
```

### Step 3: Start the First Node

Start the first HarmonyLite node:

```bash
./harmonylite -config node1-config.toml -cluster-addr localhost:4221 -cluster-peers nats://localhost:4222/
```

### Step 4: Start the Second Node

Open a new terminal window and start the second node:

```bash
./harmonylite -config node2-config.toml -cluster-addr localhost:4222 -cluster-peers nats://localhost:4221/
```

### Step 5: Test Replication

1. Open a new terminal window and insert data to the first node:

   ```bash
   sqlite3 /tmp/harmonylite-1.db
   ```

   In the SQLite shell:

   ```sql
   PRAGMA trusted_schema = ON;
   INSERT INTO notes (title, content) VALUES ('First Note', 'This is a test of HarmonyLite replication');
   .exit
   ```

2. Verify replication to the second node:

   ```bash
   sqlite3 /tmp/harmonylite-2.db
   ```

   In the SQLite shell:

   ```sql
   SELECT * FROM notes;
   .exit
   ```

   You should see the record you inserted in the first node.

## Next Steps

Congratulations! You've set up a basic HarmonyLite cluster. From here, you can:

- Try our [complete demo](demo.md) with Pocketbase
- Learn about [architecture and concepts](architecture.md)
- Explore [configuration options](configuration-reference.md)
- Set up a [production deployment](production-deployment.md)

## Troubleshooting

If you encounter issues:

- Check that both nodes are running and connected
- Ensure `PRAGMA trusted_schema = ON` is set before inserting data
- Verify that port 4221 and 4222 are available on your system
- Check log output for error messages
- See the [troubleshooting guide](troubleshooting.md) for more help

================
File: docs/docs/replication.md
================
# Replication Mechanism

This document provides a deep dive into HarmonyLite's replication process, explaining how changes propagate between nodes, how conflicts are resolved, and how to optimize the system for different workloads.

## Core Principles

HarmonyLite implements a **leaderless**, **eventually consistent** replication system with the following design principles:

1. **Any node can accept writes** without coordination
2. **Changes propagate asynchronously** to all nodes
3. **Last-writer-wins** for conflict resolution
4. **Eventual consistency** rather than strong consistency
5. **Parallel processing** through sharding

This approach prioritizes **availability** and **partition tolerance** over immediate consistency, making HarmonyLite an "AP" system in terms of the CAP theorem.

## Change Data Capture (CDC)

### Trigger Mechanism

HarmonyLite captures database changes using SQLite triggers:

```mermaid
flowchart LR
    A[Application] -->|"INSERT/UPDATE/DELETE"| B[SQLite Database]
    B -->|"AFTER Trigger"| C[Change Log Tables]
    C -->|"Poll Changes"| D[HarmonyLite Process]
    D -->|"Publish"| E[NATS JetStream]
```

The system creates three triggers for each monitored table:

1. **INSERT Trigger**: Captures new row data
2. **UPDATE Trigger**: Captures modified row data
3. **DELETE Trigger**: Captures deleted row IDs

Here's a simplified example of an INSERT trigger:

```sql
CREATE TRIGGER __harmonylite__users_change_log_on_insert
AFTER INSERT ON users
BEGIN
    INSERT INTO __harmonylite__users_change_log(
        val_id, val_name, val_email, 
        type, created_at, state
    ) VALUES(
        NEW.id, NEW.name, NEW.email,
        'insert',
        CAST((strftime('%s','now') || substr(strftime('%f','now'),4)) AS INT),
        0 -- Pending state
    );

    INSERT INTO __harmonylite___global_change_log (change_table_id, table_name)
    VALUES (
        last_insert_rowid(),
        'users'
    );
END;
```

### Change Log Tables

Two types of tables track changes:

1. **Per-Table Change Logs**: Store specific changes to each table
   - Named `__harmonylite__<table_name>_change_log`
   - Contain columns for all original table fields (prefixed with `val_`)
   - Include metadata like operation type, timestamp, and state

2. **Global Change Log**: Tracks the sequence of all changes
   - Named `__harmonylite___global_change_log`
   - References the specific change log entries
   - Provides a chronological record of all operations

### Schema Example

```mermaid
erDiagram
    USERS {
        int id PK
        string name
        string email
    }
    
    GLOBAL_CHANGE_LOG {
        int id PK
        int change_table_id FK
        string table_name
    }
    
    USERS_CHANGE_LOG {
        int id PK
        int val_id
        string val_name
        string val_email
        string type
        int created_at
        int state
    }
    
    USERS ||--o{ USERS_CHANGE_LOG : "triggers create"
    USERS_CHANGE_LOG ||--o{ GLOBAL_CHANGE_LOG : "references"
```

## Change Detection and Publishing

### Polling Mechanism

HarmonyLite periodically checks for changes using a polling mechanism:

1. **File Monitoring**: Watches the SQLite database and WAL files for changes
2. **Change Detection**: When changes are detected, queries the change log tables
3. **State Filtering**: Retrieves only changes in the "pending" state (state = 0)

### Hash-Based Sharding

For efficient parallel processing, changes are distributed across multiple streams based on a hash:

1. **Hash Calculation**: Computed from table name and primary key values
2. **Shard Selection**: The hash determines which stream receives the change
3. **Message Distribution**: Changes to the same row always go to the same stream

This ensures that changes to the same row are processed in order, while allowing changes to different rows to be processed in parallel.

### Change Serialization

Changes are serialized for transmission using CBOR (Concise Binary Object Representation):

```json
{
  "FromNodeId": 1,
  "Payload": {
    "Id": 123,
    "Type": "insert",
    "TableName": "users",
    "Row": {
      "id": 456,
      "name": "John Doe",
      "email": "john@example.com"
    }
  }
}
```

Optional compression using zstd can be enabled to reduce bandwidth usage.

### NATS JetStream Integration

HarmonyLite uses NATS JetStream for reliable message delivery:

1. **Stream Creation**: Creates persistent streams for each shard
2. **Publication**: Publishes changes to the appropriate stream
3. **Acknowledgment**: Waits for confirmation from NATS
4. **State Update**: Marks changes as published in the database (state = 1)

## Change Application

### Message Flow

Here's the complete message flow from one node to another:

```mermaid
sequenceDiagram
    participant App1 as Application (Node 1)
    participant DB1 as SQLite DB (Node 1)
    participant HL1 as HarmonyLite (Node 1)
    participant NATS as NATS JetStream
    participant HL2 as HarmonyLite (Node 2)
    participant DB2 as SQLite DB (Node 2)
    participant App2 as Application (Node 2)
    
    App1->>DB1: INSERT/UPDATE/DELETE
    DB1->>DB1: Trigger executes
    DB1->>DB1: Record in change log
    
    HL1->>DB1: Poll for changes
    DB1->>HL1: Return pending changes
    HL1->>HL1: Calculate hash
    HL1->>NATS: Publish to stream
    NATS->>HL1: Acknowledge receipt
    HL1->>DB1: Mark as published
    
    NATS->>HL2: Deliver change
    HL2->>HL2: Process change
    HL2->>DB2: Apply change
    HL2->>NATS: Acknowledge processing
    
    App2->>DB2: Read updated data
    DB2->>App2: Return data
```

### Processing on the Receiving Node

When a node receives a change message:

1. **Origin Check**: Verifies the change didn't originate from this node (prevents loops)
2. **Duplicate Check**: Ensures the change hasn't been applied already
3. **Table Existence**: Confirms the target table exists
4. **Operation Parsing**: Determines the operation type (INSERT, UPDATE, DELETE)
5. **SQL Generation**: Constructs appropriate SQL statements
6. **Statement Execution**: Applies the change to the local database
7. **Sequence Tracking**: Updates the sequence map for recovery purposes

### SQL Statement Generation

HarmonyLite generates SQL statements based on the operation type:

- **INSERT**: `INSERT INTO table (col1, col2) VALUES (val1, val2)`
- **UPDATE**: `UPDATE table SET col1 = val1, col2 = val2 WHERE id = ?`
- **DELETE**: `DELETE FROM table WHERE id = ?`

These statements are executed using SQLite's native APIs.

## Conflict Resolution

### Last-Writer-Wins Strategy

HarmonyLite uses a timestamp-based last-writer-wins (LWW) strategy for conflict resolution:

1. **Timestamp Comparison**: Changes include microsecond-precision timestamps
2. **Latest Wins**: When two nodes modify the same row, the later change prevails
3. **Node ID Tiebreaker**: If timestamps are identical, higher node ID wins

This approach ensures eventual consistency but may result in lost updates in high-conflict scenarios.

### Conflict Scenarios

Consider this example scenario:

1. Node 1 updates a user's email to "john@example.com" at timestamp 1000
2. Node 2 updates the same user's email to "john@company.com" at timestamp 1050
3. Both changes replicate to all nodes
4. The final state will be "john@company.com" (from Node 2) because it has a later timestamp

### Multi-Column Conflicts

If different nodes update different columns of the same row:

1. Node 1 updates a user's email at timestamp 1000
2. Node 2 updates the same user's phone at timestamp 1050
3. Column-level merging does not occur
4. The entire row from Node 2 replaces the row from Node 1
5. The email update from Node 1 is lost

## Replication Configuration

### Sharding Configuration

```toml
[replication_log]
# Number of shards for parallel processing
shards = 4

# Maximum entries per stream before cleanup
max_entries = 1024

# Number of stream replicas for fault tolerance
replicas = 3

# Enable compression for network efficiency
compress = true
```

Increasing `shards` improves parallel processing but requires more NATS resources.

### Write-Only and Read-Only Nodes

HarmonyLite supports specialized node configurations:

```toml
# Write-only node (doesn't apply incoming changes)
publish = true
replicate = false

# Read-only node (doesn't publish changes)
publish = false
replicate = true
```

These settings enable custom topologies like hub-and-spoke or primary-replica.

## Performance Considerations

### Optimizing Replication Performance

To improve replication performance:

1. **Increase Shards** for parallel processing:
   ```toml
   [replication_log]
   shards = 8  # More shards for higher throughput
   ```

2. **Enable Compression** for bandwidth efficiency:
   ```toml
   [replication_log]
   compress = true
   ```

3. **Tune Cleanup Interval** for reduced overhead:
   ```toml
   # Time in milliseconds between change log cleanup runs
   cleanup_interval = 60000  # 1 minute
   ```

### Transaction Batching

When making multiple changes, use transactions to reduce overhead:

```sql
BEGIN TRANSACTION;
INSERT INTO users (name, email) VALUES ('John', 'john@example.com');
INSERT INTO users (name, email) VALUES ('Jane', 'jane@example.com');
COMMIT;
```

This reduces trigger overhead and improves replication efficiency.

### Network Considerations

For optimal performance:

1. **Low-Latency Connections**: Keep network latency below 50ms when possible
2. **Sufficient Bandwidth**: Ensure adequate bandwidth for your change volume
3. **Reliable Network**: Use retry mechanisms for unreliable connections
4. **Regional Clustering**: Group nodes geographically for better performance

## Replication Guarantees and Limitations

### What HarmonyLite Guarantees

HarmonyLite provides these guarantees:

1. **Eventual Consistency**: All nodes eventually reach the same state
2. **Change Preservation**: Changes are not lost if a node is temporarily offline
3. **Order Preservation**: Changes to the same row are applied in order
4. **Fault Tolerance**: The system continues to function despite node failures

### Limitations

Important limitations to be aware of:

1. **No Strong Consistency**: There's a window where nodes have different states
2. **No Cross-Row Transactions**: Transactions spanning multiple tables may not be atomic across nodes
3. **Last-Writer-Wins only**: No custom conflict resolution strategies
4. **No Quorum Enforcement**: Updates proceed without waiting for acknowledgment from other nodes

## Advanced Replication Patterns

### Multi-Region Deployment

For global deployments, configure NATS gateways between regions:

```mermaid
graph TB
    subgraph "Region A"
    A1[Node 1] --- A2[Node 2]
    A2 --- A3[Node 3]
    end
    
    subgraph "Region B"
    B1[Node 4] --- B2[Node 5]
    B2 --- B3[Node 6]
    end
    
    A2 -.-> B2
    
    style A1 fill:#bbdefb
    style A2 fill:#bbdefb
    style A3 fill:#bbdefb
    style B1 fill:#c8e6c9
    style B2 fill:#c8e6c9
    style B3 fill:#c8e6c9
```

This setup enables efficient cross-region replication with local write capability.

### Hub-and-Spoke Pattern

For edge computing scenarios:

```mermaid
graph TB
    H[Hub Node] --- E1[Edge Node 1]
    H --- E2[Edge Node 2]
    H --- E3[Edge Node 3]
    H --- E4[Edge Node 4]
    
    style H fill:#f44336,color:white
    style E1 fill:#bbdefb
    style E2 fill:#bbdefb
    style E3 fill:#bbdefb
    style E4 fill:#bbdefb
```

Configure edge nodes with:
```toml
# Edge nodes primarily consume updates
publish = true   # Can publish when needed
replicate = true # Always receive updates from hub
```

### Read Scaling Pattern

For read-heavy workloads:

```mermaid
graph TB
    W1[Write Node 1] --- W2[Write Node 2]
    W1 --- R1[Read-Only Node 1]
    W1 --- R2[Read-Only Node 2]
    W1 --- R3[Read-Only Node 3]
    W1 --- R4[Read-Only Node 4]
    W2 --- R1
    W2 --- R2
    W2 --- R3
    W2 --- R4
    
    style W1 fill:#f44336,color:white
    style W2 fill:#f44336,color:white
    style R1 fill:#bbdefb
    style R2 fill:#bbdefb
    style R3 fill:#bbdefb
    style R4 fill:#bbdefb
```

Configure read-only nodes with:
```toml
# Read-only nodes don't publish changes
publish = false
replicate = true
```

## Monitoring Replication

### Key Metrics to Watch

Monitor these metrics for replication health:

1. **Published Changes**: Rate of change publishing
   ```
   harmonylite_published
   ```

2. **Pending Changes**: Changes waiting for publication
   ```
   harmonylite_pending_publish
   ```

3. **Processing Latency**: Time to process changes
   ```
   harmonylite_count_changes
   harmonylite_scan_changes
   ```

## Troubleshooting Replication

### Common Issues

1. **Changes not propagating**:
   - Check NATS connectivity
   - Verify trigger installation
   - Ensure `PRAGMA trusted_schema = ON`
   - Check the `publish` and `replicate` settings

2. **High replication latency**:
   - Increase the number of shards
   - Enable compression for large changes
   - Check network performance
   - Tune the cleanup interval

3. **Conflicts and lost updates**:
   - Review your application's update patterns
   - Consider implementing application-level conflict handling
   - Use transactions where appropriate

4. **Database locks during replication**:
   - Ensure proper WAL mode configuration
   - Set appropriate timeout values
   - Check for long-running transactions

## Conclusion

HarmonyLite's replication mechanism provides a flexible, efficient way to maintain database consistency across distributed nodes. By understanding these internals, you can better optimize, monitor, and troubleshoot your deployment.

For more information on related topics, see:
- [Snapshots](snapshots.md) - How database state is preserved and restored
- [NATS Configuration](nats-configuration.md) - Details on messaging configuration
- [Production Deployment](production-deployment.md) - Best practices for production

================
File: docs/docs/snapshots.md
================
# Snapshot Management

This document explains HarmonyLite's snapshot system, which is critical for efficient node recovery and synchronization. Snapshots provide a way to quickly bootstrap new nodes or resynchronize nodes that have been offline for extended periods.

## Why Snapshots Matter

In a distributed system like HarmonyLite, snapshots serve several essential purposes:

1. **Efficient Recovery**: New or recovering nodes can start from a recent state rather than replaying all historical changes
2. **Performance Optimization**: Reduces the need to store and process all historical change logs
3. **Storage Management**: Allows for cleanup of old change records that have been superseded
4. **Disaster Recovery**: Provides point-in-time backups of database state

Without snapshots, nodes would need to replay the entire history of changes from the beginning of time, which becomes impractical as the system ages.

## How Snapshots Work

HarmonyLite's snapshot system operates through several coordinated components:

### Sequence Map

The Sequence Map is a critical component that tracks processed message sequences across JetStream shards:

- **Purpose**: Records the last processed message sequence for each stream
- **Implementation**: Stored as a key-value map (stream name → sequence number) serialized with CBOR
- **Location**: Specified via `seq_map_path` in the configuration
- **Recovery Role**: Acts as a database checkpoint to avoid reprocessing messages

```mermaid
graph LR
    SM[Sequence Map<br>seq-map.cbor] -->|Tracks| SS[Streams State]
    SS -->|Determines| NG{Need Snapshot?}
    NG -->|Yes| RS[Restore Snapshot]
    NG -->|No| CP[Continue Processing]
    SM -->|Enables| WR[Warm Restarts]
    WR -->|Skip| AP[Already Processed<br>Messages]
```

### Snapshot Creation Process

Snapshots are created based on configured criteria or manual triggers:

```mermaid
graph TB
    A[Monitor Sequence Numbers] --> B{Threshold Reached?}
    B -->|Yes| C[Create Temp Directory]
    B -->|No| A
    C --> D["VACUUM INTO (Temp Copy)"]
    D --> E[Remove Triggers & Logs]
    E --> F[Optimize with VACUUM]
    F --> G[Upload Snapshot]
    G --> H[Update Sequence Map]
    H --> A
```

The process involves:

1. **Determining Need**: Checking if sequence numbers have advanced enough to warrant a snapshot
2. **Creating Clean Copy**: Using SQLite's `VACUUM INTO` to create a copy without WAL files
3. **Cleaning Up**: Removing HarmonyLite-specific tables and triggers for a clean snapshot
4. **Optimizing**: Running `VACUUM` to optimize storage
5. **Storing**: Uploading the snapshot to the configured storage backend

================
File: docs/docs/troubleshooting.md
================
# Troubleshooting Guide

This guide helps you diagnose and resolve common issues with HarmonyLite deployments. It covers installation problems, replication issues, performance bottlenecks, and recovery procedures.

## Diagnostic Tools

Before diving into specific issues, familiarize yourself with these diagnostic tools:

### Log Analysis

HarmonyLite logs provide valuable troubleshooting information. Enable verbose logging temporarily:

```toml
[logging]
verbose = true
format = "json"  # or "console" for human-readable format
```

### Prometheus Metrics

Enable Prometheus metrics to monitor performance:

```toml
[prometheus]
enable = true
bind = "0.0.0.0:3010"
```

Access metrics at `http://<node-ip>:3010/metrics`

### NATS Monitoring

Check NATS server status:

```bash
# If using embedded NATS
curl http://localhost:8222/varz
curl http://localhost:8222/jsz

# List streams
curl http://localhost:8222/jsz?streams=1

# Check specific stream
curl http://localhost:8222/jsz?stream=harmonylite-changes-1
```

### SQLite Analysis

Examine the SQLite database directly:

```bash
sqlite3 /path/to/your.db
```

Useful SQLite commands:
```sql
-- Check if triggers are installed
SELECT name FROM sqlite_master WHERE type='trigger' AND name LIKE '__harmonylite%';

-- Check change log tables
SELECT name FROM sqlite_master WHERE type='table' AND name LIKE '__harmonylite%';

-- Count pending changes
SELECT COUNT(*) FROM __harmonylite___global_change_log;
```

## Common Issues and Solutions

### Installation and Setup

#### Problem: HarmonyLite Fails to Start

**Symptoms**:
- Service fails to start
- "command not found" errors
- Permission denied errors

**Potential Causes and Solutions**:

1. **Binary not executable**:
   ```bash
   chmod +x /path/to/harmonylite
   ```

2. **Missing dependencies**:
   ```bash
   ldd /path/to/harmonylite
   # Install any missing dependencies
   ```

3. **Permission issues**:
   ```bash
   # Check file ownership
   ls -la /path/to/harmonylite
   
   # Check directory permissions
   ls -la /var/lib/harmonylite
   
   # Fix permissions
   chown harmonylite:harmonylite /var/lib/harmonylite
   chmod 750 /var/lib/harmonylite
   ```

4. **Config file problems**:
   ```bash
   # Check config syntax
   /path/to/harmonylite -config /path/to/config.toml -check
   ```

#### Problem: Configuration Validation Errors

**Symptoms**:
- "Invalid configuration" errors
- Service starts but exits immediately

**Solutions**:

1. Verify TOML syntax is valid
2. Check that all required fields are present
3. Ensure paths exist and are accessible
4. Validate that `node_id` is unique within the cluster

### Replication Issues

#### Problem: Changes Not Replicating

**Symptoms**:
- Changes made on one node are not appearing on other nodes
- Replication metrics show no activity

**Potential Causes and Solutions**:

1. **NATS connectivity issues**:
   ```bash
   # Check NATS status
   curl http://localhost:8222/varz
   
   # Test connection from other nodes
   telnet <nats-server-ip> 4222
   ```

2. **Triggers not installed**:
   ```sql
   -- Check triggers
   SELECT name FROM sqlite_master WHERE type='trigger' AND name LIKE '__harmonylite%';
   
   -- Reinstall triggers
   -- Exit SQLite and run:
   harmonylite -config /path/to/config.toml -cleanup
   -- Then restart HarmonyLite
   ```

3. **Change logs not being created**:
   ```sql
   -- Make a test change with trusted_schema enabled
   PRAGMA trusted_schema = ON;
   INSERT INTO test_table (name) VALUES ('test');
   
   -- Check if it appears in change log
   SELECT * FROM __harmonylite__test_table_change_log ORDER BY id DESC LIMIT 1;
   ```

4. **Publishing disabled**:
   ```
   # Check config.toml for:
   publish = false  # Should be true for nodes that need to send changes
   ```

5. **NATS stream not created**:
   ```bash
   # Check if streams exist
   curl http://localhost:8222/jsz?streams=1
   
   # Recreate streams
   # First stop HarmonyLite, then restart with clean state
   rm /path/to/seq-map.cbor
   # Restart HarmonyLite
   ```

#### Problem: High Replication Latency

**Symptoms**:
- Changes take a long time to propagate
- High `pending_publish` metrics

**Solutions**:

1. **Increase shards**:
   ```toml
   [replication_log]
   shards = 4  # Increase from default
   ```

2. **Enable compression**:
   ```toml
   [replication_log]
   compress = true
   ```

3. **Check network latency**:
   ```bash
   ping <other-node-ip>
   ```

4. **Monitor disk I/O**:
   ```bash
   iostat -x 1
   ```

5. **Adjust cleanup interval**:
   ```toml
   # Decrease to cleanup more frequently
   cleanup_interval = 30000  # 30 seconds
   ```

### Database Issues

#### Problem: Database Locks

**Symptoms**:
- "database is locked" errors
- Operations timing out
- Replication stalls

**Solutions**:

1. **Check for long-running transactions**:
   ```sql
   PRAGMA busy_timeout = 30000;  -- Set in your application
   ```

2. **Use WAL journal mode**:
   ```sql
   PRAGMA journal_mode = WAL;  -- Set in your application
   ```

3. **Check for other processes accessing the database**:
   ```bash
   lsof | grep your.db
   ```

4. **Verify SQLite version**:
   ```bash
   sqlite3 --version
   # Should be 3.35.0 or newer
   ```

5. **Consider timeout settings**:
   ```bash
   # Add to application connection string
   ?_timeout=30000&_journal_mode=WAL
   ```

#### Problem: Database Corruption

**Symptoms**:
- "malformed database" errors
- Unexpected query results
- Application crashes

**Solutions**:

1. **Check database integrity**:
   ```sql
   PRAGMA integrity_check;
   ```

2. **Restore from snapshot**:
   ```bash
   # Stop HarmonyLite
   harmonylite -config /path/to/config.toml -stop
   
   # Remove corrupt database
   rm /path/to/your.db
   
   # Restart to trigger recovery
   harmonylite -config /path/to/config.toml
   ```

3. **Recover from backup**:
   ```bash
   # Restore from backup
   cp /path/to/backup.db /path/to/your.db
   
   # Remove sequence map to force reinitialization
   rm /path/to/seq-map.cbor
   
   # Restart HarmonyLite
   harmonylite -config /path/to/config.toml
   ```

### Snapshot and Recovery

#### Problem: Snapshot Creation Fails

**Symptoms**:
- "Failed to create snapshot" errors
- No snapshots appearing in storage
- `snapshot_age` metric keeps increasing

**Solutions**:

1. **Check storage connectivity**:
   ```bash
   # Test S3 access
   aws s3 ls s3://your-bucket/
   
   # Test WebDAV
   curl -u username:password https://webdav.example.com/
   ```

2. **Verify permissions**:
   ```bash
   # For local file storage
   ls -la /path/to/snapshot/dir
   
   # For S3
   aws s3 ls s3://your-bucket/ --debug
   ```

3. **Ensure enough disk space**:
   ```bash
   df -h
   ```

4. **Force snapshot creation**:
   ```bash
   harmonylite -config /path/to/config.toml -save-snapshot
   ```

5. **Check storage configuration**:
   ```toml
   [snapshot]
   enabled = true
   store = "s3"  # Verify this matches your credentials
   
   [snapshot.s3]
   # Verify all credentials are correct
   ```

#### Problem: Recovery from Snapshot Fails

**Symptoms**:
- "Failed to restore snapshot" errors
- Service fails to start after deleting database
- Inconsistent state after recovery

**Solutions**:

1. **Check sequence map**:
   ```bash
   # Remove sequence map to force full recovery
   rm /path/to/seq-map.cbor
   ```

2. **Verify snapshot access**:
   ```bash
   # For S3
   aws s3 ls s3://your-bucket/harmonylite/snapshots/
   ```

3. **Try manual restore**:
   ```bash
   # Download snapshot manually
   aws s3 cp s3://your-bucket/harmonylite/snapshots/latest.db /tmp/
   
   # Replace database
   cp /tmp/latest.db /path/to/your.db
   
   # Fix permissions
   chown harmonylite:harmonylite /path/to/your.db
   
   # Remove sequence map
   rm /path/to/seq-map.cbor
   
   # Restart
   harmonylite -config /path/to/config.toml
   ```

4. **Check logs for specific errors**:
   ```bash
   grep "snapshot" /var/log/harmonylite/harmonylite.log
   ```

### Performance Issues

#### Problem: High CPU Usage

**Symptoms**:
- CPU consistently above 70%
- Slow response times
- Process using excessive resources

**Solutions**:

1. **Profile the process**:
   ```bash
   top -p $(pgrep harmonylite)
   ```

2. **Check if compression is causing overhead**:
   ```toml
   # Try disabling compression temporarily
   [replication_log]
   compress = false
   ```

3. **Adjust shard count**:
   ```toml
   # If too high, reduce:
   [replication_log]
   shards = 2  # Start low and increase as needed
   ```

4. **Monitor change volume**:
   ```bash
   # Check Prometheus metrics
   curl http://localhost:3010/metrics | grep harmonylite_published
   ```

5. **Consider hardware upgrade** if consistently high

#### Problem: Memory Leaks

**Symptoms**:
- Steadily increasing memory usage
- Eventually crashes with out-of-memory errors
- Degraded performance over time

**Solutions**:

1. **Monitor memory usage**:
   ```bash
   ps -o pid,user,%mem,rss,command -p $(pgrep harmonylite)
   ```

2. **Set memory limits in systemd**:
   ```ini
   # In /etc/systemd/system/harmonylite.service
   [Service]
   MemoryLimit=512M
   ```

3. **Restart periodically** if needed:
   ```bash
   # In crontab
   0 4 * * * systemctl restart harmonylite
   ```

4. **Update to latest version** as memory leaks are often fixed in updates

### NATS Issues

#### Problem: NATS Connection Failures

**Symptoms**:
- "Failed to connect to NATS" errors
- Intermittent disconnections
- Stream creation failures

**Solutions**:

1. **Check NATS server status**:
   ```bash
   curl http://localhost:8222/varz
   ```

2. **Verify NATS URLs**:
   ```toml
   [nats]
   urls = ["nats://server1:4222", "nats://server2:4222"]
   # Verify all servers are running
   ```

3. **Increase connection retry settings**:
   ```toml
   [nats]
   connect_retries = 10
   reconnect_wait_seconds = 5
   ```

4. **Check authentication**:
   ```toml
   [nats]
   # Verify credentials match server configuration
   user_name = "harmonylite"
   user_password = "your-password"
   ```

5. **Test NATS connectivity directly**:
   ```bash
   # Install NATS CLI
   curl -sf https://install.nats.io/install.sh | sh
   
   # Test connection
   nats pub test.subject "hello" --server nats://server:4222
   ```

#### Problem: JetStream Errors

**Symptoms**:
- "Failed to create stream" errors
- "No responders available" errors
- Stream memory or storage errors

**Solutions**:

1. **Check JetStream status**:
   ```bash
   curl http://localhost:8222/jsz
   ```

2. **Verify JetStream is enabled** on NATS server

3. **Check storage limits**:
   ```bash
   # On NATS server
   df -h /path/to/jetstream/storage
   ```

4. **Adjust stream settings**:
   ```toml
   [replication_log]
   max_entries = 1024  # Reduce if storage is limited
   ```

5. **Recreate streams** if corrupted:
   ```bash
   # Using NATS CLI
   nats stream ls --server nats://server:4222
   nats stream rm harmonylite-changes-1 --server nats://server:4222
   # Then restart HarmonyLite
   ```

## Fixing Triggers and Schema Issues

### Problem: Missing or Corrupted Triggers

**Symptoms**:
- Changes not being captured
- Missing change log tables
- Schema change errors

**Solutions**:

1. **Check if triggers exist**:
   ```sql
   SELECT name FROM sqlite_master WHERE type='trigger' AND name LIKE '__harmonylite%';
   ```

2. **Clean up and reinstall triggers**:
   ```bash
   harmonylite -config /path/to/config.toml -cleanup
   ```

3. **Verify SQLite version compatibility**:
   ```bash
   sqlite3 --version
   # Should be 3.35.0 or newer
   ```

4. **Enable trusted schema in applications**:
   ```sql
   PRAGMA trusted_schema = ON;
   ```

### Problem: Schema Changes Break Replication

**Symptoms**:
- Errors after changing table structures
- "no such column" errors
- Replication stops after ALTER TABLE operations

**Solutions**:

1. **Proper schema change procedure**:
   - Stop applications
   - Apply changes on one node
   - Run cleanup to reset triggers:
     ```bash
     harmonylite -config /path/to/config.toml -cleanup
     ```
   - Restart HarmonyLite
   - Wait for replication
   - Repeat on other nodes

2. **Verify table structure is identical** on all nodes:
   ```sql
   .schema table_name
   ```

3. **Check for foreign key issues**:
   ```sql
   PRAGMA foreign_key_check;
   ```

## Recovery Procedures

### Full Node Recovery

If a node is completely corrupted or needs to be rebuilt:

1. **Stop HarmonyLite**:
   ```bash
   systemctl stop harmonylite
   ```

2. **Clean up existing files**:
   ```bash
   rm /var/lib/harmonylite/data.db
   rm /var/lib/harmonylite/seq-map.cbor
   ```

3. **Start HarmonyLite** (it will recover automatically):
   ```bash
   systemctl start harmonylite
   ```

4. **Monitor logs** for recovery progress:
   ```bash
   tail -f /var/log/harmonylite/harmonylite.log
   ```

### Manual Database Repair

For advanced recovery when automatic procedures fail:

1. **Create a backup first**:
   ```bash
   cp /var/lib/harmonylite/data.db /var/lib/harmonylite/data.db.bak
   ```

2. **Try SQLite recovery**:
   ```bash
   sqlite3 /var/lib/harmonylite/data.db "PRAGMA integrity_check;"
   ```

3. **Dump and restore** if integrity check fails:
   ```bash
   # Dump schema
   echo .schema | sqlite3 /var/lib/harmonylite/data.db.bak > schema.sql
   
   # Dump data (excluding HarmonyLite tables)
   sqlite3 /var/lib/harmonylite/data.db.bak <<EOF
   .mode insert
   .output data.sql
   SELECT * FROM sqlite_master WHERE type='table' AND name NOT LIKE '__harmonylite%';
   .quit
   EOF
   
   # Create new database
   sqlite3 /var/lib/harmonylite/data.db < schema.sql
   sqlite3 /var/lib/harmonylite/data.db < data.sql
   
   # Reset sequence map
   rm /var/lib/harmonylite/seq-map.cbor
   
   # Restart HarmonyLite
   systemctl restart harmonylite
   ```

## Diagnostic Commands Reference

| Issue | Diagnostic Command | What to Look For |
|-------|-------------------|------------------|
| Node Status | `systemctl status harmonylite` | Active (running) status |
| Logs | `journalctl -u harmonylite -n 100` | Recent error messages |
| Process Resources | `ps -o pid,%cpu,%mem,vsz,rss -p $(pgrep harmonylite)` | CPU/memory usage |
| NATS Status | `curl http://localhost:8222/varz` | Server running, connections |
| NATS Streams | `curl http://localhost:8222/jsz?streams=1` | Stream existence, message counts |
| Database Size | `du -sh /var/lib/harmonylite/data.db` | Growth trends |
| Database Integrity | `echo "PRAGMA integrity_check;" | sqlite3 /var/lib/harmonylite/data.db` | "ok" result |
| Triggers | `echo "SELECT count(*) FROM sqlite_master WHERE type='trigger' AND name LIKE '__harmonylite%';" | sqlite3 /var/lib/harmonylite/data.db` | Non-zero count |
| Change Log Tables | `echo "SELECT count(*) FROM sqlite_master WHERE type='table' AND name LIKE '__harmonylite%';" | sqlite3 /var/lib/harmonylite/data.db` | Non-zero count |
| Pending Changes | `echo "SELECT count(*) FROM __harmonylite___global_change_log;" | sqlite3 /var/lib/harmonylite/data.db` | Should be low or zero |
| Network Connectivity | `ss -tpln | grep harmonylite` | Listening ports |

## Getting More Help

If you're still having issues after following this guide:

1. **Check GitHub Issues** for similar problems and solutions

2. **Gather diagnostic information**:
   ```bash
   # Create diagnostic bundle
   mkdir -p /tmp/harmonylite-diag
   cp /etc/harmonylite/config.toml /tmp/harmonylite-diag/
   cp /var/log/harmonylite/harmonylite.log /tmp/harmonylite-diag/
   curl http://localhost:3010/metrics > /tmp/harmonylite-diag/metrics.txt
   curl http://localhost:8222/varz > /tmp/harmonylite-diag/nats-varz.json
   curl http://localhost:8222/jsz > /tmp/harmonylite-diag/nats-jsz.json
   harmonylite -version > /tmp/harmonylite-diag/version.txt
   tar -czf harmonylite-diag.tar.gz -C /tmp harmonylite-diag
   ```

3. **Open a GitHub Issue** with the diagnostic bundle and detailed description of your problem

4. **Join Community Discussion** for assistance from other users and developers

================
File: docs/src/components/HomepageFeatures/index.tsx
================
import type {ReactNode} from 'react';
import clsx from 'clsx';
import Heading from '@theme/Heading';
import styles from './styles.module.css';

type FeatureItem = {
  title: string;
  Svg: React.ComponentType<React.ComponentProps<'svg'>>;
  description: ReactNode;
};

const FeatureList: FeatureItem[] = [
  {
    title: 'Easy to Use',
    Svg: require('@site/static/img/undraw_docusaurus_mountain.svg').default,
    description: (
      <>
        Docusaurus was designed from the ground up to be easily installed and
        used to get your website up and running quickly.
      </>
    ),
  },
  {
    title: 'Focus on What Matters',
    Svg: require('@site/static/img/undraw_docusaurus_tree.svg').default,
    description: (
      <>
        Docusaurus lets you focus on your docs, and we&apos;ll do the chores. Go
        ahead and move your docs into the <code>docs</code> directory.
      </>
    ),
  },
  {
    title: 'Powered by React',
    Svg: require('@site/static/img/undraw_docusaurus_react.svg').default,
    description: (
      <>
        Extend or customize your website layout by reusing React. Docusaurus can
        be extended while reusing the same header and footer.
      </>
    ),
  },
];

function Feature({title, Svg, description}: FeatureItem) {
  return (
    <div className={clsx('col col--4')}>
      <div className="text--center">
        <Svg className={styles.featureSvg} role="img" />
      </div>
      <div className="text--center padding-horiz--md">
        <Heading as="h3">{title}</Heading>
        <p>{description}</p>
      </div>
    </div>
  );
}

export default function HomepageFeatures(): ReactNode {
  return (
    <section className={styles.features}>
      <div className="container">
        <div className="row">
          {FeatureList.map((props, idx) => (
            <Feature key={idx} {...props} />
          ))}
        </div>
      </div>
    </section>
  );
}

================
File: docs/src/components/HomepageFeatures/styles.module.css
================
.features {
  display: flex;
  align-items: center;
  padding: 2rem 0;
  width: 100%;
}

.featureSvg {
  height: 200px;
  width: 200px;
}

================
File: docs/src/css/custom.css
================
/**
 * Any CSS included here will be global. The classic template
 * bundles Infima by default. Infima is a CSS framework designed to
 * work well for content-centric websites.
 */

/* You can override the default Infima variables here. */
:root {
  --ifm-color-primary: #2e8555;
  --ifm-color-primary-dark: #29784c;
  --ifm-color-primary-darker: #277148;
  --ifm-color-primary-darkest: #205d3b;
  --ifm-color-primary-light: #33925d;
  --ifm-color-primary-lighter: #359962;
  --ifm-color-primary-lightest: #3cad6e;
  --ifm-code-font-size: 95%;
  --docusaurus-highlighted-code-line-bg: rgba(0, 0, 0, 0.1);
}

/* For readability concerns, you should choose a lighter palette in dark mode. */
[data-theme='dark'] {
  --ifm-color-primary: #25c2a0;
  --ifm-color-primary-dark: #21af90;
  --ifm-color-primary-darker: #1fa588;
  --ifm-color-primary-darkest: #1a8870;
  --ifm-color-primary-light: #29d5b0;
  --ifm-color-primary-lighter: #32d8b4;
  --ifm-color-primary-lightest: #4fddbf;
  --docusaurus-highlighted-code-line-bg: rgba(0, 0, 0, 0.3);
}

================
File: docs/src/pages/index.module.css
================
/**
 * CSS files with the .module.css suffix will be treated as CSS modules
 * and scoped locally.
 */

.heroBanner {
  padding: 4rem 0;
  text-align: center;
  position: relative;
  overflow: hidden;
}

@media screen and (max-width: 996px) {
  .heroBanner {
    padding: 2rem;
  }
}

.buttons {
  display: flex;
  align-items: center;
  justify-content: center;
}

================
File: docs/src/pages/index.tsx
================
// src/pages/index.tsx
import React from 'react';
import Layout from '@theme/Layout';
import Link from '@docusaurus/Link';
import useDocusaurusContext from '@docusaurus/useDocusaurusContext';
import styles from './index.module.css';
import HomepageFeatures from '@site/src/components/HomepageFeatures';

export default function Home() {
  const {siteConfig} = useDocusaurusContext();
  return (
    <Layout
      title={`${siteConfig.title}`}
      description="A distributed SQLite replicator with leaderless architecture">
      <header className={styles.heroBanner}>
        <div className="container">
          <h1 className="hero__title">{siteConfig.title}</h1>
          <p className="hero__subtitle">{siteConfig.tagline}</p>
          <div className={styles.buttons}>
            <Link
              className="button button--primary button--lg"
              to="https://github.com/wongfei2009/harmonylite/releases/latest">
              Download Latest
            </Link>
            <Link
              className="button button--secondary button--lg"
              to="/docs/introduction">
              Read the Docs
            </Link>
            <Link
              className="button button--secondary button--lg"
              to="/docs/demo">
              See it in Action
            </Link>
          </div>
        </div>
      </header>
    </Layout>
  );
}

================
File: docs/static/img/logo.svg
================
<?xml version="1.0" encoding="iso-8859-1"?>
<!-- Uploaded to: SVG Repo, www.svgrepo.com, Generator: SVG Repo Mixer Tools -->
<svg height="800px" width="800px" version="1.1" id="Capa_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" 
	 viewBox="0 0 60 60" xml:space="preserve">
<g>
	<path style="fill:#424A60;" d="M24,35v-0.375V34.25v-8.625V25.25h0.034C24.013,25.374,24,25.499,24,25.625
		c0-2.437,3.862-4.552,9.534-5.625H3.608C1.616,20,0,21.615,0,23.608v11.783C0,37.385,1.616,39,3.608,39H24V35z"/>
</g>
<g>
	<path style="fill:#556080;" d="M24.034,53H24v-9v-0.375V43.25V39H3.608C1.616,39,0,40.615,0,42.608v11.783
		C0,56.385,1.616,58,3.608,58h28.718C27.601,56.931,24.378,55.103,24.034,53z"/>
</g>
<path style="fill:#556080;" d="M54.392,20H3.608C1.616,20,0,18.384,0,16.392V4.608C0,2.616,1.616,1,3.608,1h50.783
	C56.384,1,58,2.616,58,4.608v11.783C58,18.384,56.384,20,54.392,20z"/>
<circle style="fill:#7383BF;" cx="9.5" cy="10.5" r="3.5"/>
<circle style="fill:#7383BF;" cx="49" cy="9" r="1"/>
<circle style="fill:#7383BF;" cx="45" cy="9" r="1"/>
<circle style="fill:#7383BF;" cx="51" cy="12" r="1"/>
<circle style="fill:#7383BF;" cx="47" cy="12" r="1"/>
<circle style="fill:#7383BF;" cx="41" cy="9" r="1"/>
<circle style="fill:#7383BF;" cx="43" cy="12" r="1"/>
<circle style="fill:#7383BF;" cx="37" cy="9" r="1"/>
<circle style="fill:#7383BF;" cx="39" cy="12" r="1"/>
<circle style="fill:#7383BF;" cx="33" cy="9" r="1"/>
<circle style="fill:#7383BF;" cx="35" cy="12" r="1"/>
<circle style="fill:#7383BF;" cx="9.5" cy="29.5" r="3.5"/>
<circle style="fill:#7383BF;" cx="9.5" cy="48.5" r="3.5"/>
<g>
	<path style="fill:#1A9172;" d="M42,48.75c-9.941,0-18-2.854-18-6.375V53h0.034c0.548,3.346,8.381,6,17.966,6s17.418-2.654,17.966-6
		H60V42.375C60,45.896,51.941,48.75,42,48.75z"/>
	<path style="fill:#1A9172;" d="M24,42v0.375c0-0.126,0.013-0.251,0.034-0.375H24z"/>
	<path style="fill:#1A9172;" d="M59.966,42C59.987,42.124,60,42.249,60,42.375V42H59.966z"/>
</g>
<g>
	<path style="fill:#25AE88;" d="M42,38c-9.941,0-18-2.854-18-6.375V42.75h0.034c0.548,3.346,8.381,6,17.966,6s17.418-2.654,17.966-6
		H60V31.625C60,35.146,51.941,38,42,38z"/>
	<path style="fill:#25AE88;" d="M24,31.25v0.375c0-0.126,0.013-0.251,0.034-0.375H24z"/>
	<path style="fill:#25AE88;" d="M59.966,31.25C59.987,31.374,60,31.499,60,31.625V31.25H59.966z"/>
</g>
<ellipse style="fill:#88C057;" cx="42" cy="21.375" rx="18" ry="6.375"/>
<g>
	<path style="fill:#61B872;" d="M42,27.75c-9.941,0-18-2.854-18-6.375V32h0.034c0.548,3.346,8.381,6,17.966,6s17.418-2.654,17.966-6
		H60V21.375C60,24.896,51.941,27.75,42,27.75z"/>
	<path style="fill:#61B872;" d="M24,21v0.375c0-0.126,0.013-0.251,0.034-0.375H24z"/>
	<path style="fill:#61B872;" d="M59.966,21C59.987,21.124,60,21.249,60,21.375V21H59.966z"/>
</g>
</svg>

================
File: docs/.gitignore
================
# Dependencies
/node_modules

# Production
/build

# Generated files
.docusaurus
.cache-loader

# Misc
.DS_Store
.env.local
.env.development.local
.env.test.local
.env.production.local

npm-debug.log*
yarn-debug.log*
yarn-error.log*

================
File: docs/docusaurus.config.ts
================
import { themes } from 'prism-react-renderer';
import type { Config } from '@docusaurus/types';

const config: Config = {
  title: 'HarmonyLite',
  tagline: 'A distributed SQLite replicator',
  url: 'https://wongfei2009.github.io',
  baseUrl: '/harmonylite/',
  onBrokenLinks: 'throw',
  onBrokenMarkdownLinks: 'warn',
  favicon: 'img/favicon.ico',

  organizationName: 'wongfei2009',
  projectName: 'harmonylite',
  deploymentBranch: 'gh-pages',

  markdown: {
    mermaid: true,
  },
  
  presets: [
    [
      'classic',
      {
        docs: {
          sidebarPath: './sidebars.ts',
          editUrl: 'https://github.com/wongfei2009/harmonylite/tree/master/harmonylite-docusaurus/docs/',
        },
        blog: false, // Disable blog if not needed
        theme: {
          customCss: './src/css/custom.css',
        },
      },
    ],
  ],

  themes: [
    '@docusaurus/theme-mermaid'
  ],

  themeConfig: {
    colorMode: {
      defaultMode: 'light', // Set the default and only mode to 'light'
      disableSwitch: true,  // Disable the theme toggle switch
      respectPrefersColorScheme: false, // Ignore user's system preferences
    },
    navbar: {
      title: 'HarmonyLite',
      logo: {
        alt: 'HarmonyLite Logo',
        src: 'img/logo.svg',
      },
      items: [
        {
          type: 'docSidebar',
          sidebarId: 'docs',
          position: 'left',
          label: 'Documentation',
        },
        {
          href: 'https://github.com/wongfei2009/harmonylite',
          label: 'GitHub',
          position: 'right',
        },
      ],
    },
    footer: {
      style: 'dark', // This can remain 'dark' for contrast, or change to 'light'
      copyright: `Copyright © ${new Date().getFullYear()} HarmonyLite.`,
    },
    prism: {
      theme: themes.github, // Use only the light theme
      // Remove darkTheme to ensure no dark mode fallback
    },
    mermaid: {
      theme: { light: 'neutral' }, // Specify only the light theme
    },
  },
};

export default config;

================
File: docs/package.json
================
{
  "name": "harmonylite-docusaurus",
  "version": "0.0.0",
  "private": true,
  "scripts": {
    "docusaurus": "docusaurus",
    "start": "docusaurus start",
    "build": "docusaurus build",
    "swizzle": "docusaurus swizzle",
    "deploy": "docusaurus deploy",
    "clear": "docusaurus clear",
    "serve": "docusaurus serve",
    "write-translations": "docusaurus write-translations",
    "write-heading-ids": "docusaurus write-heading-ids",
    "typecheck": "tsc"
  },
  "dependencies": {
    "@docusaurus/core": "3.7.0",
    "@docusaurus/preset-classic": "3.7.0",
    "@docusaurus/theme-mermaid": "^3.7.0",
    "@mdx-js/react": "^3.0.0",
    "clsx": "^2.0.0",
    "prism-react-renderer": "^2.3.0",
    "react": "^19.0.0",
    "react-dom": "^19.0.0"
  },
  "devDependencies": {
    "@docusaurus/module-type-aliases": "3.7.0",
    "@docusaurus/tsconfig": "3.7.0",
    "@docusaurus/types": "3.7.0",
    "typescript": "~5.6.2"
  },
  "browserslist": {
    "production": [
      ">0.5%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 3 chrome version",
      "last 3 firefox version",
      "last 5 safari version"
    ]
  },
  "engines": {
    "node": ">=18.0"
  },
  "packageManager": "yarn@1.22.22+sha512.a6b2f7906b721bba3d67d4aff083df04dad64c399707841b7acf00f6b133b7ac24255f2652fa22ae3534329dc6180534e98d17432037ff6fd140556e2bb3137e"
}

================
File: docs/README.md
================
# Website

This website is built using [Docusaurus](https://docusaurus.io/), a modern static website generator.

### Installation

```
$ yarn
```

### Local Development

```
$ yarn start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.

### Build

```
$ yarn build
```

This command generates static content into the `build` directory and can be served using any static contents hosting service.

### Deployment

Using SSH:

```
$ USE_SSH=true yarn deploy
```

Not using SSH:

```
$ GIT_USER=<Your GitHub username> yarn deploy
```

If you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.

================
File: docs/sidebars.ts
================
import type { SidebarsConfig } from '@docusaurus/plugin-content-docs';

const sidebars: SidebarsConfig = {
  docs: [
    {
      type: 'category',
      label: 'Introduction',
      items: ['introduction'],
    },
    {
      type: 'category',
      label: 'Getting Started',
      items: ['quick-start', 'demo'],
    },
    {
      type: 'category',
      label: 'Concepts',
      items: ['architecture', 'replication', 'snapshots'],
    },
    {
      type: 'category',
      label: 'Deployment',
      items: ['configuration-reference', 'nats-configuration', 'production-deployment'],
    },
    {
      type: 'category',
      label: 'Operations',
      items: ['troubleshooting', 'faq'],
    },
  ],
};

export default sidebars;

================
File: docs/tsconfig.json
================
{
  // This file is not used in compilation. It is here just for a nice editor experience.
  "extends": "@docusaurus/tsconfig",
  "compilerOptions": {
    "baseUrl": "."
  },
  "exclude": [".docusaurus", "build"]
}

================
File: examples/nats-server-1.conf
================
monitor_port = 8222
jetstream = {
  store_dir = "/tmp/nats-1/jetstream/"
  max_memory_store = 52428800
  max_file_store = 5368709120
}

================
File: examples/nats-server-2.conf
================
monitor_port = 8223
jetstream = {
  store_dir = "/tmp/nats-2/jetstream/"
  max_memory_store = 52428800
  max_file_store = 5368709120
}

================
File: examples/nats-server-3.conf
================
monitor_port = 8224
jetstream = {
  store_dir = "/tmp/nats-3/jetstream/"
  max_memory_store = 52428800
  max_file_store = 5368709120
}

================
File: examples/node-1-config.toml
================
seq_map_path="/tmp/harmonylite-1-sm.cbor"
db_path="/tmp/harmonylite-1.db"
node_id=1

[replication_log]
shards=2
max_entries=2048
replicas=1
compress=true

[nats]
user_name = "harmonylite"
user_password = "secure-password-here"
reconnect_wait_seconds=5
server_config = "examples/nats-server-1.conf"

[logging]
verbose = false
format = "console"

================
File: examples/node-2-config.toml
================
seq_map_path="/tmp/harmonylite-2-sm.cbor"
db_path="/tmp/harmonylite-2.db"
node_id=2

[replication_log]
shards=2
max_entries=2048
replicas=1
compress=true

[nats]
user_name = "harmonylite"
user_password = "secure-password-here"
reconnect_wait_seconds=5
server_config = "examples/nats-server-2.conf"

[logging]
verbose = false
format = "console"

================
File: examples/node-3-config.toml
================
seq_map_path="/tmp/harmonylite-3-sm.cbor"
db_path="/tmp/harmonylite-3.db"
node_id=3

[replication_log]
shards=2
max_entries=2048
replicas=1
compress=true

[nats]
user_name = "harmonylite"
user_password = "secure-password-here"
reconnect_wait_seconds=5
server_config = "examples/nats-server-3.conf"

[logging]
verbose = false
format = "console"

================
File: examples/run-cluster.sh
================
#!/bin/bash
create_db() {
    local db_file="$1"
    cat <<EOF | sqlite3 "$db_file"
DROP TABLE IF EXISTS Books;
CREATE TABLE Books (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    author TEXT NOT NULL,
    publication_year INTEGER
);
INSERT INTO Books (title, author, publication_year)
VALUES
('The Hitchhiker''s Guide to the Galaxy', 'Douglas Adams', 1979),
('The Lord of the Rings', 'J.R.R. Tolkien', 1954),
('Harry Potter and the Sorcerer''s Stone', 'J.K. Rowling', 1997),
('The Catcher in the Rye', 'J.D. Salinger', 1951),
('To Kill a Mockingbird', 'Harper Lee', 1960),
('1984', 'George Orwell', 1949),
('The Great Gatsby', 'F. Scott Fitzgerald', 1925);
EOF
    echo "Created $db_file"
}

rm -rf /tmp/harmonylite-1-* /tmp/harmonylite-2-* /tmp/harmonylite-3-*
create_db /tmp/harmonylite-1.db
create_db /tmp/harmonylite-2.db
create_db /tmp/harmonylite-3.db


cleanup() {
    kill "$job1" "$job2" "$job3"
    cd "$ORIGINAL_DIR"  # Return to original directory on exit
}

# Save the current directory
ORIGINAL_DIR=$(pwd)

# Change to the directory where harmonylite binary is located
cd "$(dirname "$0")/.."
HARMONY_DIR=$(pwd)
echo "Changed to harmonylite directory: $HARMONY_DIR"

trap cleanup EXIT
rm -rf /tmp/nats*
./harmonylite -config examples/node-1-config.toml -cluster-addr localhost:4221 -cluster-peers 'nats://localhost:4222/,nats://localhost:4223/' &
job1=$!

sleep 1
./harmonylite -config examples/node-2-config.toml -cluster-addr localhost:4222 -cluster-peers 'nats://localhost:4221/,nats://localhost:4223/' &
job2=$!

sleep 1
./harmonylite -config examples/node-3-config.toml -cluster-addr localhost:4223 -cluster-peers 'nats://localhost:4221/,nats://localhost:4222/' &
job3=$!

wait $job1 $job2 $job3

# Return to original directory (this will also happen via cleanup if exiting early)
cd "$ORIGINAL_DIR"

================
File: logstream/replication_event_test.go
================
package logstream

import (
	"fmt"
	"reflect"
	"testing"
	"time"

	"github.com/fxamacker/cbor/v2"
	"github.com/wongfei2009/harmonylite/core"
)

// MockEvent implements the core.ReplicableEvent interface for testing
type MockEvent struct {
	ID        int64
	Name      string
	Timestamp time.Time
	Data      map[string]interface{}
	Wrapped   bool // Flag to track if the event has been wrapped/unwrapped
}

// Wrap satisfies the core.ReplicableEvent interface
func (m MockEvent) Wrap() (MockEvent, error) {
	// Create a copy with the wrapped flag set to true
	return MockEvent{
		ID:        m.ID,
		Name:      m.Name,
		Timestamp: m.Timestamp,
		Data:      m.Data,
		Wrapped:   true,
	}, nil
}

// Unwrap satisfies the core.ReplicableEvent interface
func (m MockEvent) Unwrap() (MockEvent, error) {
	// Create a copy with the wrapped flag set to false
	return MockEvent{
		ID:        m.ID,
		Name:      m.Name,
		Timestamp: m.Timestamp,
		Data:      m.Data,
		Wrapped:   false,
	}, nil
}

// timestampsApproximatelyEqual checks if two timestamps are equal within a small tolerance
// This accounts for precision loss during serialization
func timestampsApproximatelyEqual(t1, t2 time.Time) bool {
	// If either time is the zero value, they should match exactly
	if t1.IsZero() || t2.IsZero() {
		return t1.Equal(t2)
	}

	// Compare only to second precision
	return t1.Truncate(time.Second).Equal(t2.Truncate(time.Second))
}

// Register the MockEvent type with CBOR before running tests
func init() {
	// Register the MockEvent type with CBOR
	err := core.CBORTags.Add(
		cbor.TagOptions{
			DecTag: cbor.DecTagOptional,
			EncTag: cbor.EncTagRequired,
		},
		reflect.TypeOf(MockEvent{}),
		1000, // Use a test tag number
	)

	if err != nil {
		panic(err)
	}
}

func TestReplicationEvent_Marshal(t *testing.T) {
	// Create a fixed timestamp without microseconds for consistent testing
	fixedTime := time.Date(2025, 3, 4, 16, 22, 22, 0, time.Local) // No microseconds

	// Test cases
	tests := []struct {
		name    string
		event   ReplicationEvent[MockEvent]
		wantErr bool
	}{
		{
			name: "Basic event",
			event: ReplicationEvent[MockEvent]{
				FromNodeId: 123,
				Payload: MockEvent{
					ID:        1,
					Name:      "Test Event",
					Timestamp: time.Date(2025, 3, 1, 12, 0, 0, 0, time.UTC),
					Data: map[string]interface{}{
						"key1": "value1",
						"key2": 42,
					},
					Wrapped: false,
				},
			},
			wantErr: false,
		},
		{
			name: "Event with empty payload",
			event: ReplicationEvent[MockEvent]{
				FromNodeId: 456,
				Payload: MockEvent{
					ID:        2,
					Name:      "",
					Timestamp: time.Time{},
					Data:      map[string]interface{}{},
					Wrapped:   false,
				},
			},
			wantErr: false,
		},
		{
			name: "Event with complex data",
			event: ReplicationEvent[MockEvent]{
				FromNodeId: 789,
				Payload: MockEvent{
					ID:        3,
					Name:      "Complex Event",
					Timestamp: fixedTime, // Using fixed timestamp without microseconds
					Data: map[string]interface{}{
						"nestedMap": map[string]interface{}{
							"nested1": "value",
							"nested2": 123,
						},
						"arrayValue": []int{1, 2, 3, 4, 5},
					},
					Wrapped: false,
				},
			},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Marshal the event
			data, err := tt.event.Marshal()

			// Check error condition matches expectation
			if (err != nil) != tt.wantErr {
				t.Errorf("ReplicationEvent.Marshal() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			if tt.wantErr {
				return
			}

			// Verify data is not empty
			if len(data) == 0 {
				t.Errorf("ReplicationEvent.Marshal() returned empty data")
				return
			}

			// Unmarshal back and verify
			var unmarshaledEvent ReplicationEvent[MockEvent]
			err = unmarshaledEvent.Unmarshal(data)
			if err != nil {
				t.Errorf("Failed to unmarshal marshaled data: %v", err)
				return
			}

			// Check that node ID was preserved
			if unmarshaledEvent.FromNodeId != tt.event.FromNodeId {
				t.Errorf("Unmarshaled event has different FromNodeId: got %v, want %v",
					unmarshaledEvent.FromNodeId, tt.event.FromNodeId)
			}

			// Check basic payload fields
			if unmarshaledEvent.Payload.ID != tt.event.Payload.ID {
				t.Errorf("Unmarshaled payload has different ID: got %v, want %v",
					unmarshaledEvent.Payload.ID, tt.event.Payload.ID)
			}

			if unmarshaledEvent.Payload.Name != tt.event.Payload.Name {
				t.Errorf("Unmarshaled payload has different Name: got %v, want %v",
					unmarshaledEvent.Payload.Name, tt.event.Payload.Name)
			}

			// Use the more robust timestamp comparison with detailed diagnostics
			if !timestampsApproximatelyEqual(unmarshaledEvent.Payload.Timestamp, tt.event.Payload.Timestamp) {
				t.Errorf("Unmarshaled payload has different Timestamp: got %v (Unix: %d.%09d), want %v (Unix: %d.%09d)",
					unmarshaledEvent.Payload.Timestamp,
					unmarshaledEvent.Payload.Timestamp.Unix(),
					unmarshaledEvent.Payload.Timestamp.Nanosecond(),
					tt.event.Payload.Timestamp,
					tt.event.Payload.Timestamp.Unix(),
					tt.event.Payload.Timestamp.Nanosecond())
			}

			// Verify the wrap/unwrap process was applied
			if unmarshaledEvent.Payload.Wrapped != false {
				t.Errorf("Payload should be unwrapped after unmarshaling, but Wrapped = %v",
					unmarshaledEvent.Payload.Wrapped)
			}
		})
	}
}

func TestReplicationEvent_Unmarshal(t *testing.T) {
	// Create a sample event
	originalEvent := ReplicationEvent[MockEvent]{
		FromNodeId: 123,
		Payload: MockEvent{
			ID:        42,
			Name:      "Test Event",
			Timestamp: time.Date(2025, 3, 1, 12, 0, 0, 0, time.UTC),
			Data: map[string]interface{}{
				"key1": "value1",
				"key2": 42,
			},
			Wrapped: false,
		},
	}

	// Marshal it
	data, err := originalEvent.Marshal()
	if err != nil {
		t.Fatalf("Failed to marshal event for unmarshal test: %v", err)
	}

	// Test cases for unmarshaling
	tests := []struct {
		name        string
		data        []byte
		wantErr     bool
		corruptData func([]byte) []byte // Function to corrupt data for negative tests
	}{
		{
			name:    "Valid data",
			data:    data,
			wantErr: false,
		},
		{
			name:    "Empty data",
			data:    []byte{},
			wantErr: true,
		},
		{
			name:    "Corrupted data",
			data:    data,
			wantErr: true,
			corruptData: func(data []byte) []byte {
				// Just corrupt some bytes in the middle
				if len(data) > 10 {
					corrupted := make([]byte, len(data))
					copy(corrupted, data)
					corrupted[len(data)/2] = 0xFF
					corrupted[len(data)/2+1] = 0xFF
					return corrupted
				}
				return data
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			testData := tt.data
			if tt.corruptData != nil {
				testData = tt.corruptData(testData)
			}

			var unmarshaledEvent ReplicationEvent[MockEvent]
			err := unmarshaledEvent.Unmarshal(testData)

			// Check error condition matches expectation
			if (err != nil) != tt.wantErr {
				t.Errorf("ReplicationEvent.Unmarshal() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			if tt.wantErr {
				return
			}

			// Check that unmarshaled event matches original event
			if unmarshaledEvent.FromNodeId != originalEvent.FromNodeId {
				t.Errorf("Unmarshaled event has different FromNodeId: got %v, want %v",
					unmarshaledEvent.FromNodeId, originalEvent.FromNodeId)
			}

			if unmarshaledEvent.Payload.ID != originalEvent.Payload.ID {
				t.Errorf("Unmarshaled payload has different ID: got %v, want %v",
					unmarshaledEvent.Payload.ID, originalEvent.Payload.ID)
			}

			if unmarshaledEvent.Payload.Name != originalEvent.Payload.Name {
				t.Errorf("Unmarshaled payload has different Name: got %v, want %v",
					unmarshaledEvent.Payload.Name, originalEvent.Payload.Name)
			}

			// Use the robust timestamp comparison
			if !timestampsApproximatelyEqual(unmarshaledEvent.Payload.Timestamp, originalEvent.Payload.Timestamp) {
				t.Errorf("Unmarshaled payload has different Timestamp: got %v (Unix: %d.%09d), want %v (Unix: %d.%09d)",
					unmarshaledEvent.Payload.Timestamp,
					unmarshaledEvent.Payload.Timestamp.Unix(),
					unmarshaledEvent.Payload.Timestamp.Nanosecond(),
					originalEvent.Payload.Timestamp,
					originalEvent.Payload.Timestamp.Unix(),
					originalEvent.Payload.Timestamp.Nanosecond())
			}
		})
	}
}

func TestReplicationEvent_RoundTrip(t *testing.T) {
	// Create fixed timestamps without microseconds for consistent testing
	fixedTime1 := time.Date(2025, 3, 1, 10, 0, 0, 0, time.UTC)
	fixedTime2 := time.Date(2025, 3, 2, 11, 30, 0, 0, time.UTC)
	fixedTime3 := time.Date(2025, 3, 3, 15, 45, 0, 0, time.UTC)

	// Test a variety of payload sizes and contents
	testCases := []struct {
		name    string
		payload MockEvent
	}{
		{
			name: "Small payload",
			payload: MockEvent{
				ID:        1,
				Name:      "Small",
				Timestamp: fixedTime1,
				Data: map[string]interface{}{
					"key": "value",
				},
			},
		},
		{
			name: "Medium payload",
			payload: MockEvent{
				ID:        2,
				Name:      "Medium",
				Timestamp: fixedTime2,
				Data: map[string]interface{}{
					"key1": "value1",
					"key2": 42,
					"key3": true,
					"key4": []string{"a", "b", "c"},
				},
			},
		},
		{
			name: "Large payload",
			payload: MockEvent{
				ID:        3,
				Name:      "Large",
				Timestamp: fixedTime3,
				Data:      generateLargeData(),
			},
		},
	}

	for _, tc := range testCases {
		t.Run(tc.name, func(t *testing.T) {
			// Create the event
			original := ReplicationEvent[MockEvent]{
				FromNodeId: 42,
				Payload:    tc.payload,
			}

			// Marshal
			data, err := original.Marshal()
			if err != nil {
				t.Fatalf("Marshal failed: %v", err)
			}

			// Unmarshal
			var unmarshaled ReplicationEvent[MockEvent]
			err = unmarshaled.Unmarshal(data)
			if err != nil {
				t.Fatalf("Unmarshal failed: %v", err)
			}

			// Check the node ID
			if unmarshaled.FromNodeId != original.FromNodeId {
				t.Errorf("Node ID mismatch: got %d, want %d",
					unmarshaled.FromNodeId, original.FromNodeId)
			}

			// Check basic payload fields
			if unmarshaled.Payload.ID != original.Payload.ID {
				t.Errorf("Payload ID mismatch: got %d, want %d",
					unmarshaled.Payload.ID, original.Payload.ID)
			}

			if unmarshaled.Payload.Name != original.Payload.Name {
				t.Errorf("Payload Name mismatch: got %s, want %s",
					unmarshaled.Payload.Name, original.Payload.Name)
			}

			// Use the more robust timestamp comparison
			if !timestampsApproximatelyEqual(unmarshaled.Payload.Timestamp, original.Payload.Timestamp) {
				t.Errorf("Payload Timestamp mismatch: got %v (Unix: %d.%09d), want %v (Unix: %d.%09d)",
					unmarshaled.Payload.Timestamp,
					unmarshaled.Payload.Timestamp.Unix(),
					unmarshaled.Payload.Timestamp.Nanosecond(),
					original.Payload.Timestamp,
					original.Payload.Timestamp.Unix(),
					original.Payload.Timestamp.Nanosecond())
			}

			// Verify the wrap/unwrap flag
			if unmarshaled.Payload.Wrapped != false {
				t.Errorf("Expected Payload.Wrapped to be false, got true")
			}

			// Check data map keys (complete equality might be tricky with maps)
			for key := range original.Payload.Data {
				if _, exists := unmarshaled.Payload.Data[key]; !exists {
					t.Errorf("Key %s missing from unmarshaled data", key)
				}
			}
		})
	}
}

// generateLargeData creates a larger, more complex data structure for testing
func generateLargeData() map[string]interface{} {
	data := make(map[string]interface{})

	// Add some basic key-values
	for i := 0; i < 50; i++ {
		data[fmt.Sprintf("key%d", i)] = fmt.Sprintf("value%d", i)
	}

	// Add nested maps
	nestedMap := make(map[string]interface{})
	for i := 0; i < 20; i++ {
		nestedMap[fmt.Sprintf("nested%d", i)] = i
	}
	data["nested"] = nestedMap

	// Add array values
	data["array"] = make([]int, 100)
	for i := 0; i < 100; i++ {
		data["array"].([]int)[i] = i
	}

	return data
}

// TestMarshalPerformance measures the performance of marshaling and unmarshaling
// This test is skipped by default but can be run with -test.run=Performance
func TestMarshalPerformance(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping performance test in short mode")
	}

	// Use a fixed timestamp without microseconds for consistency
	fixedTime := time.Date(2025, 3, 4, 16, 22, 22, 0, time.UTC)

	payload := MockEvent{
		ID:        1,
		Name:      "Performance Test",
		Timestamp: fixedTime,
		Data:      generateLargeData(),
	}

	event := ReplicationEvent[MockEvent]{
		FromNodeId: 42,
		Payload:    payload,
	}

	// Benchmark marshaling
	t.Run("Marshal", func(t *testing.T) {
		start := time.Now()
		iterations := 1000

		for i := 0; i < iterations; i++ {
			_, err := event.Marshal()
			if err != nil {
				t.Fatalf("Marshal failed: %v", err)
			}
		}

		elapsed := time.Since(start)
		t.Logf("Marshal: %d iterations in %s (%.2f µs/op)",
			iterations, elapsed, float64(elapsed.Microseconds())/float64(iterations))
	})

	// Create marshaled data for unmarshal test
	data, err := event.Marshal()
	if err != nil {
		t.Fatalf("Failed to marshal for benchmark: %v", err)
	}

	// Benchmark unmarshaling
	t.Run("Unmarshal", func(t *testing.T) {
		start := time.Now()
		iterations := 1000

		for i := 0; i < iterations; i++ {
			var unmarshaled ReplicationEvent[MockEvent]
			err := unmarshaled.Unmarshal(data)
			if err != nil {
				t.Fatalf("Unmarshal failed: %v", err)
			}
		}

		elapsed := time.Since(start)
		t.Logf("Unmarshal: %d iterations in %s (%.2f µs/op)",
			iterations, elapsed, float64(elapsed.Microseconds())/float64(iterations))
	})
}

================
File: logstream/replication_event.go
================
package logstream

import (
	"github.com/fxamacker/cbor/v2"
	"github.com/wongfei2009/harmonylite/core"
)

type ReplicationEvent[T core.ReplicableEvent[T]] struct {
	FromNodeId uint64
	Payload    T
}

func (e *ReplicationEvent[T]) Marshal() ([]byte, error) {
	wrappedPayload, err := e.Payload.Wrap()
	if err != nil {
		return nil, err
	}

	ev := ReplicationEvent[T]{
		FromNodeId: e.FromNodeId,
		Payload:    wrappedPayload,
	}

	em, err := cbor.EncOptions{}.EncModeWithTags(core.CBORTags)
	if err != nil {
		return nil, err
	}

	return em.Marshal(ev)
}

func (e *ReplicationEvent[T]) Unmarshal(data []byte) error {
	dm, err := cbor.DecOptions{}.DecModeWithTags(core.CBORTags)
	if err != nil {
		return nil
	}

	err = dm.Unmarshal(data, e)
	if err != nil {
		return err
	}

	e.Payload, err = e.Payload.Unwrap()
	if err != nil {
		return err
	}

	return nil
}

================
File: logstream/replication_state_test.go
================
package logstream

import (
	"io/ioutil"
	"os"
	"path/filepath"
	"sync"
	"testing"

	"github.com/fxamacker/cbor/v2"
	"github.com/stretchr/testify/assert"
	"github.com/wongfei2009/harmonylite/cfg"
)

func TestReplicationState_Init(t *testing.T) {
	// Create a temporary file for testing
	tempDir, err := ioutil.TempDir("", "replication-state-test")
	assert.NoError(t, err)
	defer os.RemoveAll(tempDir)

	seqMapPath := filepath.Join(tempDir, "seq-map.cbor")

	// Backup original path
	originalPath := cfg.Config.SeqMapPath
	defer func() {
		cfg.Config.SeqMapPath = originalPath
	}()

	// Set config to use the test file
	cfg.Config.SeqMapPath = seqMapPath

	t.Run("InitWithEmptyFile", func(t *testing.T) {
		// Create a new replicationState
		state := &replicationState{}

		// Initialize it with an empty file
		err := state.init()
		assert.NoError(t, err)
		defer state.fl.Close()

		// Check the initialized state
		assert.NotNil(t, state.seq)
		assert.NotNil(t, state.lock)
		assert.NotNil(t, state.fl)
		assert.Equal(t, 0, len(state.seq))
	})

	t.Run("InitWithExistingData", func(t *testing.T) {
		// Create test data
		testData := map[string]uint64{
			"stream1": 100,
			"stream2": 200,
		}

		// Write test data to the file
		file, err := os.Create(seqMapPath)
		assert.NoError(t, err)

		err = cbor.NewEncoder(file).Encode(testData)
		assert.NoError(t, err)
		file.Close()

		// Create a new replicationState
		state := &replicationState{}

		// Initialize it with the existing file
		err = state.init()
		assert.NoError(t, err)
		defer state.fl.Close()

		// Check that the data was loaded correctly
		assert.Equal(t, uint64(100), state.seq["stream1"])
		assert.Equal(t, uint64(200), state.seq["stream2"])
	})

	t.Run("InitWithCorruptedFile", func(t *testing.T) {
		// Write invalid CBOR data to the file
		err := ioutil.WriteFile(seqMapPath, []byte("not cbor data"), 0666)
		assert.NoError(t, err)

		// Create a new replicationState
		state := &replicationState{}

		// Initialize should return an error
		err = state.init()
		assert.Error(t, err)

		// Clean up if file was created despite error
		if state.fl != nil {
			state.fl.Close()
		}
	})
}

func TestReplicationState_Save(t *testing.T) {
	// Backup original path
	originalPath := cfg.Config.SeqMapPath
	defer func() {
		cfg.Config.SeqMapPath = originalPath
	}()

	t.Run("SaveWithUninitializedState", func(t *testing.T) {
		// Create an uninitialized state with a lock but no file
		state := &replicationState{
			seq:  make(map[string]uint64),
			lock: &sync.RWMutex{},
			fl:   nil, // Explicitly nil file
		}

		// Attempt to save without initializing
		_, err := state.save("stream1", 100)
		assert.Equal(t, ErrNotInitialized, err)
	})

	t.Run("SaveNewSequence", func(t *testing.T) {
		// Create a temporary file for this test
		tempDir, err := ioutil.TempDir("", "replication-state-test-save-new")
		assert.NoError(t, err)
		defer os.RemoveAll(tempDir)

		seqMapPath := filepath.Join(tempDir, "seq-map.cbor")
		cfg.Config.SeqMapPath = seqMapPath

		// Create and initialize a state
		state := &replicationState{}
		err = state.init()
		assert.NoError(t, err)
		defer state.fl.Close()

		// Save a new sequence
		seq, err := state.save("stream1", 100)
		assert.NoError(t, err)
		assert.Equal(t, uint64(100), seq)

		// Verify it was saved
		assert.Equal(t, uint64(100), state.seq["stream1"])

		// Verify it was written to the file by reading it back with a new state
		state.fl.Close()

		newState := &replicationState{}
		err = newState.init()
		assert.NoError(t, err)
		defer newState.fl.Close()

		assert.Equal(t, uint64(100), newState.seq["stream1"])
	})

	t.Run("SaveLowerSequence", func(t *testing.T) {
		// Create a temporary file for this test
		tempDir, err := ioutil.TempDir("", "replication-state-test-save-lower")
		assert.NoError(t, err)
		defer os.RemoveAll(tempDir)

		seqMapPath := filepath.Join(tempDir, "seq-map.cbor")
		cfg.Config.SeqMapPath = seqMapPath

		// Create and initialize a state
		state := &replicationState{}
		err = state.init()
		assert.NoError(t, err)
		defer state.fl.Close()

		// Save an initial sequence
		_, err = state.save("stream1", 100)
		assert.NoError(t, err)

		// Try to save a lower sequence
		seq, err := state.save("stream1", 50)
		assert.NoError(t, err)
		assert.Equal(t, uint64(100), seq) // Should return the existing higher value

		// Verify the value wasn't changed
		assert.Equal(t, uint64(100), state.seq["stream1"])
	})

	t.Run("SaveHigherSequence", func(t *testing.T) {
		// Create a temporary file for this test
		tempDir, err := ioutil.TempDir("", "replication-state-test-save-higher")
		assert.NoError(t, err)
		defer os.RemoveAll(tempDir)

		seqMapPath := filepath.Join(tempDir, "seq-map.cbor")
		cfg.Config.SeqMapPath = seqMapPath

		// Create and initialize a state
		state := &replicationState{}
		err = state.init()
		assert.NoError(t, err)
		defer state.fl.Close()

		// Save an initial sequence
		_, err = state.save("stream1", 100)
		assert.NoError(t, err)

		// Save a higher sequence
		seq, err := state.save("stream1", 150)
		assert.NoError(t, err)
		assert.Equal(t, uint64(150), seq)

		// Verify it was updated
		assert.Equal(t, uint64(150), state.seq["stream1"])
	})

	t.Run("SaveMultipleStreams", func(t *testing.T) {
		// Create a temporary file for this test
		tempDir, err := ioutil.TempDir("", "replication-state-test-save-multiple")
		assert.NoError(t, err)
		defer os.RemoveAll(tempDir)

		seqMapPath := filepath.Join(tempDir, "seq-map.cbor")
		cfg.Config.SeqMapPath = seqMapPath

		// Create and initialize a state
		state := &replicationState{}
		err = state.init()
		assert.NoError(t, err)
		defer state.fl.Close()

		// Save sequences for multiple streams
		_, err = state.save("stream1", 100)
		assert.NoError(t, err)

		_, err = state.save("stream2", 200)
		assert.NoError(t, err)

		// Verify both were saved
		assert.Equal(t, uint64(100), state.seq["stream1"])
		assert.Equal(t, uint64(200), state.seq["stream2"])

		// Verify they were written to the file
		state.fl.Close()

		newState := &replicationState{}
		err = newState.init()
		assert.NoError(t, err)
		defer newState.fl.Close()

		assert.Equal(t, uint64(100), newState.seq["stream1"])
		assert.Equal(t, uint64(200), newState.seq["stream2"])
	})
}

func TestReplicationState_Get(t *testing.T) {
	// Backup original path
	originalPath := cfg.Config.SeqMapPath
	defer func() {
		cfg.Config.SeqMapPath = originalPath
	}()

	t.Run("GetExistingSequence", func(t *testing.T) {
		// Create and initialize a state
		state := &replicationState{}
		state.seq = map[string]uint64{
			"stream1": 100,
			"stream2": 200,
		}
		state.lock = &sync.RWMutex{}

		// Get existing sequences
		assert.Equal(t, uint64(100), state.get("stream1"))
		assert.Equal(t, uint64(200), state.get("stream2"))
	})

	t.Run("GetNonExistingSequence", func(t *testing.T) {
		// Create and initialize a state
		state := &replicationState{}
		state.seq = map[string]uint64{
			"stream1": 100,
		}
		state.lock = &sync.RWMutex{}

		// Get a non-existing sequence
		assert.Equal(t, uint64(0), state.get("nonexistent"))
	})

	t.Run("GetWithRealFile", func(t *testing.T) {
		// Create a temporary file for this test
		tempDir, err := ioutil.TempDir("", "replication-state-test-get-real")
		assert.NoError(t, err)
		defer os.RemoveAll(tempDir)

		seqMapPath := filepath.Join(tempDir, "seq-map.cbor")
		cfg.Config.SeqMapPath = seqMapPath

		// Create and initialize a state
		state := &replicationState{}
		err = state.init()
		assert.NoError(t, err)
		defer state.fl.Close()

		// Save some sequences
		_, err = state.save("stream1", 100)
		assert.NoError(t, err)

		_, err = state.save("stream2", 200)
		assert.NoError(t, err)

		// Get the sequences
		assert.Equal(t, uint64(100), state.get("stream1"))
		assert.Equal(t, uint64(200), state.get("stream2"))
		assert.Equal(t, uint64(0), state.get("nonexistent"))
	})
}

================
File: logstream/replication_state.go
================
package logstream

import (
	"errors"
	"io"
	"os"
	"sync"

	"github.com/fxamacker/cbor/v2"
	"github.com/wongfei2009/harmonylite/cfg"
)

var ErrNotInitialized = errors.New("not initialized")

type replicationState struct {
	seq  map[string]uint64
	lock *sync.RWMutex
	fl   *os.File
}

func (r *replicationState) init() error {
	fl, err := os.OpenFile(cfg.Config.SeqMapPath, os.O_RDWR|os.O_CREATE|os.O_SYNC, 0666)
	if err != nil {
		return err
	}

	r.seq = make(map[string]uint64)
	r.lock = &sync.RWMutex{}
	r.fl = fl

	idx, err := fl.Seek(0, io.SeekEnd)
	if err != nil {
		return err
	}

	if idx < 1 {
		return nil
	}

	_, err = fl.Seek(0, io.SeekStart)
	if err != nil {
		return err
	}

	return cbor.NewDecoder(fl).Decode(&r.seq)
}

func (r *replicationState) save(streamName string, seq uint64) (uint64, error) {
	r.lock.Lock()
	defer r.lock.Unlock()
	if r.fl == nil {
		return 0, ErrNotInitialized
	}

	if old, found := r.seq[streamName]; found && seq <= old {
		return old, nil
	}

	_, err := r.fl.Seek(0, io.SeekStart)
	if err != nil {
		return 0, err
	}
	defer r.fl.Sync()

	r.seq[streamName] = seq
	err = cbor.NewEncoder(r.fl).Encode(r.seq)
	if err != nil {
		return 0, err
	}

	return seq, nil
}

func (r *replicationState) get(streamName string) uint64 {
	r.lock.RLock()
	defer r.lock.RUnlock()

	if old, found := r.seq[streamName]; found {
		return old
	}

	return 0
}

================
File: logstream/replicator_meta_store.go
================
package logstream

import (
	"context"
	"time"

	"github.com/fxamacker/cbor/v2"
	"github.com/nats-io/nats.go"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
)

type replicatorMetaStore struct {
	nats.KeyValue
}

type replicatorLockInfo struct {
	NodeID    uint64
	Timestamp int64
}

func newReplicatorMetaStore(name string, nc *nats.Conn) (*replicatorMetaStore, error) {
	jsx, err := nc.JetStream()
	if err != nil {
		return nil, err
	}

	kv, err := jsx.KeyValue(name)
	if err == nats.ErrBucketNotFound {
		kv, err = jsx.CreateKeyValue(&nats.KeyValueConfig{
			Storage:  nats.FileStorage,
			Bucket:   name,
			Replicas: cfg.Config.ReplicationLog.Replicas,
		})
	}

	if err != nil {
		return nil, err
	}

	return &replicatorMetaStore{KeyValue: kv}, nil
}

func (m *replicatorMetaStore) AcquireLease(name string, duration time.Duration) (bool, error) {
	now := time.Now().UnixMilli()
	info := &replicatorLockInfo{
		NodeID:    cfg.Config.NodeID,
		Timestamp: now,
	}
	payload, err := info.Serialize()
	if err != nil {
		return false, err
	}

	entry, err := m.Get(name)
	if err == nats.ErrKeyNotFound {
		rev := uint64(0)
		rev, err = m.Create(name, payload)
		if rev != 0 && err == nil {
			return true, nil
		}
	}

	if err != nil {
		return false, err
	}

	err = info.DeserializeFrom(entry.Value())
	if err != nil {
		return false, err
	}

	if info.NodeID != cfg.Config.NodeID && info.Timestamp+duration.Milliseconds() > now {
		return false, err
	}

	_, err = m.Update(name, payload, entry.Revision())
	if err != nil {
		return false, err
	}

	return true, nil
}

func (m *replicatorMetaStore) ContextRefreshingLease(
	name string,
	duration time.Duration,
	ctx context.Context,
) (bool, error) {
	locked, err := m.AcquireLease(name, duration)
	go func(locked bool, err error) {
		if !locked || err != nil {
			return
		}

		refresh := time.NewTicker(duration / 2)
		for {
			locked, err = m.AcquireLease(name, duration)
			if err != nil {
				log.Warn().Err(err).Str("name", name).Msg("Error acquiring lease")
				return
			} else if !locked {
				log.Warn().Str("name", name).Msg("Unable to acquire lease")
				continue
			}

			refresh.Reset(duration / 2)
			select {
			case <-refresh.C:
				continue
			case <-ctx.Done():
				return
			}
		}
	}(locked, err)

	return locked, err
}

func (r *replicatorLockInfo) Serialize() ([]byte, error) {
	return cbor.Marshal(r)
}

func (r *replicatorLockInfo) DeserializeFrom(data []byte) error {
	return cbor.Unmarshal(data, r)
}

================
File: logstream/replicator.go
================
package logstream

import (
	"context"
	"errors"
	"fmt"
	"time"

	"github.com/wongfei2009/harmonylite/stream"

	"github.com/klauspost/compress/zstd"
	"github.com/nats-io/nats.go"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
	"github.com/wongfei2009/harmonylite/snapshot"
)

const maxReplicateRetries = 7
const SnapshotShardID = uint64(1)

var SnapshotLeaseTTL = 10 * time.Second

type Replicator struct {
	nodeID             uint64
	shards             uint64
	compressionEnabled bool
	lastSnapshot       time.Time

	client    *nats.Conn
	repState  *replicationState
	metaStore *replicatorMetaStore
	snapshot  snapshot.NatsSnapshot
	streamMap map[uint64]nats.JetStreamContext
}

func NewReplicator(
	snapshot snapshot.NatsSnapshot,
) (*Replicator, error) {
	nodeID := cfg.Config.NodeID
	shards := cfg.Config.ReplicationLog.Shards
	compress := cfg.Config.ReplicationLog.Compress
	updateExisting := cfg.Config.ReplicationLog.UpdateExisting

	nc, err := stream.Connect()
	if err != nil {
		return nil, err
	}

	streamMap := map[uint64]nats.JetStreamContext{}
	for i := uint64(0); i < shards; i++ {
		shard := i + 1
		js, err := nc.JetStream()
		if err != nil {
			return nil, err
		}

		streamCfg := makeShardStreamConfig(shard, shards, compress)
		info, err := js.StreamInfo(streamName(shard, compress), nats.MaxWait(10*time.Second))
		if err == nats.ErrStreamNotFound {
			log.Debug().Uint64("shard", shard).Msg("Creating stream")
			info, err = js.AddStream(streamCfg)
		}

		if err != nil {
			log.Error().
				Err(err).
				Str("name", streamName(shard, compress)).
				Msg("Unable to get stream info...")
			return nil, err
		}

		if updateExisting && !eqShardStreamConfig(&info.Config, streamCfg) {
			log.Warn().Msgf("Stream configuration not same for %s, updating...", streamName(shard, compress))
			info, err = js.UpdateStream(streamCfg)
			if err != nil {
				log.Error().
					Err(err).
					Str("name", streamName(shard, compress)).
					Msg("Unable update stream info...")
				return nil, err
			}
		}

		leader := ""
		if info.Cluster != nil {
			leader = info.Cluster.Leader
		}

		log.Debug().
			Uint64("shard", shard).
			Str("name", info.Config.Name).
			Int("replicas", info.Config.Replicas).
			Str("leader", leader).
			Msg("Stream ready...")

		if err != nil {
			return nil, err
		}

		streamMap[shard] = js
	}

	repState := &replicationState{}
	err = repState.init()
	if err != nil {
		return nil, err
	}

	metaStore, err := newReplicatorMetaStore(cfg.EmbeddedClusterName, nc)
	if err != nil {
		return nil, err
	}

	return &Replicator{
		client:             nc,
		nodeID:             nodeID,
		compressionEnabled: compress,
		lastSnapshot:       time.Time{},

		shards:    shards,
		streamMap: streamMap,
		snapshot:  snapshot,
		repState:  repState,
		metaStore: metaStore,
	}, nil
}

func (r *Replicator) Publish(hash uint64, payload []byte) error {
	shardID := (hash % r.shards) + 1
	js, ok := r.streamMap[shardID]
	if !ok {
		log.Panic().
			Uint64("shard", shardID).
			Msg("Invalid shard")
	}

	if r.compressionEnabled {
		compPayload, err := payloadCompress(payload)
		if err != nil {
			return err
		}

		payload = compPayload
	}

	ack, err := js.Publish(subjectName(shardID), payload)
	if err != nil {
		return err
	}

	if cfg.Config.Snapshot.Enable {
		seq, err := r.repState.save(ack.Stream, ack.Sequence)
		if err != nil {
			return err
		}

		snapshotEntries := uint64(cfg.Config.ReplicationLog.MaxEntries) / r.shards
		if snapshotEntries != 0 && seq%snapshotEntries == 0 && shardID == SnapshotShardID {
			log.Debug().
				Uint64("seq", seq).
				Str("stream", ack.Stream).
				Msg("Initiating save snapshot")
			go r.SaveSnapshot()
		}
	}

	return nil
}

func (r *Replicator) Listen(shardID uint64, callback func(payload []byte) error) error {
	js := r.streamMap[shardID]

	sub, err := js.SubscribeSync(subjectName(shardID))
	if err != nil {
		return err
	}
	defer sub.Unsubscribe()

	savedSeq := r.repState.get(streamName(shardID, r.compressionEnabled))
	for sub.IsValid() {
		msg, err := sub.NextMsg(5 * time.Second)
		if errors.Is(err, nats.ErrTimeout) {
			continue
		}

		if err != nil {
			return err
		}

		meta, err := msg.Metadata()
		if err != nil {
			return err
		}

		if meta.Sequence.Stream <= savedSeq {
			continue
		}

		err = r.invokeListener(callback, msg)
		if err != nil {
			msg.Nak()
			if errors.Is(err, context.Canceled) {
				return nil
			}

			log.Error().Err(err).Msg("Replication failed, terminating...")
			return err
		}

		savedSeq, err = r.repState.save(meta.Stream, meta.Sequence.Stream)
		if err != nil {
			return err
		}

		err = msg.Ack()
		if err != nil {
			return err
		}
	}

	return nil
}

func (r *Replicator) RestoreSnapshot() error {
	if r.snapshot == nil {
		return nil
	}

	for shardID, js := range r.streamMap {
		strName := streamName(shardID, r.compressionEnabled)
		info, err := js.StreamInfo(strName)
		if err != nil {
			return err
		}

		savedSeq := r.repState.get(strName)
		if savedSeq < info.State.FirstSeq {
			return r.snapshot.RestoreSnapshot()
		}
	}

	return nil
}

func (r *Replicator) LastSaveSnapshotTime() time.Time {
	return r.lastSnapshot
}

func (r *Replicator) SaveSnapshot() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	locked, err := r.metaStore.ContextRefreshingLease("snapshot", SnapshotLeaseTTL, ctx)
	if err != nil {
		log.Warn().Err(err).Msg("Error acquiring snapshot lock")
		return
	}

	if !locked {
		log.Info().Msg("Snapshot saving already locked, skipping")
		return
	}

	r.ForceSaveSnapshot()
}

func (r *Replicator) ForceSaveSnapshot() {
	if r.snapshot == nil {
		return
	}

	err := r.snapshot.SaveSnapshot()
	if err != nil {
		log.Error().
			Err(err).
			Msg("Unable snapshot database")
		return
	}

	r.lastSnapshot = time.Now()
}

func (r *Replicator) ReloadCertificates() error {
	if cfg.Config.NATS.CAFile != "" {
		err := nats.RootCAs(cfg.Config.NATS.CAFile)(&r.client.Opts)
		if err != nil {
			return err
		}
	}

	if cfg.Config.NATS.CertFile != "" && cfg.Config.NATS.KeyFile != "" {
		err := nats.ClientCert(cfg.Config.NATS.CertFile, cfg.Config.NATS.KeyFile)(&r.client.Opts)
		if err != nil {
			return err
		}
	}

	return nil
}

func (r *Replicator) invokeListener(callback func(payload []byte) error, msg *nats.Msg) error {
	var err error
	payload := msg.Data

	if r.compressionEnabled {
		payload, err = payloadDecompress(msg.Data)
		if err != nil {
			return err
		}
	}

	for repRetry := 0; repRetry < maxReplicateRetries; repRetry++ {
		// Don't invoke for first iteration
		if repRetry != 0 {
			err = msg.InProgress()
			if err != nil {
				return err
			}
		}

		err = callback(payload)
		if err == context.Canceled {
			return err
		}

		if err == nil {
			return nil
		}

		log.Error().
			Err(err).
			Int("attempt", repRetry).
			Msg("Unable to process message retrying")
	}

	return err
}

func makeShardStreamConfig(shardID uint64, totalShards uint64, compressed bool) *nats.StreamConfig {
	streamName := streamName(shardID, compressed)
	replicas := cfg.Config.ReplicationLog.Replicas
	if replicas < 1 {
		replicas = int(totalShards>>1) + 1
	}

	if replicas > 5 {
		replicas = 5
	}

	return &nats.StreamConfig{
		Name:              streamName,
		Subjects:          []string{subjectName(shardID)},
		Discard:           nats.DiscardOld,
		MaxMsgs:           cfg.Config.ReplicationLog.MaxEntries,
		Storage:           nats.FileStorage,
		Retention:         nats.LimitsPolicy,
		AllowDirect:       true,
		MaxConsumers:      -1,
		MaxMsgsPerSubject: -1,
		Duplicates:        0,
		DenyDelete:        true,
		Replicas:          replicas,
	}
}

func eqShardStreamConfig(a *nats.StreamConfig, b *nats.StreamConfig) bool {
	return a.Name == b.Name &&
		len(a.Subjects) == 1 &&
		len(b.Subjects) == 1 &&
		a.Subjects[0] == b.Subjects[0] &&
		a.Discard == b.Discard &&
		a.MaxMsgs == b.MaxMsgs &&
		a.Storage == b.Storage &&
		a.Retention == b.Retention &&
		a.AllowDirect == b.AllowDirect &&
		a.MaxConsumers == b.MaxConsumers &&
		a.MaxMsgsPerSubject == b.MaxMsgsPerSubject &&
		a.Duplicates == b.Duplicates &&
		a.DenyDelete == b.DenyDelete &&
		a.Replicas == b.Replicas
}

func streamName(shardID uint64, compressed bool) string {
	compPostfix := ""
	if compressed {
		compPostfix = "-c"
	}

	return fmt.Sprintf("%s%s-%d", cfg.Config.NATS.StreamPrefix, compPostfix, shardID)
}

func subjectName(shardID uint64) string {
	return fmt.Sprintf("%s-%d", cfg.Config.NATS.SubjectPrefix, shardID)
}

func payloadCompress(payload []byte) ([]byte, error) {
	enc, err := zstd.NewWriter(nil)
	if err != nil {
		return nil, err
	}

	return enc.EncodeAll(payload, nil), nil
}

func payloadDecompress(payload []byte) ([]byte, error) {
	dec, err := zstd.NewReader(nil)
	if err != nil {
		return nil, err
	}

	return dec.DecodeAll(payload, nil)
}

================
File: pool/connection_pool_test.go
================
package pool

import (
	"database/sql"
	"io/ioutil"
	"os"
	"path/filepath"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
)

// MockConnectionDisposer implements ConnectionDisposer for testing
type MockConnectionDisposer struct {
	mock.Mock
}

func (m *MockConnectionDisposer) Dispose(obj *SQLiteConnection) error {
	args := m.Called(obj)
	return args.Error(0)
}

// createTestDB creates a temporary SQLite database file for testing
func createTestDB(t *testing.T) (string, func()) {
	tempDir, err := ioutil.TempDir("", "harmonylite-test-*")
	if err != nil {
		t.Fatalf("Failed to create temp directory: %v", err)
	}

	dbPath := filepath.Join(tempDir, "test.db")

	// Create a minimal database
	db, err := sql.Open("sqlite3", dbPath)
	if err != nil {
		os.RemoveAll(tempDir)
		t.Fatalf("Failed to create test database: %v", err)
	}
	defer db.Close()

	// Create a test table
	_, err = db.Exec("CREATE TABLE IF NOT EXISTS test (id INTEGER PRIMARY KEY, name TEXT)")
	if err != nil {
		os.RemoveAll(tempDir)
		t.Fatalf("Failed to create test table: %v", err)
	}

	cleanup := func() {
		os.RemoveAll(tempDir)
	}

	return dbPath, cleanup
}

func TestSQLiteConnection_Init(t *testing.T) {
	t.Run("successful initialization", func(t *testing.T) {
		dbPath, cleanup := createTestDB(t)
		defer cleanup()

		conn := &SQLiteConnection{state: 0}
		mockDisposer := new(MockConnectionDisposer)

		// Test initialization
		err := conn.init(dbPath, mockDisposer)

		// Assertions
		assert.NoError(t, err)
		assert.NotNil(t, conn.db)
		assert.NotNil(t, conn.raw)
		assert.NotNil(t, conn.gSQL)
		assert.Equal(t, mockDisposer, conn.disposer)
		assert.Equal(t, int32(1), conn.state)
	})

	t.Run("already initialized", func(t *testing.T) {
		conn := &SQLiteConnection{state: 1}
		mockDisposer := new(MockConnectionDisposer)

		// Init should return immediately without changing state
		err := conn.init("non-existent.db", mockDisposer)

		assert.NoError(t, err)
		assert.Nil(t, conn.db)
		assert.Nil(t, conn.raw)
		assert.Nil(t, conn.gSQL)
		assert.Nil(t, conn.disposer)
		assert.Equal(t, int32(1), conn.state)
	})

	t.Run("initialization error", func(t *testing.T) {
		conn := &SQLiteConnection{state: 0}
		mockDisposer := new(MockConnectionDisposer)

		// Test initialization with invalid path
		err := conn.init("/invalid/path/that/does/not/exist.db", mockDisposer)

		// Assertions
		assert.Error(t, err)
		assert.Nil(t, conn.db)
		assert.Nil(t, conn.raw)
		assert.Nil(t, conn.gSQL)
		assert.Nil(t, conn.disposer)
		assert.Equal(t, int32(0), conn.state) // State should be reset
	})
}

func TestSQLiteConnection_Reset(t *testing.T) {
	t.Run("successful reset", func(t *testing.T) {
		dbPath, cleanup := createTestDB(t)
		defer cleanup()

		// Create and initialize a connection
		conn := &SQLiteConnection{state: 0}
		mockDisposer := new(MockConnectionDisposer)
		err := conn.init(dbPath, mockDisposer)
		assert.NoError(t, err)

		// Reset connection
		conn.reset()

		// Assertions
		assert.Nil(t, conn.db)
		assert.Nil(t, conn.raw)
		assert.Nil(t, conn.gSQL)
		assert.Nil(t, conn.disposer)
		assert.Equal(t, int32(0), conn.state)
	})

	t.Run("already reset", func(t *testing.T) {
		conn := &SQLiteConnection{state: 0}

		// Reset should return immediately without changing state
		conn.reset()

		assert.Equal(t, int32(0), conn.state)
	})
}

func TestNewSQLitePool(t *testing.T) {
	t.Run("lazy initialization", func(t *testing.T) {
		dbPath, cleanup := createTestDB(t)
		defer cleanup()

		// Test creating a pool with lazy initialization
		pool, err := NewSQLitePool(dbPath, 5, true)

		// Assertions
		assert.NoError(t, err)
		assert.NotNil(t, pool)
		assert.Equal(t, dbPath, pool.dns)
		assert.Equal(t, 5, cap(pool.connections))
		assert.Equal(t, 5, len(pool.connections))

		// Check that connections have state = 0 (not initialized)
		conn := <-pool.connections
		assert.Equal(t, int32(0), conn.state)
		pool.connections <- conn // Return it to the pool
	})

	t.Run("eager initialization", func(t *testing.T) {
		dbPath, cleanup := createTestDB(t)
		defer cleanup()

		// Test creating a pool with eager initialization
		pool, err := NewSQLitePool(dbPath, 3, false)

		// Assertions
		assert.NoError(t, err)
		assert.NotNil(t, pool)
		assert.Equal(t, 3, cap(pool.connections))
		assert.Equal(t, 3, len(pool.connections))

		// Check that connections have state = 1 (initialized)
		conn := <-pool.connections
		assert.Equal(t, int32(1), conn.state)
		assert.NotNil(t, conn.db)
		pool.connections <- conn // Return it to the pool
	})

	t.Run("initialization error", func(t *testing.T) {
		// Test creating a pool with eager initialization that fails
		pool, err := NewSQLitePool("/invalid/path/to/db.db", 3, false)

		// Assertions
		assert.Error(t, err)
		assert.Nil(t, pool)
	})
}

func TestSQLitePool_Borrow(t *testing.T) {
	t.Run("successful borrow", func(t *testing.T) {
		dbPath, cleanup := createTestDB(t)
		defer cleanup()

		// Create a pool with one connection
		pool, err := NewSQLitePool(dbPath, 1, true)
		assert.NoError(t, err)

		// Borrow the connection
		conn, err := pool.Borrow()

		// Assertions
		assert.NoError(t, err)
		assert.NotNil(t, conn)
		assert.NotNil(t, conn.db)
		assert.NotNil(t, conn.raw)
		assert.Equal(t, pool, conn.disposer)
		assert.Equal(t, int32(1), conn.state)

		// Pool should be empty now
		assert.Equal(t, 0, len(pool.connections))

		// Return the connection to cleanup
		err = conn.Return()
		assert.NoError(t, err)
	})

	t.Run("initialization error", func(t *testing.T) {
		// Create a pool with one connection
		pool, err := NewSQLitePool("/invalid/path/to/db.db", 1, true)
		assert.NoError(t, err)

		// Borrow the connection
		conn, err := pool.Borrow()

		// Assertions
		assert.Error(t, err)
		assert.Nil(t, conn)

		// Pool should have a new empty connection
		assert.Equal(t, 1, len(pool.connections))
	})
}

func TestSQLitePool_Dispose(t *testing.T) {
	t.Run("dispose to correct pool", func(t *testing.T) {
		dbPath, cleanup := createTestDB(t)
		defer cleanup()

		// Create a pool with one connection
		pool, err := NewSQLitePool(dbPath, 1, true)
		assert.NoError(t, err)

		// Borrow a connection
		conn, err := pool.Borrow()
		assert.NoError(t, err)

		// Test disposing
		err = pool.Dispose(conn)

		// Assertions
		assert.NoError(t, err)
		assert.Equal(t, 1, len(pool.connections))
	})

	t.Run("dispose to wrong pool", func(t *testing.T) {
		dbPath1, cleanup1 := createTestDB(t)
		defer cleanup1()

		dbPath2, cleanup2 := createTestDB(t)
		defer cleanup2()

		// Create two pools
		pool1, err := NewSQLitePool(dbPath1, 1, true)
		assert.NoError(t, err)

		pool2, err := NewSQLitePool(dbPath2, 1, true)
		assert.NoError(t, err)

		// Borrow from pool1
		conn, err := pool1.Borrow()
		assert.NoError(t, err)

		// Try to dispose to pool2
		err = pool2.Dispose(conn)

		// Assertions
		assert.Error(t, err)
		assert.Equal(t, ErrWrongPool, err)
		assert.Equal(t, 1, len(pool2.connections)) // pool2 should still have its original connection

		// Return to correct pool to clean up
		err = pool1.Dispose(conn)
		assert.NoError(t, err)
	})
}

func TestSQLiteConnection_Return(t *testing.T) {
	dbPath, cleanup := createTestDB(t)
	defer cleanup()

	// Create a pool with one connection
	pool, err := NewSQLitePool(dbPath, 1, true)
	assert.NoError(t, err)

	// Borrow the connection
	conn, err := pool.Borrow()
	assert.NoError(t, err)

	// Test returning the connection
	err = conn.Return()

	// Assertions
	assert.NoError(t, err)
	assert.Equal(t, 1, len(pool.connections))
}

func TestSQLiteConnection_Accessors(t *testing.T) {
	dbPath, cleanup := createTestDB(t)
	defer cleanup()

	// Create and initialize a connection
	conn := &SQLiteConnection{state: 0}
	mockDisposer := new(MockConnectionDisposer)
	err := conn.init(dbPath, mockDisposer)
	assert.NoError(t, err)

	// Test accessor methods
	assert.NotNil(t, conn.SQL())
	assert.NotNil(t, conn.Raw())
	assert.NotNil(t, conn.DB())

	// Clean up
	conn.reset()
}

func TestSQLitePool_Concurrency(t *testing.T) {
	// Skip for quick tests as this may be slightly slower
	if testing.Short() {
		t.Skip("Skipping concurrency test in short mode")
	}

	dbPath, cleanup := createTestDB(t)
	defer cleanup()

	// Create a pool with 5 connections
	poolSize := 5
	pool, err := NewSQLitePool(dbPath, poolSize, false)
	assert.NoError(t, err)

	// Test with 10 goroutines (more than pool size) borrowing and returning
	numGoroutines := 10
	var wg sync.WaitGroup
	wg.Add(numGoroutines)

	for i := 0; i < numGoroutines; i++ {
		go func() {
			defer wg.Done()

			// Each goroutine borrows and returns 3 times
			for j := 0; j < 3; j++ {
				conn, err := pool.Borrow()
				if err != nil {
					t.Errorf("Failed to borrow connection: %v", err)
					return
				}

				// Simulate some work
				time.Sleep(10 * time.Millisecond)

				err = conn.Return()
				if err != nil {
					t.Errorf("Failed to return connection: %v", err)
					return
				}
			}
		}()
	}

	// Wait for all goroutines to complete
	wg.Wait()

	// After all operations, the pool should have all connections back
	assert.Equal(t, poolSize, len(pool.connections))
}

================
File: pool/connection_pool.go
================
package pool

import (
	"database/sql"
	"errors"
	"sync/atomic"
	"time"

	"github.com/doug-martin/goqu/v9"
	"github.com/mattn/go-sqlite3"
)

var ErrWrongPool = errors.New("returning object to wrong pool")

type ConnectionDisposer interface {
	Dispose(obj *SQLiteConnection) error
}

type SQLiteConnection struct {
	db       *sql.DB
	raw      *sqlite3.SQLiteConn
	gSQL     *goqu.Database
	disposer ConnectionDisposer
	state    int32
}

type SQLitePool struct {
	connections chan *SQLiteConnection
	dns         string
}

func (q *SQLiteConnection) SQL() *sql.DB {
	return q.db
}

func (q *SQLiteConnection) Raw() *sqlite3.SQLiteConn {
	return q.raw
}

func (q *SQLiteConnection) DB() *goqu.Database {
	return q.gSQL
}

func (q *SQLiteConnection) Return() error {
	return q.disposer.Dispose(q)
}

func (q *SQLiteConnection) init(dns string, disposer ConnectionDisposer) error {
	if !atomic.CompareAndSwapInt32(&q.state, 0, 1) {
		return nil
	}

	dbC, rawC, err := OpenRaw(dns)
	if err != nil {
		atomic.SwapInt32(&q.state, 0)
		return err
	}

	q.raw = rawC
	q.db = dbC
	q.gSQL = goqu.New("sqlite", dbC)
	q.disposer = disposer
	return nil
}

func (q *SQLiteConnection) reset() {
	if !atomic.CompareAndSwapInt32(&q.state, 1, 0) {
		return
	}

	q.db.Close()
	q.raw.Close()

	q.db = nil
	q.raw = nil
	q.gSQL = nil
	q.disposer = nil
}

func NewSQLitePool(dns string, poolSize int, lazy bool) (*SQLitePool, error) {
	ret := &SQLitePool{
		connections: make(chan *SQLiteConnection, poolSize),
		dns:         dns,
	}

	for i := 0; i < poolSize; i++ {
		con := &SQLiteConnection{}
		if !lazy {
			err := con.init(dns, ret)
			if err != nil {
				return nil, err
			}
		}
		ret.connections <- con
	}

	return ret, nil
}

func (q *SQLitePool) Borrow() (*SQLiteConnection, error) {
	c := <-q.connections
	err := c.init(q.dns, q)

	if err != nil {
		q.connections <- &SQLiteConnection{}
		return nil, err
	}

	return c, nil
}

func (q *SQLitePool) Dispose(obj *SQLiteConnection) error {
	if obj.disposer != q {
		return ErrWrongPool
	}

	q.connections <- obj
	return nil
}

func OpenRaw(dns string) (*sql.DB, *sqlite3.SQLiteConn, error) {
	var rawConn *sqlite3.SQLiteConn
	d := &sqlite3.SQLiteDriver{
		ConnectHook: func(conn *sqlite3.SQLiteConn) error {
			rawConn = conn
			return conn.RegisterFunc("harmonylite_version", func() string {
				return "0.1"
			}, true)
		},
	}

	conn := sql.OpenDB(SqliteDriverConnector{driver: d, dns: dns})
	conn.SetConnMaxLifetime(0)
	conn.SetConnMaxIdleTime(10 * time.Second)

	err := conn.Ping()
	if err != nil {
		return nil, nil, err
	}

	return conn, rawConn, nil
}

================
File: pool/sqlite_driver_connector.go
================
package pool

import (
	"context"
	"database/sql/driver"
)

type SqliteDriverConnector struct {
	driver driver.Driver
	dns    string
}

func (t SqliteDriverConnector) Connect(_ context.Context) (driver.Conn, error) {
	return t.driver.Open(t.dns)
}

func (t SqliteDriverConnector) Driver() driver.Driver {
	return t.driver
}

================
File: snapshot/db_snapshot_test.go
================
package snapshot

import (
	"errors"
	"fmt"
	"os"
	"path"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/mock"
)

// Create interfaces that match the subset of methods we need to mock
type dbBackupper interface {
	BackupTo(path string) error
	GetPath() string
}

// MockDB implements the dbBackupper interface
type MockDB struct {
	mock.Mock
}

func (m *MockDB) BackupTo(path string) error {
	args := m.Called(path)
	return args.Error(0)
}

func (m *MockDB) GetPath() string {
	args := m.Called()
	return args.String(0)
}

// MockStorage implements the Storage interface
type MockStorage struct {
	mock.Mock
}

func (m *MockStorage) Upload(name, filePath string) error {
	args := m.Called(name, filePath)
	return args.Error(0)
}

func (m *MockStorage) Download(filePath, name string) error {
	args := m.Called(filePath, name)
	return args.Error(0)
}

// Create a test version of NatsDBSnapshot that uses our interface
type testNatsDBSnapshot struct {
	mutex   *sync.Mutex
	db      dbBackupper
	storage Storage
}

// Copy of the main NewNatsDBSnapshot constructor but using our interface
func newTestSnapshot(d dbBackupper, s Storage) *testNatsDBSnapshot {
	return &testNatsDBSnapshot{
		mutex:   &sync.Mutex{},
		db:      d,
		storage: s,
	}
}

// Copy of the SaveSnapshot method but adapted for our test struct
func (n *testNatsDBSnapshot) SaveSnapshot() error {
	locked := n.mutex.TryLock()
	if !locked {
		return ErrPendingSnapshot
	}

	defer n.mutex.Unlock()
	tmpSnapshot, err := os.MkdirTemp(os.TempDir(), tempDirPattern)
	if err != nil {
		return err
	}
	defer cleanupDir(tmpSnapshot)

	bkFilePath := path.Join(tmpSnapshot, snapshotFileName)
	err = n.db.BackupTo(bkFilePath)
	if err != nil {
		return err
	}

	return n.storage.Upload(snapshotFileName, bkFilePath)
}

// Copy of the RestoreSnapshot method but adapted for our test struct
// and using a function parameter for RestoreFrom to make it testable
func (n *testNatsDBSnapshot) RestoreSnapshot(restoreFromFn func(destPath, bkFilePath string) error) error {
	n.mutex.Lock()
	defer n.mutex.Unlock()

	tmpSnapshotPath, err := os.MkdirTemp(os.TempDir(), tempDirPattern)
	if err != nil {
		return err
	}
	defer cleanupDir(tmpSnapshotPath)

	bkFilePath := path.Join(tmpSnapshotPath, snapshotFileName)
	err = n.storage.Download(bkFilePath, snapshotFileName)
	if err == ErrNoSnapshotFound {
		return nil
	}

	if err != nil {
		return err
	}

	err = restoreFromFn(n.db.GetPath(), bkFilePath)
	if err != nil {
		return err
	}

	return nil
}

// TestNatsDBSnapshot_SaveSnapshot tests the SaveSnapshot method
func TestNatsDBSnapshot_SaveSnapshot(t *testing.T) {
	t.Run("SuccessfulSave", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object
		snapshot := newTestSnapshot(mockDB, mockStorage)

		// Set expectations
		mockDB.On("BackupTo", mock.AnythingOfType("string")).Return(nil)
		mockStorage.On("Upload", snapshotFileName, mock.AnythingOfType("string")).Return(nil)

		// Call the method
		err := snapshot.SaveSnapshot()

		// Assert
		assert.NoError(t, err)
		mockDB.AssertExpectations(t)
		mockStorage.AssertExpectations(t)
	})

	t.Run("ConcurrentSaveAttempts", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object with a manually controlled mutex
		snapshot := &testNatsDBSnapshot{
			mutex:   &sync.Mutex{},
			db:      mockDB,
			storage: mockStorage,
		}

		// Lock the mutex to simulate an ongoing snapshot
		snapshot.mutex.Lock()

		// Try to save while locked
		err := snapshot.SaveSnapshot()

		// Assert that we get the pending error
		assert.Equal(t, ErrPendingSnapshot, err)

		// Unlock for cleanup
		snapshot.mutex.Unlock()
	})

	t.Run("BackupFailure", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object
		snapshot := newTestSnapshot(mockDB, mockStorage)

		// Expected error
		expectedErr := errors.New("backup failed")

		// Set expectations
		mockDB.On("BackupTo", mock.AnythingOfType("string")).Return(expectedErr)

		// Call the method
		err := snapshot.SaveSnapshot()

		// Assert
		assert.Equal(t, expectedErr, err)
		mockDB.AssertExpectations(t)
		// Storage should not be called if backup fails
		mockStorage.AssertNotCalled(t, "Upload")
	})

	t.Run("UploadFailure", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object
		snapshot := newTestSnapshot(mockDB, mockStorage)

		// Expected error
		expectedErr := errors.New("upload failed")

		// Set expectations
		mockDB.On("BackupTo", mock.AnythingOfType("string")).Return(nil)
		mockStorage.On("Upload", snapshotFileName, mock.AnythingOfType("string")).Return(expectedErr)

		// Call the method
		err := snapshot.SaveSnapshot()

		// Assert
		assert.Equal(t, expectedErr, err)
		mockDB.AssertExpectations(t)
		mockStorage.AssertExpectations(t)
	})
}

// TestNatsDBSnapshot_RestoreSnapshot tests the RestoreSnapshot method
func TestNatsDBSnapshot_RestoreSnapshot(t *testing.T) {
	t.Run("SuccessfulRestore", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object
		snapshot := newTestSnapshot(mockDB, mockStorage)

		// Set up expectations
		dbPath := "/path/to/db.sqlite"
		mockDB.On("GetPath").Return(dbPath)
		mockStorage.On("Download", mock.AnythingOfType("string"), snapshotFileName).Return(nil)

		// Mock restore function
		restoreCalled := false
		mockRestoreFrom := func(destPath, bkFilePath string) error {
			assert.Equal(t, dbPath, destPath)
			assert.Contains(t, bkFilePath, snapshotFileName)
			restoreCalled = true
			return nil
		}

		// Call the method
		err := snapshot.RestoreSnapshot(mockRestoreFrom)

		// Assert
		assert.NoError(t, err)
		assert.True(t, restoreCalled, "RestoreFrom should have been called")
		mockDB.AssertExpectations(t)
		mockStorage.AssertExpectations(t)
	})

	t.Run("NoSnapshotFound", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object
		snapshot := newTestSnapshot(mockDB, mockStorage)

		// Set up expectations
		mockStorage.On("Download", mock.AnythingOfType("string"), snapshotFileName).Return(ErrNoSnapshotFound)

		// Call the method
		mockRestoreFrom := func(destPath, bkFilePath string) error {
			t.Fatal("RestoreFrom should not be called")
			return nil
		}

		err := snapshot.RestoreSnapshot(mockRestoreFrom)

		// Assert
		assert.NoError(t, err, "Should not return error when no snapshot is found")
		mockStorage.AssertExpectations(t)
		mockDB.AssertNotCalled(t, "GetPath")
	})

	t.Run("DownloadFailure", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object
		snapshot := newTestSnapshot(mockDB, mockStorage)

		// Expected error
		expectedErr := errors.New("download failed")

		// Set up expectations
		mockStorage.On("Download", mock.AnythingOfType("string"), snapshotFileName).Return(expectedErr)

		// Call the method
		mockRestoreFrom := func(destPath, bkFilePath string) error {
			t.Fatal("RestoreFrom should not be called")
			return nil
		}

		err := snapshot.RestoreSnapshot(mockRestoreFrom)

		// Assert
		assert.Equal(t, expectedErr, err)
		mockStorage.AssertExpectations(t)
		mockDB.AssertNotCalled(t, "GetPath")
	})

	t.Run("RestoreFailure", func(t *testing.T) {
		// Create mocks
		mockDB := new(MockDB)
		mockStorage := new(MockStorage)

		// Create snapshot object
		snapshot := newTestSnapshot(mockDB, mockStorage)

		// Expected error
		expectedErr := errors.New("restore failed")

		// Set up expectations
		dbPath := "/path/to/db.sqlite"
		mockDB.On("GetPath").Return(dbPath)
		mockStorage.On("Download", mock.AnythingOfType("string"), snapshotFileName).Return(nil)

		// Mock restore function that fails
		mockRestoreFrom := func(destPath, bkFilePath string) error {
			return expectedErr
		}

		// Call the method
		err := snapshot.RestoreSnapshot(mockRestoreFrom)

		// Assert
		assert.Equal(t, expectedErr, err)
		mockDB.AssertExpectations(t)
		mockStorage.AssertExpectations(t)
	})
}

// TestFileHash tests the fileHash function
func TestFileHash(t *testing.T) {
	// Create a temporary file
	tmpFile, err := os.CreateTemp("", "hash-test-*.txt")
	assert.NoError(t, err)
	defer os.Remove(tmpFile.Name())

	// Write some content
	content := "This is test content for hashing"
	_, err = tmpFile.Write([]byte(content))
	assert.NoError(t, err)
	tmpFile.Close()

	// Get the hash
	hash1, err := fileHash(tmpFile.Name())
	assert.NoError(t, err)
	assert.NotEmpty(t, hash1)

	// Get the hash again to verify it's deterministic
	hash2, err := fileHash(tmpFile.Name())
	assert.NoError(t, err)
	assert.Equal(t, hash1, hash2)

	// Test with a non-existent file
	_, err = fileHash("non-existent-file")
	assert.Error(t, err)
}

// TestCleanupDir tests the cleanupDir function
func TestCleanupDir(t *testing.T) {
	t.Run("SuccessfulCleanup", func(t *testing.T) {
		// Create a temporary directory
		tmpDir, err := os.MkdirTemp("", tempDirPattern)
		assert.NoError(t, err)

		// Create a file in the directory
		testFile := path.Join(tmpDir, "test.txt")
		err = os.WriteFile(testFile, []byte("test"), 0644)
		assert.NoError(t, err)

		// Call cleanupDir
		cleanupDir(tmpDir)

		// Verify directory is gone
		_, err = os.Stat(tmpDir)
		assert.True(t, os.IsNotExist(err), "Directory should not exist after cleanup")
	})

	t.Run("CleanupNonexistentDir", func(t *testing.T) {
		// Use a path that doesn't exist
		nonExistentDir := path.Join(os.TempDir(), "nonexistent-"+fmt.Sprintf("%d", time.Now().UnixNano()))

		// Call cleanupDir - this should not panic
		cleanupDir(nonExistentDir)
	})
}

================
File: snapshot/db_snapshot.go
================
package snapshot

import (
	"errors"
	"fmt"
	"hash/fnv"
	"io"
	"os"
	"path"
	"sync"
	"time"

	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/db"
)

var ErrPendingSnapshot = errors.New("system busy capturing snapshot")

const snapshotFileName = "snapshot.db"
const tempDirPattern = "harmonylite-snapshot-*"

type NatsDBSnapshot struct {
	mutex   *sync.Mutex
	db      *db.SqliteStreamDB
	storage Storage
}

func NewNatsDBSnapshot(d *db.SqliteStreamDB, snapshotStorage Storage) *NatsDBSnapshot {
	return &NatsDBSnapshot{
		mutex:   &sync.Mutex{},
		db:      d,
		storage: snapshotStorage,
	}
}

func (n *NatsDBSnapshot) SaveSnapshot() error {
	locked := n.mutex.TryLock()
	if !locked {
		return ErrPendingSnapshot
	}

	defer n.mutex.Unlock()
	tmpSnapshot, err := os.MkdirTemp(os.TempDir(), tempDirPattern)
	if err != nil {
		return err
	}
	defer cleanupDir(tmpSnapshot)

	bkFilePath := path.Join(tmpSnapshot, snapshotFileName)
	err = n.db.BackupTo(bkFilePath)
	if err != nil {
		return err
	}

	return n.storage.Upload(snapshotFileName, bkFilePath)
}

func (n *NatsDBSnapshot) RestoreSnapshot() error {
	n.mutex.Lock()
	defer n.mutex.Unlock()

	tmpSnapshotPath, err := os.MkdirTemp(os.TempDir(), tempDirPattern)
	if err != nil {
		return err
	}
	defer cleanupDir(tmpSnapshotPath)

	bkFilePath := path.Join(tmpSnapshotPath, snapshotFileName)
	err = n.storage.Download(bkFilePath, snapshotFileName)
	if err == ErrNoSnapshotFound {
		log.Warn().Err(err).Msg("System will now continue without restoring snapshot")
		return nil
	}

	if err != nil {
		return err
	}

	log.Info().Str("path", bkFilePath).Msg("Downloaded snapshot, restoring...")
	err = db.RestoreFrom(n.db.GetPath(), bkFilePath)
	if err != nil {
		return err
	}

	log.Info().Str("path", bkFilePath).Msg("Restore complete...")
	return nil
}

func cleanupDir(p string) {
	for i := 0; i < 5; i++ {
		err := os.RemoveAll(p)
		if err == nil {
			return
		}

		log.Warn().Err(err).Str("path", p).Msg("Unable to cleanup directory path")
		time.Sleep(1 * time.Second)
	}

	log.Error().Str("path", p).Msg("Unable to cleanup temp path, this might cause disk wastage")
}

func fileHash(p string) (string, error) {
	f, err := os.Open(p)
	if err != nil {
		return "", err
	}
	defer f.Close()

	h := fnv.New64()
	if _, err := io.Copy(h, f); err != nil {
		return "", err
	}

	return fmt.Sprintf("%x", h.Sum64()), nil
}

================
File: snapshot/nats_snapshot.go
================
package snapshot

import (
	"errors"

	"github.com/wongfei2009/harmonylite/cfg"
)

var ErrInvalidStorageType = errors.New("invalid snapshot storage type")
var ErrNoSnapshotFound = errors.New("no snapshot found")
var ErrRequiredParameterMissing = errors.New("required parameter missing")

type NatsSnapshot interface {
	SaveSnapshot() error
	RestoreSnapshot() error
}

type Storage interface {
	Upload(name, filePath string) error
	Download(filePath, name string) error
}

func NewSnapshotStorage() (Storage, error) {
	c := cfg.Config

	switch c.SnapshotStorageType() {
	case cfg.SFTP:
		return newSFTPStorage()
	case cfg.WebDAV:
		return newWebDAVStorage()
	case cfg.Nats:
		return newNatsStorage()
	case cfg.S3:
		return newS3Storage()
	}

	return nil, ErrInvalidStorageType
}

================
File: snapshot/nats_storage.go
================
package snapshot

import (
	"os"
	"time"

	"github.com/nats-io/nats.go"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
	"github.com/wongfei2009/harmonylite/stream"
)

const hashHeaderKey = "harmonylite-snapshot-tag"

type natsStorage struct {
	nc *nats.Conn
}

func (n *natsStorage) Upload(name, filePath string) error {
	blb, err := getBlobStore(n.nc)
	if err != nil {
		return err
	}

	err = blb.Delete(name)
	if err != nil && err != nats.ErrObjectNotFound {
		return err
	}

	hash, err := fileHash(filePath)
	if err != nil {
		return err
	}

	rfl, err := os.Open(filePath)
	if err != nil {
		return err
	}
	defer rfl.Close()

	info, err := blb.Put(&nats.ObjectMeta{
		Name: name,
		Headers: map[string][]string{
			hashHeaderKey: {hash},
		},
	}, rfl)
	if err != nil {
		return err
	}

	log.Info().
		Str("hash", hash).
		Str("file_name", name).
		Str("file_path", filePath).
		Uint64("size", info.Size).
		Uint32("chunks", info.Chunks).
		Msg("Snapshot saved to NATS")

	return nil
}

func (n *natsStorage) Download(filePath, name string) error {
	blb, err := getBlobStore(n.nc)
	if err != nil {
		return err
	}

	for {
		err = blb.GetFile(name, filePath)
		if err == nil {
			return nil
		}

		if err == nats.ErrObjectNotFound {
			return ErrNoSnapshotFound
		}

		if jsmErr, ok := err.(nats.JetStreamError); ok {
			log.Warn().
				Err(err).
				Int("Status", jsmErr.APIError().Code).
				Msg("Error downloading snapshot")

			if jsmErr.APIError().Code == 503 {
				time.Sleep(time.Second)
				continue
			}
		}

		return err
	}
}

func getBlobStore(conn *nats.Conn) (nats.ObjectStore, error) {
	js, err := conn.JetStream(nats.MaxWait(30 * time.Second))
	if err != nil {
		return nil, err
	}

	blb, err := js.ObjectStore(blobBucketName())
	if err == nats.ErrStreamNotFound {
		blb, err = js.CreateObjectStore(&nats.ObjectStoreConfig{
			Bucket:      blobBucketName(),
			Replicas:    cfg.Config.Snapshot.Nats.Replicas,
			Storage:     nats.FileStorage,
			Description: "Bucket to store snapshot",
		})
	}

	return blb, err
}

func newNatsStorage() (*natsStorage, error) {
	nc, err := stream.Connect()
	if err != nil {
		return nil, err
	}

	return &natsStorage{nc: nc}, nil
}

func blobBucketName() string {
	if cfg.Config.Snapshot.Nats.BucketName == "" {
		return cfg.Config.NATS.StreamPrefix + "-snapshot-store"
	}

	return cfg.Config.Snapshot.Nats.BucketName
}

================
File: snapshot/s3_storage.go
================
package snapshot

import (
	"context"
	"fmt"
	"net/http"
	"time"

	"github.com/minio/minio-go/v7"
	"github.com/minio/minio-go/v7/pkg/credentials"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
)

type s3Storage struct {
	mc *minio.Client
}

func (s s3Storage) Upload(name, filePath string) error {
	ctx := context.Background()
	cS3 := cfg.Config.Snapshot.S3
	bucketPath := fmt.Sprintf("%s/%s", cS3.DirPath, name)
	info, err := s.mc.FPutObject(ctx, cS3.Bucket, bucketPath, filePath, minio.PutObjectOptions{})
	if err != nil {
		return err
	}

	log.Info().
		Str("file_name", name).
		Int64("size", info.Size).
		Str("file_path", filePath).
		Str("bucket", info.Bucket).
		Msg("Snapshot saved to S3")

	return nil
}

func (s s3Storage) Download(filePath, name string) error {
	ctx := context.Background()
	cS3 := cfg.Config.Snapshot.S3
	bucketPath := fmt.Sprintf("%s/%s", cS3.DirPath, name)
	err := s.mc.FGetObject(ctx, cS3.Bucket, bucketPath, filePath, minio.GetObjectOptions{})
	if mErr, ok := err.(minio.ErrorResponse); ok {
		if mErr.StatusCode == http.StatusNotFound {
			return ErrNoSnapshotFound
		}
	}

	return err
}

func newS3Storage() (*s3Storage, error) {
	c := cfg.Config
	cS3 := c.Snapshot.S3
	v := credentials.Value{
		AccessKeyID:     cS3.AccessKey,
		SecretAccessKey: cS3.SecretKey,
		SessionToken:    cS3.SessionToken,
	}

	if cS3.AccessKey == "" && cS3.SecretKey == "" {
		v.SignerType = credentials.SignatureAnonymous
	} else {
		v.SignerType = credentials.SignatureV4
	}

	chain := []credentials.Provider{
		&credentials.EnvAWS{},
		&credentials.EnvMinio{},
		&credentials.Static{
			Value: v,
		},
	}

	creds := credentials.NewChainCredentials(chain)
	mc, err := minio.New(cS3.Endpoint, &minio.Options{
		Creds:  creds,
		Secure: cS3.UseSSL,
	})
	if err != nil {
		return nil, err
	}

	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	exists, err := mc.BucketExists(ctx, cS3.Bucket)
	if err != nil {
		return nil, err
	}

	if !exists {
		err = mc.MakeBucket(ctx, cS3.Bucket, minio.MakeBucketOptions{})
		if err != nil {
			return nil, err
		}
	}

	return &s3Storage{
		mc: mc,
	}, nil
}

================
File: snapshot/sftp_storage.go
================
package snapshot

import (
	"net"
	"net/url"
	"os"
	"path"

	"github.com/pkg/sftp"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
	"golang.org/x/crypto/ssh"
)

type sftpStorage struct {
	client     *sftp.Client
	uploadPath string
}

func (s *sftpStorage) Upload(name, filePath string) error {
	err := s.client.MkdirAll(s.uploadPath)
	if err != nil {
		return err
	}

	srcFile, err := os.Open(filePath)
	if err != nil {
		return err
	}
	defer srcFile.Close()

	uploadPath := path.Join(s.uploadPath, name)
	dstFile, err := s.client.OpenFile(uploadPath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC)
	if err != nil {
		return err
	}
	defer dstFile.Close()

	bytes, err := dstFile.ReadFrom(srcFile)
	if err != nil {
		return err
	}

	log.Info().
		Str("file_name", name).
		Str("file_path", filePath).
		Str("sftp_path", uploadPath).
		Int64("bytes", bytes).
		Msg("Snapshot uploaded to SFTP server")
	return nil
}

func (s *sftpStorage) Download(filePath, name string) error {
	remotePath := path.Join(s.uploadPath, name)
	srcFile, err := s.client.Open(remotePath)
	if err != nil {
		if err.Error() == "file does not exist" {
			return ErrNoSnapshotFound
		}
		return err
	}
	defer srcFile.Close()

	dstFile, err := os.Create(filePath)
	if err != nil {
		return err
	}
	defer dstFile.Close()

	bytes, err := dstFile.ReadFrom(srcFile)
	log.Info().
		Str("file_name", name).
		Str("file_path", filePath).
		Str("sftp_path", remotePath).
		Int64("bytes", bytes).
		Msg("Snapshot downloaded from SFTP server")
	return err
}

func newSFTPStorage() (*sftpStorage, error) {
	// Get the SFTP URL from the environment
	sftpURL := cfg.Config.Snapshot.SFTP.Url
	u, err := url.Parse(sftpURL)
	if err != nil {
		return nil, err
	}

	password, hasPassword := u.User.Password()
	authMethod := make([]ssh.AuthMethod, 0)
	if hasPassword {
		authMethod = append(authMethod, ssh.Password(password))
	}

	// Set up the SSH config
	config := &ssh.ClientConfig{
		User: u.User.Username(),
		Auth: authMethod,
		HostKeyCallback: func(hostname string, remote net.Addr, key ssh.PublicKey) error {
			log.Info().
				Str("hostname", hostname).
				Str("remote", remote.String()).
				Str("public_key_type", key.Type()).
				Msg("Host connected for SFTP storage")
			return nil
		},
		BannerCallback: func(message string) error {
			log.Info().Str("message", message).Msgf("Server message...")
			return nil
		},
	}

	// Connect to the SSH server
	conn, err := ssh.Dial("tcp", u.Host, config)
	if err != nil {
		return nil, err
	}

	// Open the SFTP client
	client, err := sftp.NewClient(conn)
	if err != nil {
		return nil, err
	}

	return &sftpStorage{client: client, uploadPath: u.Path}, nil
}

================
File: snapshot/webdav_storage.go
================
package snapshot

import (
	"fmt"
	"io"
	"io/fs"
	"net/url"
	"os"
	"path"
	"time"

	"github.com/rs/zerolog/log"
	"github.com/studio-b12/gowebdav"
	"github.com/wongfei2009/harmonylite/cfg"
)

const queryParamTargetDir = "dir"
const queryParamLogin = "login"
const queryParamSecret = "secret"

type webDAVStorage struct {
	client *gowebdav.Client
	path   string
}

func (w *webDAVStorage) Upload(name, filePath string) error {
	rfl, err := os.Open(filePath)
	if err != nil {
		return err
	}
	defer rfl.Close()

	err = w.makeStoragePath()
	if err != nil {
		return err
	}

	nodePath := fmt.Sprintf("%s-%d-temp-%s", cfg.Config.NodeName(), time.Now().UnixMilli(), name)
	err = w.client.WriteStream(nodePath, rfl, 0644)
	if err != nil {
		return err
	}

	completedPath := path.Join("/", w.path, name)
	err = w.client.Rename(nodePath, completedPath, true)
	if err != nil {
		return err
	}

	log.Info().
		Str("file_name", name).
		Str("file_path", filePath).
		Str("webdav_path", completedPath).
		Msg("Snapshot saved to WebDAV")
	return nil
}

func (w *webDAVStorage) Download(filePath, name string) error {
	completedPath := path.Join(w.path, name)
	rst, err := w.client.ReadStream(completedPath)
	if err != nil {
		if fsErr, ok := err.(*fs.PathError); ok {
			if wdErr, ok := fsErr.Err.(gowebdav.StatusError); ok && wdErr.Status == 404 {
				return ErrNoSnapshotFound
			}
		}
		return err
	}
	defer rst.Close()

	wst, err := os.Create(filePath)
	if err != nil {
		return err
	}
	defer wst.Close()

	if _, err = io.Copy(wst, rst); err != nil {
		return err
	}

	log.Info().
		Str("file_name", name).
		Str("file_path", filePath).
		Str("webdav_path", completedPath).
		Msg("Snapshot downloaded from WebDAV")
	return nil
}

func (w *webDAVStorage) makeStoragePath() error {
	err := w.client.MkdirAll(w.path, 0740)
	if err == nil {
		return nil
	}

	if fsError, ok := err.(*os.PathError); ok {
		if wdErr, ok := fsError.Err.(gowebdav.StatusError); ok {
			if wdErr.Status == 409 { // Conflict means directory already exists!
				return nil
			}
		}
	}

	return err
}

func newWebDAVStorage() (*webDAVStorage, error) {
	webDAVCfg := cfg.Config.Snapshot.WebDAV
	u, err := url.Parse(webDAVCfg.Url)
	if err != nil {
		return nil, err
	}

	qp := u.Query()
	targetDir := qp.Get(queryParamTargetDir)
	if targetDir == "" {
		targetDir = "/"
	}

	login := qp.Get(queryParamLogin)
	secret := qp.Get(queryParamSecret)
	if login == "" || secret == "" {
		return nil, ErrRequiredParameterMissing
	}

	// Remove webdav parameters from query params
	qp.Del(queryParamTargetDir)
	qp.Del(queryParamLogin)
	qp.Del(queryParamSecret)

	// Set query params without parameters
	u.RawQuery = qp.Encode()
	cl := gowebdav.NewAuthClient(u.String(), gowebdav.NewAutoAuth(login, secret))
	ret := &webDAVStorage{client: cl, path: targetDir}

	err = cl.Connect()
	if err != nil {
		return nil, err
	}

	return ret, nil
}

================
File: stream/embedded_nats.go
================
package stream

import (
	"net"
	"path"
	"strconv"
	"sync"
	"time"

	"github.com/nats-io/nats-server/v2/server"
	"github.com/nats-io/nats.go"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
)

type embeddedNats struct {
	server *server.Server
	lock   *sync.Mutex
}

var embeddedIns = &embeddedNats{
	server: nil,
	lock:   &sync.Mutex{},
}

func parseHostAndPort(adr string) (string, int, error) {
	host, portStr, err := net.SplitHostPort(adr)
	if err != nil {
		return "", 0, err
	}

	port, err := strconv.Atoi(portStr)
	if err != nil {
		return "", 0, err
	}

	return host, port, nil
}

func startEmbeddedServer(nodeName string) (*embeddedNats, error) {
	embeddedIns.lock.Lock()
	defer embeddedIns.lock.Unlock()

	if embeddedIns.server != nil {
		return embeddedIns, nil
	}

	host, port, err := parseHostAndPort(cfg.Config.NATS.BindAddress)
	if err != nil {
		return nil, err
	}

	opts := &server.Options{
		ServerName:         nodeName,
		Host:               host,
		Port:               port,
		NoSigs:             true,
		JetStream:          true,
		JetStreamMaxMemory: -1,
		JetStreamMaxStore:  -1,
		Cluster: server.ClusterOpts{
			Name: cfg.EmbeddedClusterName,
		},
		LeafNode: server.LeafNodeOpts{},
	}

	if *cfg.ClusterPeersFlag != "" {
		opts.Routes = server.RoutesFromStr(*cfg.ClusterPeersFlag)
	}

	if *cfg.ClusterAddrFlag != "" {
		host, port, err := parseHostAndPort(*cfg.ClusterAddrFlag)
		if err != nil {
			return nil, err
		}

		opts.Cluster.ListenStr = *cfg.ClusterAddrFlag
		opts.Cluster.Host = host
		opts.Cluster.Port = port
	}

	if *cfg.LeafServerFlag != "" {
		opts.LeafNode.Remotes = parseRemoteLeafOpts()
	}

	if cfg.Config.NATS.ServerConfigFile != "" {
		err := opts.ProcessConfigFile(cfg.Config.NATS.ServerConfigFile)
		if err != nil {
			return nil, err
		}
	}

	originalRoutes := opts.Routes
	if len(opts.Routes) != 0 {
		opts.Routes = flattenRoutes(originalRoutes, true)
	}

	if opts.StoreDir == "" {
		opts.StoreDir = path.Join(cfg.DataRootDir, "nats", nodeName)
	}

	s, err := server.NewServer(opts)
	if err != nil {
		return nil, err
	}

	s.SetLogger(
		&natsLogger{log.With().Str("from", "nats").Logger()},
		opts.Debug,
		opts.Trace,
	)
	s.Start()

	embeddedIns.server = s
	return embeddedIns, nil
}

func (e *embeddedNats) prepareConnection(opts ...nats.Option) (*nats.Conn, error) {
	e.lock.Lock()
	s := e.server
	e.lock.Unlock()

	for !s.ReadyForConnections(1 * time.Second) {
		continue
	}

	opts = append(opts, nats.InProcessServer(s))
	for {
		c, err := nats.Connect("", opts...)
		if err != nil {
			log.Warn().Err(err).Msg("NATS server not accepting connections...")
			continue
		}

		j, err := c.JetStream()
		if err != nil {
			return nil, err
		}

		st, err := j.StreamInfo("harmonylite-r", nats.MaxWait(1*time.Second))
		if err == nats.ErrStreamNotFound || st != nil {
			log.Info().Msg("Streaming ready...")
			return c, nil
		}

		c.Close()
		log.Debug().Err(err).Msg("Streams not ready, waiting for NATS streams to come up...")
		time.Sleep(1 * time.Second)
	}
}

================
File: stream/nats_logger.go
================
package stream

import "github.com/rs/zerolog"

type natsLogger struct {
	zerolog.Logger
}

func (n *natsLogger) Noticef(format string, v ...interface{}) {
	n.Info().Msgf(format, v...)
}

func (n *natsLogger) Warnf(format string, v ...interface{}) {
	n.Warn().Msgf(format, v...)
}

func (n *natsLogger) Fatalf(format string, v ...interface{}) {
	n.Fatal().Msgf(format, v...)
}

func (n *natsLogger) Errorf(format string, v ...interface{}) {
	n.Error().Msgf(format, v...)
}

func (n *natsLogger) Debugf(format string, v ...interface{}) {
	n.Debug().Msgf(format, v...)
}

func (n *natsLogger) Tracef(format string, v ...interface{}) {
	n.Trace().Msgf(format, v...)
}

================
File: stream/nats.go
================
package stream

import (
	"strings"
	"time"

	"github.com/nats-io/nats.go"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
)

func Connect() (*nats.Conn, error) {
	opts := setupConnOptions()

	creds, err := getNatsAuthFromConfig()
	if err != nil {
		return nil, err
	}

	tls, err := getNatsTLSFromConfig()
	if err != nil {
		return nil, err
	}

	opts = append(opts, creds...)
	opts = append(opts, tls...)
	if len(cfg.Config.NATS.URLs) == 0 {
		embedded, err := startEmbeddedServer(cfg.Config.NodeName())
		if err != nil {
			return nil, err
		}

		return embedded.prepareConnection(opts...)
	}

	url := strings.Join(cfg.Config.NATS.URLs, ", ")

	var conn *nats.Conn
	for i := 0; i < cfg.Config.NATS.ConnectRetries; i++ {
		conn, err = nats.Connect(url, opts...)
		if err == nil && conn.Status() == nats.CONNECTED {
			break
		}

		log.Warn().
			Err(err).
			Int("attempt", i+1).
			Int("attempt_limit", cfg.Config.NATS.ConnectRetries).
			Str("status", conn.Status().String()).
			Msg("NATS connection failed")
	}

	return conn, err
}

func getNatsAuthFromConfig() ([]nats.Option, error) {
	opts := make([]nats.Option, 0)

	if cfg.Config.NATS.CredsUser != "" {
		opt := nats.UserInfo(cfg.Config.NATS.CredsUser, cfg.Config.NATS.CredsPassword)
		opts = append(opts, opt)
	}

	if cfg.Config.NATS.SeedFile != "" {
		opt, err := nats.NkeyOptionFromSeed(cfg.Config.NATS.SeedFile)
		if err != nil {
			return nil, err
		}

		opts = append(opts, opt)
	}

	return opts, nil
}

func getNatsTLSFromConfig() ([]nats.Option, error) {
	opts := make([]nats.Option, 0)

	if cfg.Config.NATS.CAFile != "" {
		opt := nats.RootCAs(cfg.Config.NATS.CAFile)
		opts = append(opts, opt)
	}

	if cfg.Config.NATS.CertFile != "" && cfg.Config.NATS.KeyFile != "" {
		opt := nats.ClientCert(cfg.Config.NATS.CertFile, cfg.Config.NATS.KeyFile)
		opts = append(opts, opt)
	}

	return opts, nil
}

func setupConnOptions() []nats.Option {
	return []nats.Option{
		nats.Name(cfg.Config.NodeName()),
		nats.RetryOnFailedConnect(true),
		nats.ReconnectWait(time.Duration(cfg.Config.NATS.ReconnectWaitSeconds) * time.Second),
		nats.MaxReconnects(cfg.Config.NATS.ConnectRetries),
		nats.ClosedHandler(func(nc *nats.Conn) {
			log.Error().
				Err(nc.LastError()).
				Msg("NATS client exiting")
		}),
		nats.DisconnectErrHandler(func(nc *nats.Conn, err error) {
			log.Error().
				Err(err).
				Msg("NATS client disconnected")
		}),
		nats.ReconnectHandler(func(nc *nats.Conn) {
			log.Info().
				Str("url", nc.ConnectedUrl()).
				Msg("NATS client reconnected")
		}),
	}
}

================
File: stream/routes_discover.go
================
package stream

import (
	"fmt"
	"net"
	"net/url"
	"strconv"
	"strings"
	"time"

	"github.com/nats-io/nats-server/v2/server"
	"github.com/rs/zerolog/log"
	"github.com/samber/lo"
	"github.com/wongfei2009/harmonylite/cfg"
)

func parseRemoteLeafOpts() []*server.RemoteLeafOpts {
	leafServers := server.RoutesFromStr(*cfg.LeafServerFlag)
	if len(leafServers) != 0 {
		leafServers = flattenRoutes(leafServers, true)
	}

	return lo.Map[*url.URL, *server.RemoteLeafOpts](
		leafServers,
		func(u *url.URL, _ int) *server.RemoteLeafOpts {
			hub := u.Query().Get("hub") == "true"
			r := &server.RemoteLeafOpts{
				URLs: []*url.URL{u},
				Hub:  hub,
			}

			return r
		})
}

func flattenRoutes(urls []*url.URL, waitDNSEntries bool) []*url.URL {
	ret := make([]*url.URL, 0)
	for _, u := range urls {
		if u.Scheme == "dns" {
			ret = append(ret, queryDNSRoutes(u, waitDNSEntries)...)
			continue
		}

		ret = append(ret, u)
	}

	return ret
}

func queryDNSRoutes(u *url.URL, waitDNSEntries bool) []*url.URL {
	minPeerStr := u.Query().Get("min")
	intervalStr := u.Query().Get("interval_ms")

	minPeers, err := strconv.Atoi(minPeerStr)
	if err != nil {
		minPeers = 2
	}

	interval, err := strconv.Atoi(intervalStr)
	if err != nil {
		interval = 1000
	}

	log.Info().
		Str("url", u.String()).
		Int("min_peers", minPeers).
		Int("interval", interval).
		Bool("wait_dns_entries", waitDNSEntries).
		Msg("Starting DNS A/AAAA peer discovery")

	if waitDNSEntries {
		minPeers = 0
	}

	for {
		peers, err := getDirectNATSAddresses(u)
		if err != nil {
			log.Error().
				Err(err).
				Str("url", u.String()).
				Msg("Unable to discover peer URLs")
			time.Sleep(time.Duration(interval) * time.Millisecond)
			continue
		}

		urls := strings.Join(
			lo.Map[*url.URL, string](peers, func(i *url.URL, _ int) string { return i.String() }),
			", ",
		)
		log.Info().Str("urls", urls).Msg("Peers discovered")

		if len(peers) >= minPeers {
			return peers
		} else {
			time.Sleep(time.Duration(interval) * time.Millisecond)
		}
	}
}

func getDirectNATSAddresses(u *url.URL) ([]*url.URL, error) {
	v4, v6, err := queryDNS(u.Hostname())
	if err != nil {
		return nil, err
	}
	var ret []*url.URL
	for _, ip := range v4 {
		peerUrl := fmt.Sprintf("nats://%s:%s/", ip, u.Port())
		peer, err := url.Parse(peerUrl)
		if err != nil {
			log.Warn().
				Str("peer_url", peerUrl).
				Msg("Unable to parse URL, might be due to bad DNS entry")
			continue
		}
		ret = append(ret, peer)
	}

	for _, ip := range v6 {
		peerUrl := fmt.Sprintf("nats://[%s]:%s/", ip, u.Port())
		peer, err := url.Parse(peerUrl)
		if err != nil {
			log.Warn().
				Str("peer_url", peerUrl).
				Msg("Unable to parse URL, might be due to bad DNS entry")
			continue
		}
		ret = append(ret, peer)
	}

	return ret, nil
}

func queryDNS(domain string) ([]string, []string, error) {
	ips, err := net.LookupIP(domain)
	if err != nil {
		return nil, nil, err
	}

	var ipv4 []string
	var ipv6 []string
	for _, ip := range ips {
		if v4 := ip.To4(); v4 != nil {
			ipv4 = append(ipv4, v4.String()) // A record
		} else if v6 := ip.To16(); v6 != nil {
			ipv6 = append(ipv6, v6.String()) // AAAA record
		}
	}

	return ipv4, ipv6, nil
}

================
File: telemetry/telemetry.go
================
package telemetry

import (
	"net/http"
	"strconv"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"github.com/rs/zerolog/log"
	"github.com/wongfei2009/harmonylite/cfg"
)

var registry *prometheus.Registry

type Histogram interface {
	Observe(float64)
}

type Counter interface {
	Inc()
	Add(float64)
}

type Gauge interface {
	Set(float64)
	Inc()
	Dec()
	Add(float64)
	Sub(float64)
	SetToCurrentTime()
}

type NoopStat struct{}

func (n NoopStat) Observe(float64) {
}

func (n NoopStat) Set(float64) {
}

func (n NoopStat) Dec() {
}

func (n NoopStat) Sub(float64) {
}

func (n NoopStat) SetToCurrentTime() {
}

func (n NoopStat) Inc() {
}

func (n NoopStat) Add(float64) {
}

func NewCounter(name string, help string) Counter {
	if registry == nil {
		return NoopStat{}
	}

	ret := prometheus.NewCounter(prometheus.CounterOpts{
		Namespace: cfg.Config.Prometheus.Namespace,
		Subsystem: cfg.Config.Prometheus.Subsystem,
		Name:      name,
		Help:      help,
		ConstLabels: map[string]string{
			"node_id": strconv.FormatUint(cfg.Config.NodeID, 10),
		},
	})

	registry.MustRegister(ret)
	return ret
}

func NewGauge(name string, help string) Gauge {
	if registry == nil {
		return NoopStat{}
	}

	ret := prometheus.NewGauge(prometheus.GaugeOpts{
		Namespace: cfg.Config.Prometheus.Namespace,
		Subsystem: cfg.Config.Prometheus.Subsystem,
		Name:      name,
		Help:      help,
		ConstLabels: map[string]string{
			"node_id": strconv.FormatUint(cfg.Config.NodeID, 10),
		},
	})

	registry.MustRegister(ret)
	return ret
}

func NewHistogram(name string, help string) Histogram {
	if registry == nil {
		return NoopStat{}
	}

	ret := prometheus.NewHistogram(prometheus.HistogramOpts{
		Namespace: cfg.Config.Prometheus.Namespace,
		Subsystem: cfg.Config.Prometheus.Subsystem,
		Name:      name,
		Help:      help,
		ConstLabels: map[string]string{
			"node_id": strconv.FormatUint(cfg.Config.NodeID, 10),
		},
	})

	registry.MustRegister(ret)
	return ret
}

func InitializeTelemetry() {
	if !cfg.Config.Prometheus.Enable {
		return
	}

	registry = prometheus.NewRegistry()
	server := http.Server{
		Addr:    cfg.Config.Prometheus.Bind,
		Handler: promhttp.HandlerFor(registry, promhttp.HandlerOpts{Registry: registry}),
	}

	go func() {
		if err := server.ListenAndServe(); err != nil {
			log.Error().Err(err).Msg("Unable to start controller listener")
		}
	}()
}

================
File: tests/e2e/config_test.go
================
package main_test

import "time"

const (
	// Timeouts and Delays
	maxWaitTime         = 30 * time.Second
	pollInterval        = 500 * time.Millisecond
	nodeStartupDelay    = 10 * time.Second
	nodeShutdownTimeout = 5 * time.Second

	// NATS Configuration
	natsConnectTimeout = 10 * time.Second
	natsRetryAttempts  = 5
	natsRetryDelay     = 2 * time.Second

	// Database Configuration
	dbDir = "/tmp"
)

================
File: tests/e2e/e2e_suite_test.go
================
package main_test

import (
	"testing"

	_ "github.com/mattn/go-sqlite3"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

// Run e2e tests using the Ginkgo runner.
func TestE2E(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "HarmonyLite E2E Suite")
}

================
File: tests/e2e/e2e_test.go
================
package main_test

import (
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"sync"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

var wd string

var _ = Describe("HarmonyLite End-to-End Tests", Ordered, func() {
	var node1, node2, node3 *exec.Cmd

	BeforeAll(func() {
		var err error
		wd, err = os.Getwd()
		Expect(err).To(BeNil())
		wd = wd[:len(wd)-len("/tests/e2e")]
		cleanup()

		// Create databases with both tables
		for i := 1; i <= 3; i++ {
			dbPath := filepath.Join(dbDir, fmt.Sprintf("harmonylite-%d.db", i))
			createDatabase(dbPath)     // Creates Books table
			createAuthorsTable(dbPath) // Creates Authors table
		}

		node1, node2, node3 = startCluster()
	})

	AfterAll(func() {
		stopNodes(node1, node2, node3)
	})

	Context("Basic Replication", func() {
		It("should replicate INSERT operations", func() {
			id := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Pride and Prejudice", "Jane Austen", 1813)
			Eventually(func() int {
				return countBooksByTitle(filepath.Join(dbDir, "harmonylite-2.db"), "Pride and Prejudice")
			}, maxWaitTime, pollInterval).Should(Equal(1), "Insert replication failed")
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-3.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Insert not replicated to node 3")
		})

		It("should replicate UPDATE operations", func() {
			id := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Update Test", "Author", 2020)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Initial insert not replicated for update")
			updateBookTitle(filepath.Join(dbDir, "harmonylite-1.db"), id, "Updated Title")
			Eventually(func() string {
				return getBookTitle(filepath.Join(dbDir, "harmonylite-2.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal("Updated Title"), "Update replication failed")
			Eventually(func() string {
				return getBookTitle(filepath.Join(dbDir, "harmonylite-3.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal("Updated Title"), "Update not replicated to node 3")
		})

		It("should replicate DELETE operations", func() {
			id := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Delete Test", "Author", 2020)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Initial insert not replicated for delete")
			deleteBookByID(filepath.Join(dbDir, "harmonylite-1.db"), id)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(0), "Delete replication failed")
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-3.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(0), "Delete not replicated to node 3")
		})
	})

	Context("Concurrency and Conflict Resolution", func() {
		It("should resolve concurrent updates using last-writer-wins", func() {
			id := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Conflict Test", "Author", 2020)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Initial insert not replicated for conflict test")

			var wg sync.WaitGroup
			wg.Add(2)
			go func() {
				defer GinkgoRecover()
				defer wg.Done()
				updateBookTitle(filepath.Join(dbDir, "harmonylite-1.db"), id, "Node1 Update")
			}()
			go func() {
				defer GinkgoRecover()
				defer wg.Done()
				time.Sleep(100 * time.Millisecond) // Ensure Node2 writes last
				updateBookTitle(filepath.Join(dbDir, "harmonylite-2.db"), id, "Node2 Update")
			}()
			wg.Wait()

			Eventually(func() bool {
				t1 := getBookTitle(filepath.Join(dbDir, "harmonylite-1.db"), id)
				t2 := getBookTitle(filepath.Join(dbDir, "harmonylite-2.db"), id)
				t3 := getBookTitle(filepath.Join(dbDir, "harmonylite-3.db"), id)
				consistent := t1 == "Node2 Update" && t2 == "Node2 Update" && t3 == "Node2 Update"
				GinkgoWriter.Printf("Conflict check - Node1: %s, Node2: %s, Node3: %s\n", t1, t2, t3)
				return consistent
			}, maxWaitTime, pollInterval).Should(BeTrue(), "Conflict resolution failed; expected 'Node2 Update' to win")
		})
	})

	Context("Snapshot and Restore", func() {
		It("should save and restore snapshots correctly", func() {
			// Insert data before snapshot
			id := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Snapshot Test", "Author", 2020)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Initial insert not replicated for snapshot test")

			// Stop node 3, save snapshot, and restart
			stopNodes(node3)
			time.Sleep(2 * time.Second) // Allow snapshot to be taken
			node3Cmd := exec.Command(filepath.Join(wd, "harmonylite"), "-config", "examples/node-3-config.toml", "-save-snapshot")
			node3Cmd.Dir = wd
			node3Cmd.Stdout = GinkgoWriter
			node3Cmd.Stderr = GinkgoWriter
			Expect(node3Cmd.Run()).To(Succeed(), "Failed to save snapshot on node 3")
			node3 = startNode("examples/node-3-config.toml", "127.0.0.1:4223", "nats://127.0.0.1:4221/,nats://127.0.0.1:4222/")

			// Verify restored data
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-3.db"), id)
			}, maxWaitTime*2, pollInterval).Should(Equal(1), "Snapshot restore failed on node 3")
		})
	})

	Context("Node Failure and Recovery", func() {
		It("should recover replication after node failure", func() {
			// Insert initial data
			id := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Failure Test", "Author", 2020)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-3.db"), id)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Initial insert not replicated for failure test")

			// Simulate node 3 failure
			stopNodes(node3)
			time.Sleep(2 * time.Second) // Allow some time for failure to propagate

			// Insert more data while node 3 is down
			id2 := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Post-Failure Test", "Author", 2021)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id2)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Insert not replicated to node 2 during node 3 downtime")

			// Restart node 3
			node3 = startNode("examples/node-3-config.toml", "127.0.0.1:4223", "nats://127.0.0.1:4221/,nats://127.0.0.1:4222/")
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-3.db"), id2)
			}, maxWaitTime*2, pollInterval).Should(Equal(1), "Node 3 failed to recover replication after restart")
		})
	})

	Context("Multi-Table Replication", func() {
		BeforeEach(func() {
		})

		It("should replicate changes across multiple tables", func() {
			bookID := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Multi-Table Book", "Jane Austen", 1813)
			authorID := insertAuthor(filepath.Join(dbDir, "harmonylite-1.db"), "Jane Austen", 1775)

			Eventually(func() int {
				return countBooksByTitle(filepath.Join(dbDir, "harmonylite-2.db"), "Multi-Table Book")
			}, maxWaitTime, pollInterval).Should(Equal(1), "Book replication failed across tables")
			Eventually(func() int {
				return countAuthorsByName(filepath.Join(dbDir, "harmonylite-2.db"), "Jane Austen")
			}, maxWaitTime, pollInterval).Should(Equal(1), "Author replication failed across tables")

			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-3.db"), bookID)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Book not replicated to node 3")
			Eventually(func() int {
				return countAuthorsByID(filepath.Join(dbDir, "harmonylite-3.db"), authorID)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Author not replicated to node 3")
		})
	})

	Context("Large Data Volumes", func() {
		It("should handle replication of many records", func() {
			const numRecords = 100
			var ids []int64
			for i := 0; i < numRecords; i++ {
				id := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Bulk Test "+string(rune(i)), "Author", 2020+i)
				ids = append(ids, id)
			}

			Eventually(func() int {
				count := 0
				for _, id := range ids {
					count += countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id)
				}
				return count
			}, maxWaitTime*2, pollInterval).Should(Equal(numRecords), "Failed to replicate all records to node 2")

			Eventually(func() int {
				count := 0
				for _, id := range ids {
					count += countBooksByID(filepath.Join(dbDir, "harmonylite-3.db"), id)
				}
				return count
			}, maxWaitTime*2, pollInterval).Should(Equal(numRecords), "Failed to replicate all records to node 3")
		})
	})

	Context("Configuration Variations", func() {
		It("should work with publish disabled on one node", func() {
			// Stop node 2
			stopNodes(node2)

			// Create a temporary config file for node 2 with publish disabled
			tmpConfigPath := filepath.Join(dbDir, "node-2-disabled-publish.toml")
			originalConfig, err := os.ReadFile(filepath.Join(wd, "examples/node-2-config.toml"))
			Expect(err).To(BeNil(), "Failed to read original config file")

			// Append publish=false to the config
			modifiedConfig := append([]byte("\npublish=false\n"), originalConfig...)
			err = os.WriteFile(tmpConfigPath, modifiedConfig, 0644)
			Expect(err).To(BeNil(), "Failed to write modified config file")

			// Start node 2 with the modified config
			node2 = exec.Command(filepath.Join(wd, "harmonylite"),
				"-config", tmpConfigPath,
				"-cluster-addr", "127.0.0.1:4222",
				"-cluster-peers", "nats://127.0.0.1:4221/,nats://127.0.0.1:4223/")
			node2.Dir = wd
			node2.Stdout = GinkgoWriter
			node2.Stderr = GinkgoWriter
			Expect(node2.Start()).To(Succeed(), "Failed to start node 2 with publish disabled")

			// Insert on node 2 (should not replicate)
			id := insertBook(filepath.Join(dbDir, "harmonylite-2.db"), "No Publish Test", "Author", 2020)
			Consistently(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-1.db"), id)
			}, 5*time.Second, pollInterval).Should(Equal(0), "Data replicated from node 2 despite publish disabled")

			// Insert on node 1 (should replicate to node 2)
			id2 := insertBook(filepath.Join(dbDir, "harmonylite-1.db"), "Publish Test", "Author", 2020)
			Eventually(func() int {
				return countBooksByID(filepath.Join(dbDir, "harmonylite-2.db"), id2)
			}, maxWaitTime, pollInterval).Should(Equal(1), "Data not replicated to node 2 with publish disabled")

			// Clean up
			defer os.Remove(tmpConfigPath)
		})
	})
})

================
File: tests/e2e/helpers_test.go
================
package main_test

import (
	"database/sql"
	"log"
	"os"
	"os/exec"
	"path/filepath"
	"time"

	"github.com/nats-io/nats.go"
	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
)

// -- Cluster Management --

// startCluster initializes and starts a 3-node HarmonyLite cluster.
func startCluster() (node1, node2, node3 *exec.Cmd) {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Starting cluster setup...\n")

	// Start nodes with NATS health checks
	node1 = startNode("examples/node-1-config.toml", "127.0.0.1:4221", "nats://127.0.0.1:4222/,nats://127.0.0.1:4223/")
	node2 = startNode("examples/node-2-config.toml", "127.0.0.1:4222", "nats://127.0.0.1:4221/,nats://127.0.0.1:4223/")
	node3 = startNode("examples/node-3-config.toml", "127.0.0.1:4223", "nats://127.0.0.1:4221/,nats://127.0.0.1:4222/")

	GinkgoWriter.Printf("Cluster started, waiting %v for stabilization\n", nodeStartupDelay*2)
	return node1, node2, node3
}

// stopNodes gracefully stops the provided HarmonyLite nodes.
func stopNodes(nodes ...*exec.Cmd) {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Stopping nodes...\n")

	for _, node := range nodes {
		if node != nil && node.Process != nil {
			if err := node.Process.Kill(); err != nil {
				GinkgoWriter.Printf("Error killing node: %v\n", err)
			}
			// Use a closure to capture the node variable correctly in the Eventually
			func(n *exec.Cmd) {
				Eventually(n.Wait, nodeShutdownTimeout).ShouldNot(Succeed(), "Node should exit with error on kill")
			}(node)
		}
	}
	GinkgoWriter.Printf("Nodes stopped\n")
}

// startNode launches a HarmonyLite node and performs a NATS health check.
func startNode(config, addr, peers string) *exec.Cmd {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Starting node with config %s, addr %s, peers %s\n", config, addr, peers)

	wd, err := os.Getwd()
	Expect(err).To(BeNil())
	wd = wd[:len(wd)-len("/tests/e2e")]

	cmd := exec.Command(filepath.Join(wd, "harmonylite"), "-config", config, "-cluster-addr", addr, "-cluster-peers", peers)
	cmd.Dir = wd
	cmd.Stdout = GinkgoWriter
	cmd.Stderr = GinkgoWriter

	err = cmd.Start()
	Expect(err).To(BeNil(), "Failed to start node with config %s", config)

	// Perform NATS health check
	waitForNATSHealth(addr)

	return cmd
}

// waitForNATSHealth checks if the NATS cluster is healthy with retries.
func waitForNATSHealth(addr string) {
	natsURL := "nats://" + addr // Simplification: assumes NATS is on the same address
	var nc *nats.Conn
	var err error

	for i := 0; i < natsRetryAttempts; i++ {
		nc, err = nats.Connect(natsURL, nats.Timeout(natsConnectTimeout))
		if err == nil {
			break // Connected successfully
		}
		GinkgoWriter.Printf("NATS connection attempt %d failed: %v\n", i+1, err)
		backoff := natsRetryDelay * time.Duration(i+1) // Simple linear backoff
		if backoff > 16*time.Second {
			backoff = 16 * time.Second // Maximum backoff
		}
		time.Sleep(backoff)
	}
	Expect(err).To(BeNil(), "Failed to connect to NATS after retries")
	defer nc.Close()

	Eventually(func() bool {
		if nc.ConnectedServerId() == "" {
			GinkgoWriter.Printf("Waiting for server info\n")
			return false
		}
		GinkgoWriter.Printf("NATS Cluster Healthy\n")
		return true
	}, maxWaitTime, pollInterval).Should(BeTrue(), "NATS cluster did not become healthy")
}

// cleanup removes temporary files and directories.
func cleanup() {
	defer GinkgoRecover()
	patterns := []string{
		filepath.Join(dbDir, "harmonylite-1*"),
		filepath.Join(dbDir, "harmonylite-2*"),
		filepath.Join(dbDir, "harmonylite-3*"),
		filepath.Join(dbDir, "nats*"), // Remove the nats directory as well
	}
	for _, pattern := range patterns {
		files, err := filepath.Glob(pattern)
		if err != nil {
			log.Printf("Error globbing %s: %v", pattern, err)
			continue
		}
		for _, file := range files {
			if err := os.RemoveAll(file); err != nil {
				log.Printf("Error removing %s: %v", file, err)
			}
		}
	}
	GinkgoWriter.Printf("Cleanup completed\n")
}

// -- Database Operations --

// createDatabase creates a SQLite database with the initial schema and data.
func createDatabase(dbPath string) {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Creating database: %s\n", dbPath)
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil(), "Failed to open database %s", dbPath)
	defer db.Close()

	_, err = db.Exec(`
		DROP TABLE IF EXISTS Books;
		CREATE TABLE Books (
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			title TEXT NOT NULL,
			author TEXT NOT NULL,
			publication_year INTEGER
		);
		INSERT INTO Books (title, author, publication_year)
		VALUES
		('The Hitchhiker''s Guide to the Galaxy', 'Douglas Adams', 1979),
		('The Lord of the Rings', 'J.R.R. Tolkien', 1954),
		('Harry Potter and the Sorcerer''s Stone', 'J.K. Rowling', 1997),
		('The Catcher in the Rye', 'J.D. Salinger', 1951),
		('To Kill a Mockingbird', 'Harper Lee', 1960),
		('1984', 'George Orwell', 1949),
		('The Great Gatsby', 'F. Scott Fitzgerald', 1925);
	`)
	Expect(err).To(BeNil(), "Failed to initialize database %s", dbPath)
}

// insertBook inserts a new book into the database.
func insertBook(dbPath, title, author string, year int) int64 {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Inserting %s into %s\n", title, dbPath)
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil(), "Error opening database %s", dbPath)
	defer db.Close()

	res, err := db.Exec(`INSERT INTO Books (title, author, publication_year) VALUES (?, ?, ?)`, title, author, year)
	Expect(err).To(BeNil(), "Error inserting book into %s", dbPath)
	id, err := res.LastInsertId()
	Expect(err).To(BeNil(), "Error getting last insert ID from %s", dbPath)
	return id
}

// updateBookTitle updates the title of a book.
func updateBookTitle(dbPath string, id int64, newTitle string) {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Updating ID %d to %s on %s\n", id, newTitle, dbPath)
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil())
	defer db.Close()
	_, err = db.Exec("UPDATE Books SET title = ? WHERE id = ?", newTitle, id)
	Expect(err).To(BeNil(), "Update failed on %s", dbPath)
}

// deleteBookByID deletes a book from the database by its ID.
func deleteBookByID(dbPath string, id int64) {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Deleting ID %d from %s\n", id, dbPath)
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil())
	defer db.Close()
	_, err = db.Exec("DELETE FROM Books WHERE id = ?", id)
	Expect(err).To(BeNil(), "Delete failed on %s", dbPath)
}

// -- Assertions --

// getBookTitle retrieves the title of a book by its ID.  Returns an empty string if not found.
func getBookTitle(dbPath string, id int64) string {
	defer GinkgoRecover()
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil())
	defer db.Close()
	var title string
	err = db.QueryRow("SELECT title FROM Books WHERE id = ?", id).Scan(&title)
	if err == sql.ErrNoRows {
		return "" // Not found
	}
	Expect(err).To(BeNil(), "Error getting title for ID %d in %s", id, dbPath)
	return title
}

// countBooksByTitle counts the number of books with a given title.
func countBooksByTitle(dbPath, title string) int {
	defer GinkgoRecover()
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil(), "Error opening %s for count", dbPath)
	defer db.Close()
	var count int
	err = db.QueryRow("SELECT COUNT(*) FROM Books WHERE title = ?", title).Scan(&count)
	Expect(err).To(BeNil(), "Error counting title %s in %s", title, dbPath)
	GinkgoWriter.Printf("Counted %d '%s' in %s\n", count, title, dbPath)
	return count
}

// countBooksByID counts the number of books with a given ID.
func countBooksByID(dbPath string, id int64) int {
	defer GinkgoRecover()
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil())
	defer db.Close()
	var count int
	err = db.QueryRow("SELECT COUNT(*) FROM Books WHERE id = ?", id).Scan(&count)
	Expect(err).To(BeNil(), "Error counting ID %d in %s", id, dbPath)
	return count
}

// createAuthorsTable creates an Authors table in the specified database
func createAuthorsTable(dbPath string) {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Creating Authors table in: %s\n", dbPath)
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil(), "Failed to open database %s", dbPath)
	defer db.Close()

	_, err = db.Exec(`
		DROP TABLE IF EXISTS Authors;
		CREATE TABLE Authors (
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			name TEXT NOT NULL,
			birth_year INTEGER
		);
	`)
	Expect(err).To(BeNil(), "Failed to create Authors table in %s", dbPath)
}

// insertAuthor inserts a new author into the database
func insertAuthor(dbPath, name string, birthYear int) int64 {
	defer GinkgoRecover()
	GinkgoWriter.Printf("Inserting author %s into %s\n", name, dbPath)
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil(), "Error opening database %s", dbPath)
	defer db.Close()

	res, err := db.Exec(`INSERT INTO Authors (name, birth_year) VALUES (?, ?)`, name, birthYear)
	Expect(err).To(BeNil(), "Error inserting author into %s", dbPath)
	id, err := res.LastInsertId()
	Expect(err).To(BeNil(), "Error getting last insert ID from %s", dbPath)
	return id
}

// countAuthorsByName counts the number of authors with a given name
func countAuthorsByName(dbPath, name string) int {
	defer GinkgoRecover()
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil(), "Error opening %s for count", dbPath)
	defer db.Close()
	var count int
	err = db.QueryRow("SELECT COUNT(*) FROM Authors WHERE name = ?", name).Scan(&count)
	Expect(err).To(BeNil(), "Error counting name %s in %s", name, dbPath)
	GinkgoWriter.Printf("Counted %d '%s' in %s\n", count, name, dbPath)
	return count
}

// countAuthorsByID counts the number of authors with a given ID
func countAuthorsByID(dbPath string, id int64) int {
	defer GinkgoRecover()
	db, err := sql.Open("sqlite3", dbPath)
	Expect(err).To(BeNil())
	defer db.Close()
	var count int
	err = db.QueryRow("SELECT COUNT(*) FROM Authors WHERE id = ?", id).Scan(&count)
	Expect(err).To(BeNil(), "Error counting ID %d in %s", id, dbPath)
	return count
}

================
File: utils/deep.go
================
package utils

import (
	"bytes"
	"reflect"

	"github.com/fxamacker/cbor/v2"
)

func DeepCopy(dst, src any) error {
	var buf bytes.Buffer
	if err := cbor.NewEncoder(&buf).Encode(src); err != nil {
		return err
	}
	return cbor.NewDecoder(&buf).Decode(dst)
}

func DeepEqualArray[T any](a, b []T) bool {
	if len(a) != len(b) {
		return false
	}

	for i := 0; i < len(a); i++ {
		if !reflect.DeepEqual(a[i], b[i]) {
			return false
		}
	}

	return true
}

================
File: utils/state_context.go
================
package utils

import (
	"context"
	"time"
)

type StateContext struct {
	ctx    context.Context
	cancel context.CancelFunc
}

func NewStateContext() *StateContext {
	ctx, cancel := context.WithCancel(context.Background())
	return &StateContext{
		ctx:    ctx,
		cancel: cancel,
	}
}

func (s *StateContext) Cancel() {
	s.cancel()
}

func (s *StateContext) IsCanceled() bool {
	select {
	case <-s.ctx.Done():
		return s.ctx.Err() == context.Canceled
	case <-time.After(0):
		return false
	}
}

================
File: utils/stop_watch.go
================
package utils

import (
	"time"

	"github.com/rs/zerolog"
	"github.com/wongfei2009/harmonylite/telemetry"
)

type StopWatch struct {
	startTime time.Time
	name      string
}

func NewStopWatch(name string) *StopWatch {
	return &StopWatch{
		startTime: time.Now(),
		name:      name,
	}
}

func (t *StopWatch) Stop() time.Duration {
	return time.Since(t.startTime)
}

func (t *StopWatch) Log(e *zerolog.Event, hist telemetry.Histogram) {
	dur := t.Stop()
	if hist != nil {
		hist.Observe(float64(dur.Microseconds()))
	}

	e.Dur("duration", dur).Str("name", t.name).Send()
}

================
File: utils/timeout.go
================
package utils

import (
	"time"

	"github.com/asaskevich/EventBus"
	"github.com/rs/zerolog/log"
)

type TimeoutPublisher struct {
	duration  time.Duration
	ticker    *time.Ticker
	publisher chan time.Time
}

func AutoResetEventTimer(bus EventBus.Bus, eventName string, duration time.Duration) *TimeoutPublisher {
	t := NewTimeoutPublisher(duration)
	err := bus.Subscribe(eventName, func(args ...any) {
		t.Reset()
	})

	if err != nil {
		log.Panic().Err(err).Msg("Unable to subscribe timeout event bus")
	}

	return t
}

func NewTimeoutPublisher(duration time.Duration) *TimeoutPublisher {
	if duration == 0 {
		return &TimeoutPublisher{
			duration:  duration,
			ticker:    nil,
			publisher: make(chan time.Time),
		}
	}

	ticker := time.NewTicker(duration)
	return &TimeoutPublisher{duration: duration, ticker: ticker, publisher: nil}
}

func (t *TimeoutPublisher) Reset() {
	if t.ticker == nil {
		return
	}

	t.ticker.Reset(t.duration)
}

func (t *TimeoutPublisher) Stop() {
	if t.ticker == nil {
		return
	}

	t.ticker.Stop()
}

func (t *TimeoutPublisher) Channel() <-chan time.Time {
	if t.ticker == nil {
		return t.publisher
	}

	return t.ticker.C
}

================
File: .gitignore
================
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib

# Test binary, built with `go test -c`
*.test

# Output of the go coverage tool, specifically when used with LiteIDE
*.out

# Dependency directories (remove the comment below to include it)
# vendor/
build/
.idea
.vscode
.DS_Store
*.cert
*.crt
*.key
*.pem
harmonylite
dist/

# NPM Stuff
node_modules/
npm-debug.log
yarn-error.log
yarn-debug.log

================
File: build-linux.sh
================
#!/bin/sh

# docker run --rm -v "$PWD":/usr/src/myapp -w /usr/src/myapp -e CGO_ENABLED=1 -e GOARCH=amd64 golang:1.18 go build -v -o build/harmonylite-linux-amd64 harmonylite.go

CC=x86_64-linux-musl-gcc \
CXX=x86_64-linux-musl-g++ \
GOARCH=amd64 GOOS=linux CGO_ENABLED=1 \
go build -ldflags "-linkmode external -extldflags -static" -o dist/linux/amd64/harmonylite

================
File: CODE_OF_CONDUCT.md
================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
harmonylite-coc@googlegroups.com.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.

================
File: config.toml
================
# Path to target SQLite database
db_path="/tmp/harmonylite.db"

# ID to uniquely identify your nodes in your cluster
# It's recommended to always configure this
# node_id=1

# Path to persist the saved sequence map on disk for warm reboot
# If this file is missing HarmonyLite has to download snapshot
# and replay all logs in order to restore database
# seq_map_path="/tmp/seq-map.cbor"

# Replication enabled/disabled (default: true)
# This will allow process to consume incoming changes from NATS
# replicate = true

# Publishing enabled/disabled (default: true)
# This will allow process to control publishing of local DB changes to NATS
# publish = true

# Number of maximum rows to process per change allows configuring the maximum number of rows HarmonyLite
# will process (scan/load in memory) before publishing to NATS (default: 512)
# scan_max_changes = 512

# Cleanup interval in milliseconds used to clean up published rows. This is done in order to reduce write
# load on the system (default: 5000)
# cleanup_interval = 5000

# Sleep timeout in milliseconds, useful for serverless scenarios. If there is no activity within given timelimit,
# a snapshot will be performed, and process will exit. Value of 0 means it's disabled (default: 0).
# sleep_timeout = 15000

# Polling interval in milliseconds, that will explicitly check DB for change logs. This should not be required,
# it's only useful for broken or buggy file system watchers. Value of 0 means it's disabled (default: 0)
# polling_interval = 0

# Snapshots are used to limit log size and have a database snapshot backedup on your
# configured blob storage (NATS for now). This helps speedier recovery or cold boot
# nodes to come up. A Snapshot is taken every log entries are close to max_entries
# configured in replication_log section. It's recommended to use a large value
# for maximum entries in replication log, because SQLite can do 1000s of TPS
# replaying a couple thousands of entries should be really quick.
[snapshot]
# Disabling snapshot disables both restore and save
enabled=true
# Storage for snapshot can be "nats" | "webdav" | "s3" (default "nats")
store="nats"
# Interval sets perodic interval in milliseconds after which an automatic snapshot should be saved
# If there was a snapshot saved within interval range due to other log threshold triggers, then
# new snapshot won't be saved (since it's within time range), a value of 0 means it's disabled.
interval=0

# When setting snapshot.store to "nats" [snapshot.nats] will be used to configure snapshotting details
# NATS connection settings (urls etc.) will be loaded from global [nats] configurations
[snapshot.nats]
# Number of NATS replicas of snapshot object store (max 5). Recommended values: 2-3
replicas=1
# Bucket name for object store to save snapshot on.
#bucket="custom-bucket-name"

# When setting snapshot.store to "s3" [snapshot.s3] will be used to configure snapshotting details
[snapshot.s3]
# For S3 this will be `s3.region-code.amazonaws.com` (check your AWS Console for details).
# For Minio this will point to the host where Minio lives
endpoint="127.0.0.1:9000"

# Directory path within bucket where snapshot is saved and restore from
path="snapshots/harmonylite"

# By default false but should be set to true depending upong Minio configuration, for S3 it should be
# always true. This essentially lets you select between https and http for your hosting.
use_ssl=false

# Access key ID or Minio user name
#access_key="harmonylite"

# Secret Key or Minio password
#secret="ChangeMe"

# Bucket name where snapshots live
bucket="harmonylite"

[snapshot.webdav]
# URL of the WebDAV server root
url="https://<webdav_server>/<web_dav_path>?dir=/snapshots/path/for/harmonylite&login=<username>&secret=<password>"

[snapshot.sftp]
# URL of the SFTP server with path
url="sftp://<user>:<password>@<sftp_server>:<port>/path/to/save/snapshot"

# Change log that is published and persisted in JetStreams by HarmonyLite.
# HarmonyLite auto-configures missing JetStreams when booting up for you.
[replication_log]
# Number of replicas per log to configure (user > 1 for failover and redundancy).
replicas=1
# Number of shards to divide the logs over, each JetStream and subject will be prefixed
# by the configured `subject_prefix` and `stream_prefix` under nats
shards=1
# Max log entries JetStream should persist, JetStream is configured to drop older entries
# Each JetStream is configured to persist on file.
max_entries=1024
# Enable log compression, uses zstd to compress logs as they are streamd to NATS
# This is useful for DB storing large blobs that can be compressed.
compress=true
# Update existing stream if the configurations of JetStream don't match up with configurations
# generated due to parameters above. Use this option carefully because changing shards,
# or max_etries etc. might have undesired side-effects on existing running cluster
update_existing=false


# NATS server configurations
[nats]
# List of NATS server to use as boot server. Reference NATS documentation on how to pass
# authentication credentials as part of URL. Leaving out this list empty will result
# in embedded NATS server being started with node named `harmonylite-node-{node_id}`. 
# NATS configuration can provided via `server_config` variable
urls=[
#    "nats://localhost:4222"
#    "nats://<user>:<password>@<host>:<port>"
]
# Embedded server bind address
bind_address="0.0.0.0:4222"
# Embedded server config file (will only be used if URLs array is empty)
server_config=""
# Subject prefix used when publishing log entries, it's usually suffixed by shard number
# to get the full subject name
subject_prefix="harmonylite-change-log"
# JetStream name prefix used for publishing log entries, it's usually suffixed by shard number
# to get the full JetStream name
stream_prefix="harmonylite-changes"
# Seed file used for client nkey authentication
# nk -gen user > user.seed
# nk -inkey user.seed -pubout > user.pub
# Set to user.seed
# Reference https://docs.nats.io/running-a-nats-service/nats_admin/security/jwt#what-are-nkeys
seed_file=""
# User credentials used for plain user password authentication
user_name=""
user_password=""
# Number of retries when establishing the NATS server connection (will only be used if URLs array is not empty)
connect_retries=5
# Wait time between NATS reconnect attempts (will only be used if URLs array is not empty)
reconnect_wait_seconds=2

[prometheus]
# Enable/Disable prometheus telemetry collection
enable=false
# HTTP endpoint to expose for prometheus matrix collection
# bind=":3010"
# Namespace for prometheus (default: `harmonylite`), applies to all counters, gaugues, histograms
# namespace=""
# Subsystem for prometheus (default: empty), applies to all counters, gauges, histograms
# subsystem=""

# Console STDOUT configurations
[logging]
# Configure console logging
verbose=true
# "console" | "json"
format="console"

================
File: go.mod
================
module github.com/wongfei2009/harmonylite

go 1.24.0

require (
	github.com/BurntSushi/toml v1.4.0
	github.com/asaskevich/EventBus v0.0.0-20200907212545-49d423059eef
	github.com/denisbrodbeck/machineid v1.0.1
	github.com/doug-martin/goqu/v9 v9.19.0
	github.com/fsnotify/fsnotify v1.8.0
	github.com/fxamacker/cbor/v2 v2.7.0
	github.com/google/uuid v1.6.0
	github.com/klauspost/compress v1.18.0
	github.com/mattn/go-sqlite3 v1.14.24
	github.com/minio/minio-go/v7 v7.0.87
	github.com/nats-io/nats-server/v2 v2.10.26
	github.com/nats-io/nats.go v1.39.1
	github.com/onsi/ginkgo/v2 v2.22.2
	github.com/onsi/gomega v1.36.2
	github.com/pkg/sftp v1.13.7
	github.com/prometheus/client_golang v1.21.0
	github.com/rs/zerolog v1.33.0
	github.com/samber/lo v1.49.1
	github.com/stretchr/testify v1.10.0
	github.com/studio-b12/gowebdav v0.10.0
	golang.org/x/crypto v0.35.0
)

require (
	github.com/beorn7/perks v1.0.1 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/dustin/go-humanize v1.0.1 // indirect
	github.com/go-ini/ini v1.67.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-task/slim-sprig/v3 v3.0.0 // indirect
	github.com/goccy/go-json v0.10.5 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/pprof v0.0.0-20241210010833-40e02aabc2ad // indirect
	github.com/klauspost/cpuid/v2 v2.2.10 // indirect
	github.com/kr/fs v0.1.0 // indirect
	github.com/lib/pq v1.10.9 // indirect
	github.com/mattn/go-colorable v0.1.14 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/minio/crc64nvme v1.0.1 // indirect
	github.com/minio/highwayhash v1.0.3 // indirect
	github.com/minio/md5-simd v1.1.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/nats-io/jwt/v2 v2.7.3 // indirect
	github.com/nats-io/nkeys v0.4.10 // indirect
	github.com/nats-io/nuid v1.0.1 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/prometheus/client_model v0.6.1 // indirect
	github.com/prometheus/common v0.62.0 // indirect
	github.com/prometheus/procfs v0.15.1 // indirect
	github.com/rs/xid v1.6.0 // indirect
	github.com/stretchr/objx v0.5.2 // indirect
	github.com/x448/float16 v0.8.4 // indirect
	golang.org/x/net v0.35.0 // indirect
	golang.org/x/sys v0.30.0 // indirect
	golang.org/x/text v0.22.0 // indirect
	golang.org/x/time v0.10.0 // indirect
	golang.org/x/tools v0.30.0 // indirect
	google.golang.org/protobuf v1.36.5 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)

================
File: harmonylite.go
================
package main

import (
	"context"
	"flag"
	"io"
	"net/http"
	"net/http/pprof"
	_ "net/http/pprof"
	"os"
	"time"

	"github.com/wongfei2009/harmonylite/telemetry"
	"github.com/wongfei2009/harmonylite/utils"

	"github.com/wongfei2009/harmonylite/cfg"
	"github.com/wongfei2009/harmonylite/db"
	"github.com/wongfei2009/harmonylite/logstream"
	"github.com/wongfei2009/harmonylite/snapshot"

	"github.com/asaskevich/EventBus"
	"github.com/rs/zerolog"
	"github.com/rs/zerolog/log"
)

func main() {
	flag.Parse()
	err := cfg.Load(*cfg.ConfigPathFlag)
	if err != nil {
		panic(err)
	}

	var writer io.Writer = zerolog.NewConsoleWriter()
	if cfg.Config.Logging.Format == "json" {
		writer = os.Stdout
	}
	gLog := zerolog.New(writer).
		With().
		Timestamp().
		Uint64("node_id", cfg.Config.NodeID).
		Logger()

	if cfg.Config.Logging.Verbose {
		log.Logger = gLog.Level(zerolog.DebugLevel)
	} else {
		log.Logger = gLog.Level(zerolog.InfoLevel)
	}

	if *cfg.ProfServer != "" {
		go func() {
			mux := http.NewServeMux()
			mux.HandleFunc("/debug/pprof/", pprof.Index)
			mux.HandleFunc("/debug/pprof/cmdline", pprof.Cmdline)
			mux.HandleFunc("/debug/pprof/profile", pprof.Profile)
			mux.HandleFunc("/debug/pprof/symbol", pprof.Symbol)
			mux.HandleFunc("/debug/pprof/trace", pprof.Trace)

			err := http.ListenAndServe(*cfg.ProfServer, mux)
			if err != nil {
				log.Error().Err(err).Msg("unable to bind profiler server")
			}
		}()
	}

	log.Debug().Msg("Initializing telemetry")
	telemetry.InitializeTelemetry()

	log.Debug().Str("path", cfg.Config.DBPath).Msg("Opening database")
	streamDB, err := db.OpenStreamDB(cfg.Config.DBPath)
	if err != nil {
		log.Error().Err(err).Msg("Unable to open database")
		return
	}

	if *cfg.CleanupFlag {
		err = streamDB.RemoveCDC(true)
		if err != nil {
			log.Panic().Err(err).Msg("Unable to clean up...")
		} else {
			log.Info().Msg("Cleanup complete...")
		}

		return
	}

	snpStore, err := snapshot.NewSnapshotStorage()
	if err != nil {
		log.Panic().Err(err).Msg("Unable to initialize snapshot storage")
	}

	replicator, err := logstream.NewReplicator(snapshot.NewNatsDBSnapshot(streamDB, snpStore))
	if err != nil {
		log.Panic().Err(err).Msg("Unable to initialize replicators")
	}

	if *cfg.SaveSnapshotFlag {
		replicator.ForceSaveSnapshot()
		return
	}

	if cfg.Config.Snapshot.Enable && cfg.Config.Replicate {
		err = replicator.RestoreSnapshot()
		if err != nil {
			log.Panic().Err(err).Msg("Unable to restore snapshot")
		}
	}

	log.Info().Msg("Listing tables to watch...")
	tableNames, err := db.GetAllDBTables(cfg.Config.DBPath)
	if err != nil {
		log.Error().Err(err).Msg("Unable to list all tables")
		return
	}

	eventBus := EventBus.New()
	ctxSt := utils.NewStateContext()

	streamDB.OnChange = onTableChanged(replicator, ctxSt, eventBus, cfg.Config.NodeID)
	log.Info().Msg("Starting change data capture pipeline...")
	if err := streamDB.InstallCDC(tableNames); err != nil {
		log.Error().Err(err).Msg("Unable to install change data capture pipeline")
		return
	}

	errChan := make(chan error)
	for i := uint64(0); i < cfg.Config.ReplicationLog.Shards; i++ {
		go changeListener(streamDB, replicator, ctxSt, eventBus, i+1, errChan)
	}

	sleepTimeout := utils.AutoResetEventTimer(
		eventBus,
		"pulse",
		time.Duration(cfg.Config.SleepTimeout)*time.Millisecond,
	)
	cleanupInterval := time.Duration(cfg.Config.CleanupInterval) * time.Millisecond
	cleanupTicker := time.NewTicker(cleanupInterval)
	defer cleanupTicker.Stop()

	snapshotInterval := time.Duration(cfg.Config.Snapshot.Interval) * time.Millisecond
	snapshotTicker := utils.NewTimeoutPublisher(snapshotInterval)
	defer snapshotTicker.Stop()

	for {
		select {
		case err = <-errChan:
			if err != nil {
				log.Panic().Err(err).Msg("Terminated listener")
			}
		case t := <-cleanupTicker.C:
			cnt, err := streamDB.CleanupChangeLogs(t.Add(-cleanupInterval))
			if err != nil {
				log.Warn().Err(err).Msg("Unable to cleanup change logs")
			} else if cnt > 0 {
				log.Debug().Int64("count", cnt).Msg("Cleaned up DB change logs")
			}
		case <-snapshotTicker.Channel():
			if cfg.Config.Snapshot.Enable && cfg.Config.Publish {
				lastSnapshotTime := replicator.LastSaveSnapshotTime()
				now := time.Now()
				if now.Sub(lastSnapshotTime) >= snapshotInterval {
					log.Info().
						Time("last_snapshot", lastSnapshotTime).
						Dur("duration", now.Sub(lastSnapshotTime)).
						Msg("Triggering timer based snapshot save")
					replicator.SaveSnapshot()
				}
			}
		case <-sleepTimeout.Channel():
			log.Info().Msg("No more events to process, initiating shutdown")
			ctxSt.Cancel()
			if cfg.Config.Snapshot.Enable && cfg.Config.Publish {
				log.Info().Msg("Saving snapshot before going to sleep")
				replicator.ForceSaveSnapshot()
			}

			os.Exit(0)
		}
	}
}

func changeListener(
	streamDB *db.SqliteStreamDB,
	rep *logstream.Replicator,
	ctxSt *utils.StateContext,
	events EventBus.BusPublisher,
	shard uint64,
	errChan chan error,
) {
	log.Debug().Uint64("shard", shard).Msg("Listening stream")
	err := rep.Listen(shard, onChangeEvent(streamDB, ctxSt, events))
	if err != nil {
		errChan <- err
	}
}

func onChangeEvent(streamDB *db.SqliteStreamDB, ctxSt *utils.StateContext, events EventBus.BusPublisher) func(data []byte) error {
	return func(data []byte) error {
		events.Publish("pulse")
		if ctxSt.IsCanceled() {
			return context.Canceled
		}

		if !cfg.Config.Replicate {
			return nil
		}

		ev := &logstream.ReplicationEvent[db.ChangeLogEvent]{}
		err := ev.Unmarshal(data)
		if err != nil {
			log.Error().Err(err).Send()
			return err
		}

		return streamDB.Replicate(&ev.Payload)
	}
}

func onTableChanged(r *logstream.Replicator, ctxSt *utils.StateContext, events EventBus.BusPublisher, nodeID uint64) func(event *db.ChangeLogEvent) error {
	return func(event *db.ChangeLogEvent) error {
		events.Publish("pulse")
		if ctxSt.IsCanceled() {
			return context.Canceled
		}

		if !cfg.Config.Publish {
			return nil
		}

		ev := &logstream.ReplicationEvent[db.ChangeLogEvent]{
			FromNodeId: nodeID,
			Payload:    *event,
		}

		data, err := ev.Marshal()
		if err != nil {
			return err
		}

		hash, err := event.Hash()
		if err != nil {
			return err
		}

		err = r.Publish(hash, data)
		if err != nil {
			return err
		}

		return nil
	}
}

================
File: LICENSE
================
MIT License

Copyright (c) 2022 Zohaib Sibte Hassan

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: README.md
================
# HarmonyLite

[![Go Report Card](https://goreportcard.com/badge/github.com/wongfei2009/harmonylite)](https://goreportcard.com/report/github.com/wongfei2009/harmonylite)
![GitHub](https://img.shields.io/github/license/wongfei2009/harmonylite)
[![Tests](https://github.com/wongfei2009/harmonylite/actions/workflows/tests.yml/badge.svg)](https://github.com/wongfei2009/harmonylite/actions/workflows/tests.yml)

## What is HarmonyLite?

HarmonyLite is a distributed SQLite replication system with leaderless architecture and eventual consistency. It enables robust multi-directional replication between nodes using [NATS JetStream](https://nats.io/).

*This project is a fork of [Marmot](https://github.com/maxpert/marmot), which appears to be no longer maintained.*

## Why HarmonyLite?

HarmonyLite continues and extends the vision of Marmot by providing:

- Leaderless, eventually consistent SQLite replication
- Easy horizontal scaling for read-heavy SQLite applications
- Minimal configuration with a modern, maintained codebase
- Enhanced testing and reliability
- Active development and community support

## Quick Start

Download the [latest release](https://github.com/wongfei2009/harmonylite/releases/latest) and extract:

```bash
tar vxzf harmonylite-v*.tar.gz
```

From the extracted directory, run the example cluster:

```bash
./examples/run-cluster.sh
```

Make changes to one database and watch them propagate:

```bash
# Insert data in the first database
sqlite3 /tmp/harmonylite-1.db
> PRAGMA trusted_schema = ON;
> INSERT INTO Books (title, author, publication_year) VALUES ('Project Hail Mary', 'Andy Weir', 2021);

# See it appear in the second database
sqlite3 /tmp/harmonylite-2.db
> SELECT * FROM Books;
```

## What Makes HarmonyLite Different?

Unlike other SQLite replication solutions that require a leader-follower architecture, HarmonyLite:

- Has **no primary node** - any node can write to its local database
- Operates with **eventual consistency** - no global locking or blocking
- Requires **no changes** to your existing SQLite application code
- Runs as a **sidecar** to your existing processes

## Features

![Eventually Consistent](https://img.shields.io/badge/Eventually%20Consistent-✔️-green)
![Leaderless Replication](https://img.shields.io/badge/Leaderless%20Replication-✔️-green)
![Fault Tolerant](https://img.shields.io/badge/Fault%20Tolerant-✔️-green)
![Built on NATS](https://img.shields.io/badge/Built%20on%20NATS-✔️-green)

- Multiple snapshot storage options:
  - NATS Blob Storage
  - WebDAV
  - SFTP
  - S3 Compatible (AWS S3, Minio, Blackblaze, SeaweedFS)
- Embedded NATS server
- Log compression for content-heavy applications
- Sleep timeout support for serverless environments
- Comprehensive E2E testing

## Project Roadmap

Future plans for HarmonyLite include:
- Improved documentation and examples

## Documentation

For detailed documentation, see the [docs directory](./docs).

## CLI Documentation

HarmonyLite is designed for simplicity with minimal configuration. Key command line options:

- `config` - Path to TOML configuration file
- `cleanup` - Clean up hooks and exit
- `save-snapshot` - Create and upload a snapshot
- `cluster-addr` - Binding address for cluster
- `cluster-peers` - Comma-separated list of NATS peers
- `leaf-server` - Leaf node connection list

See `config.toml` for detailed configuration options.

## Development

### Prerequisites

- Go 1.24 or later
- SQLite development libraries

### Building from Source

```bash
export CGO_ENABLED=1 CGO_CFLAGS="-Wno-typedef-redefinition -Wno-nullability-completeness"
go build
```

### Running Tests

First, install the Ginkgo test framework:
```bash
go install github.com/onsi/ginkgo/v2/ginkgo@latest
```

Then run the tests:

```bash
ginkgo tests/e2e
```

## License

MIT License - See [LICENSE](LICENSE) for details.

## Acknowledgements

This project is a fork of [Marmot](https://github.com/maxpert/marmot) by Zohaib Sibte Hassan. We are grateful for the solid foundation provided by the original project.



================================================================
End of Codebase
================================================================
