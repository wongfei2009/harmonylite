# HarmonyLite: How It Works

import Mermaid from '../components/Mermaid';

HarmonyLite is a distributed SQLite replication system with a leaderless architecture. This document explains the core components and mechanisms that make it work.

## Architecture Overview

HarmonyLite provides distributed SQLite replication through these key components:

1. **Change Data Capture (CDC)**: Uses SQLite triggers to track all database modifications
2. **NATS JetStream**: Reliable message transport with built-in consensus
3. **Node Coordination**: Manages replication between distributed nodes
4. **Snapshot Management**: Handles database state synchronization for new or long-offline nodes

<Mermaid chart={`
flowchart TB
    subgraph "Node 1"
        DB1[SQLite DB] --> Triggers1[SQLite Triggers]
        Triggers1 --> ChangeLog1[Change Log]
        ChangeLog1 --> HarmonyLite1[HarmonyLite]
    end

    subgraph "NATS JetStream"
        Streams[(JetStream Streams)]
        Objects[(Object Store)]
    end

    subgraph "Node 2"
        HarmonyLite2[HarmonyLite] --> ChangeLog2[Change Log]
        ChangeLog2 --> Triggers2[SQLite Triggers]
        Triggers2 --> DB2[SQLite DB]
    end

    HarmonyLite1 <--> Streams
    HarmonyLite2 <--> Streams
    HarmonyLite1 <--> Objects
    HarmonyLite2 <--> Objects

    classDef sqlite fill:#f9f,stroke:#333,stroke-width:2px
    classDef harmony fill:#bbf,stroke:#333,stroke-width:2px
    classDef nats fill:#bfb,stroke:#333,stroke-width:2px
    
    class DB1,DB2 sqlite
    class HarmonyLite1,HarmonyLite2 harmony
    class Streams,Objects nats
`} />

## Triggers and Data Capture

HarmonyLite captures database changes using SQLite triggers that record modifications to dedicated tracking tables:

- **Global change log table**: `__harmonylite___global_change_log` maintains the sequence of all operations committed to the database
- **Per-table change logs**: Each monitored table gets a dedicated `__harmonylite__<table_name>_change_log` table that records changed values
- **Column mapping**: Change log tables prefix original column names with `val_` (e.g., `id` becomes `val_id`)
- **Automatic trigger setup**: Triggers are compiled using Go's templating system and installed at boot time

<Mermaid chart={`
erDiagram
    USERS {
        int id PK
        string name
        string email
    }
    
    GLOBAL_CHANGE_LOG {
        int id PK
        int change_table_id FK
        string table_name
    }
    
    USERS_CHANGE_LOG {
        int id PK
        int val_id
        string val_name
        string val_email
        string type
        int created_at
        int state
    }
    
    USERS ||--o{ USERS_CHANGE_LOG : "triggers create"
    USERS_CHANGE_LOG ||--o{ GLOBAL_CHANGE_LOG : "references"
`} />

### Trigger Implementation

HarmonyLite creates three triggers for each table (`INSERT`, `UPDATE`, `DELETE`), each firing `AFTER` the change is committed:

```sql
-- Simplified example of an INSERT trigger
CREATE TRIGGER __harmonylite__users_change_log_on_insert
AFTER INSERT ON users
BEGIN
    INSERT INTO __harmonylite__users_change_log(
        val_id, val_name, val_email, 
        type, created_at, state
    ) VALUES(
        NEW.id, NEW.name, NEW.email,
        'insert',
        CAST((strftime('%s','now') || substr(strftime('%f','now'),4)) as INT),
        0 -- Pending state
    );

    INSERT INTO __harmonylite___global_change_log (change_table_id, table_name)
    VALUES (
        last_insert_rowid(),
        'users'
    );
END;
```

For `DELETE` operations, the trigger captures the `OLD` values, while `INSERT` and `UPDATE` capture the `NEW` values.

## Replication Process

When HarmonyLite detects database changes, it follows this workflow:

1. **Change Detection**: Monitors the SQLite database and Write-Ahead Log (WAL) files for modifications
2. **Change Collection**: Gathers pending change records from the tracking tables
3. **Hash Calculation**: For each record, calculates a consistent hash based on table name and primary keys
4. **Stream Selection**: Routes each change to the appropriate JetStream based on its hash
5. **Publishing**: Sends the change to the designated JetStream subject
6. **Confirmation**: Marks the change as published once JetStream acknowledges receipt
7. **Replay**: On receiving nodes, changes are applied by replaying the records to local tables
8. **Consistency**: Ordering is guaranteed by RAFT consensus at the stream level, ensuring a deterministic sequence
9. **Last-writer-wins**: When changes conflict, the last change in the cluster-wide commit order takes precedence

<Mermaid chart={`
sequenceDiagram
    participant App1 as Application (Node 1)
    participant DB1 as SQLite DB (Node 1)
    participant HL1 as HarmonyLite (Node 1)
    participant NATS as NATS JetStream
    participant HL2 as HarmonyLite (Node 2)
    participant DB2 as SQLite DB (Node 2)
    participant App2 as Application (Node 2)
    
    App1->>DB1: INSERT/UPDATE/DELETE
    DB1->>DB1: Trigger executes
    DB1->>DB1: Record in change log table
    
    HL1->>DB1: Poll for changes
    DB1->>HL1: Return pending changes
    HL1->>HL1: Calculate hash
    HL1->>NATS: Publish change to stream
    NATS->>HL1: Acknowledge receipt
    HL1->>DB1: Mark change as published
    
    NATS->>HL2: Deliver change record
    HL2->>HL2: Process change record
    HL2->>DB2: Apply change (INSERT/UPDATE/DELETE)
    HL2->>NATS: Acknowledge processing
    
    App2->>DB2: Read updated data
    DB2->>App2: Return data
`} />

## Changelog Format

Changes are transmitted as CBOR-serialized payloads (optionally compressed) with this structure:

```typescript
interface HarmonyLitePublishedRow {
  FromNodeId: number;  // Unique ID of the originating node
  Payload: {
    Id: number;        // Change log entry ID
    Type: "insert" | "update" | "delete";  // Operation type
    TableName: string; // The affected table name
    Row: {             // Column name to value mapping
      [ColumnName: string]: any
    }
  };
}
```

## Snapshot Management

Snapshots enable nodes to recover after lengthy downtimes without replaying all historical changes.

<Mermaid chart={`
graph TB
    subgraph "Snapshot Creation"
        A[Monitor Sequence Numbers] --> B{Threshold Reached?}
        B -->|Yes| C[Create Temp Directory]
        B -->|No| A
        C --> D["VACUUM INTO (Temporary Copy)"]
        D --> E[Remove Triggers and Change Logs]
        E --> F[Optimize with VACUUM]
        F --> G[Upload Snapshot]
        G --> H[Update Sequence Map]
        H --> A
    end

    subgraph "Snapshot Restoration"
        I[Node Startup] --> J[Check DB Integrity]
        J --> K[Load Sequence Map]
        K --> L{Too Far Behind?}
        L -->|Yes| M[Download Snapshot]
        L -->|No| R[Normal Operation]
        M --> N[Exclusive Lock]
        N --> O[Replace DB Files]
        O --> P[Install Triggers]
        P --> Q[Process Recent Changes]
        Q --> R
    end
`} />

### Snapshot Frequency

HarmonyLite takes snapshots based on:
- Sequence number thresholds
- Configured time intervals
- Manual triggers

### Saving Snapshots

The snapshot process works as follows:

1. **Sequence Tracking**: Each node tracks the sequence numbers of change log entries it processes
2. **Snapshot Calculation**: A node calculates when to take snapshots based on `max-log-entries รท shards`
3. **Snapshot Creation**: When a threshold is reached:
   - A temporary copy of the database is created using SQLite's `VACUUM INTO <PATH>` feature
   - HarmonyLite removes change tracking tables and triggers from the snapshot
   - The snapshot is optimized with another `VACUUM` operation
4. **Storage**: The snapshot is uploaded to the configured storage system (NATS, S3, WebDAV, or SFTP)
5. **Sequence Mapping**: The sequence position is saved to the `seq-map-path` file for future reference

### Restoring Snapshots

When a node starts, it follows this sequence:

1. **Integrity Check**: Verifies the database file and performs WAL checkpoints
2. **Sequence Comparison**: Compares local sequence numbers to the JetStream's current position
3. **Download Decision**: If too far behind, downloads the most recent snapshot
4. **Exclusive Locking**: Uses transaction locks to ensure atomic replacement
5. **File Replacement**: Replaces the local database with the downloaded snapshot
6. **Trigger Installation**: Re-installs change tracking tables and triggers
7. **Catch-up**: Processes any changes that occurred since the snapshot was taken

## Storage Options

HarmonyLite supports multiple snapshot storage backends:

- **NATS Object Storage**: Built-in storage using JetStream's object capabilities
- **S3-compatible**: Amazon S3, MinIO, Backblaze B2, etc.
- **WebDAV**: For web-based storage systems
- **SFTP**: For secure file transfers

<Mermaid chart={`
flowchart LR
    HarmonyLite[HarmonyLite] --> Storage{Storage Provider}
    Storage -->|Default| NATS[NATS Object Storage]
    Storage -->|Compatible with AWS| S3[S3 / MinIO / Backblaze]
    Storage -->|Web-based| WebDAV[WebDAV]
    Storage -->|Secure file transfer| SFTP[SFTP]
    
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:1px
    classDef primary fill:#bbdefb,stroke:#333,stroke-width:1px
    classDef provider fill:#c8e6c9,stroke:#333,stroke-width:1px
    
    class HarmonyLite primary
    class Storage default
    class NATS,S3,WebDAV,SFTP provider
`} />

## Configuration Example

```toml
# Basic configuration
db_path="/path/to/your.db"
node_id=1

# Replication configuration
[replication_log]
shards=4
max_entries=1024
replicas=3
compress=true

# Snapshot configuration
[snapshot]
enabled=true
store="nats"
interval=3600000  # Snapshot every hour (ms)

# NATS configuration
[nats]
urls=["nats://server1:4222", "nats://server2:4222"]
subject_prefix="harmonylite-change-log"
stream_prefix="harmonylite-changes"
```

## Performance Considerations

- **Shard Count**: More shards can increase write throughput but require more resources
- **Compression**: Useful for content-heavy databases but adds CPU overhead
- **Snapshot Interval**: Balance between recovery speed and storage usage
- **Cleanup Interval**: Affects how quickly change logs are purged after replication

<Mermaid chart={`
graph TD
    subgraph "HarmonyLite Performance Factors"
        A[Database Size]
        B[Change Frequency]
        C[Network Latency]
        D[Number of Nodes]
    end
    
    subgraph "Tunable Parameters"
        E[Shard Count]
        F[Compression]
        G[Snapshot Interval]
        H[Cleanup Interval]
    end
    
    A --> E
    A --> F
    B --> E
    B --> G
    B --> H
    C --> G
    D --> E
    D --> G
`} />

## Monitoring and Troubleshooting

HarmonyLite provides several mechanisms to monitor system health and diagnose problems in a distributed setup.

### Prometheus Metrics

HarmonyLite exposes Prometheus metrics that can be scraped to monitor the system's performance and health:

```toml
[prometheus]
enable=true
bind=":3010"
namespace="harmonylite"
subsystem="replication"
```

Key metrics include:

| Metric | Type | Description |
|--------|------|-------------|
| `published` | Counter | Number of change log records successfully published |
| `pending_publish` | Gauge | Number of records waiting to be published |
| `count_changes` | Histogram | Latency for counting changes in microseconds |
| `scan_changes` | Histogram | Latency for scanning change rows in microseconds |

<Mermaid chart={`
graph LR
    A[HarmonyLite Nodes] -->|expose| B[Prometheus Metrics]
    B -->|scrape| C[Prometheus Server]
    C -->|alert| D[Alertmanager]
    C -->|visualize| E[Grafana Dashboard]
    
    classDef component fill:#ddd,stroke:#333,stroke-width:1px
    classDef monitoring fill:#bbdefb,stroke:#333,stroke-width:1px
    classDef visualization fill:#c8e6c9,stroke:#333,stroke-width:1px
    
    class A component
    class B,C,D monitoring
    class E visualization
`} />

### Logging

HarmonyLite uses structured logging via zerolog to provide detailed information about system operations:

```toml
[logging]
verbose=true   # Enable debug-level logging
format="json"  # Options: "console" or "json"
```

Important log patterns to monitor:

- Errors with `"Unable to publish"` indicate replication issues
- `"Snapshot saved"` messages confirm successful snapshot operations
- `"Cleaned up DB change logs"` shows housekeeping operations

### Common Issues and Solutions

#### Replication Delays

**Symptoms:**
- Increasing `pending_publish` metric
- Changes not propagating to other nodes

**Possible Causes and Solutions:**
1. **Network Issues**
   - Check connectivity between nodes and NATS
   - Verify firewall rules allow NATS traffic (typically port 4222)
2. **NATS Stream Full**
   - Increase `max_entries` in the configuration
   - Add more NATS storage capacity
3. **Node Overload**
   - Increase `scan_max_changes` parameter
   - Consider adding more nodes or shards

#### Snapshot Failures

**Symptoms:**
- Errors containing `"Unable to save snapshot"`
- Missing snapshots in storage

**Possible Causes and Solutions:**
1. **Storage Connectivity**
   - Verify credentials and endpoint configuration
   - Check storage service health
2. **Disk Space**
   - Ensure sufficient temporary space for snapshot creation
   - Clean up old snapshots
3. **Database Locks**
   - Check for long-running transactions preventing snapshots
   - Consider adjusting application transaction patterns

#### Divergent Data

**Symptoms:**
- Different query results on different nodes
- Application inconsistency reports

**Possible Causes and Solutions:**
1. **Schema Changes**
   - Ensure schema changes are coordinated across all nodes
   - Restart HarmonyLite after schema changes
2. **Trigger Failures**
   - Check for missing triggers with `SELECT * FROM sqlite_master WHERE type='trigger'`
   - Reinstall triggers if necessary
3. **Sequence Mismatch**
   - Compare sequence numbers across nodes
   - Force a snapshot restore on divergent nodes

### Diagnostics Commands

For deeper investigation, HarmonyLite provides several command-line options:

```bash
# Check trigger installation without starting replication
harmonylite -config config.toml -verify-only

# Clean up replication artifacts (use with caution)
harmonylite -config config.toml -cleanup

# Force snapshot creation
harmonylite -config config.toml -save-snapshot

# Enable profiling for performance analysis
harmonylite -config config.toml -pprof ":6060"
```

### NATS Monitoring

Since HarmonyLite relies on NATS, monitoring the NATS infrastructure is crucial:

1. **NATS Server Monitoring Endpoint**: Available at `http://<nats-server>:8222/`
2. **Stream Information**: Check `http://<nats-server>:8222/jsz` for JetStream details
3. **Consumer Status**: Review message delivery with `http://<nats-server>:8222/consz`

<Mermaid chart={`
gantt
    title HarmonyLite Troubleshooting Workflow
    dateFormat  YYYY-MM-DD
    section Initial Checks
    Verify All Nodes Running           :a1, 2023-01-01, 1d
    Check NATS Connectivity            :a2, after a1, 1d
    Review Logs For Errors             :a3, after a2, 1d
    section Diagnose
    Monitor Prometheus Metrics         :b1, after a3, 2d
    Examine NATS Stream State          :b2, after b1, 1d
    Verify Database Triggers           :b3, after b2, 1d
    section Resolve
    Adjust Configuration               :c1, after b3, 2d
    Restart Problematic Nodes          :c2, after c1, 1d
    Force Snapshot If Needed           :c3, after c2, 1d
`} />

## Advanced Topics

- **Multiple Replica Types**: Configure read-only or write-only nodes
- **Disaster Recovery**: Strategies for handling catastrophic failures
- **Scaling**: Adding nodes to a running cluster

<Mermaid chart={`
flowchart TB
    subgraph "HarmonyLite Deployment Options"
        direction LR
        
        subgraph "Standard Deployment"
            A1[Node 1] <--> A2[Node 2]
            A2 <--> A3[Node 3]
            A3 <--> A1
        end
        
        subgraph "Read-Heavy Configuration"
            B1[Writer Node] --> B2[Reader Node 1]
            B1 --> B3[Reader Node 2]
            B1 --> B4[Reader Node 3]
        end
        
        subgraph "Regional Deployment"
            C1[Region A Nodes] <--> C2[Region B Nodes]
            C2 <--> C3[Region C Nodes]
            C3 <--> C1
        end
    end
    
    classDef writer fill:#f9a,stroke:#333,stroke-width:1px
    classDef reader fill:#adf,stroke:#333,stroke-width:1px
    classDef standard fill:#ddd,stroke:#333,stroke-width:1px
    classDef region fill:#bfb,stroke:#333,stroke-width:1px
    
    class A1,A2,A3 standard
    class B1 writer
    class B2,B3,B4 reader
    class C1,C2,C3 region
`} />